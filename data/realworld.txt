\documentclass[10pt,journal,compsoc]{IEEEtran}
\PassOptionsToPackage{hyphens}{url}
\usepackage[utf8]{inputenc}
\usepackage{amsmath} % align environment
\usepackage{amssymb} % mathbb
\usepackage{bussproofs} % notation for inference rules
\usepackage{rotating} % sidewaysfigure
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl} % Fix URL line breaking when using dvips (e.g. arxiv.org)
\usepackage[nocompress]{cite}

% Theorem environments
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{convergence-thm}{Theorem}

% Diagrams
\usepackage{tikz}
\usetikzlibrary{arrows}

\hyphenation{da-ta-cen-ter da-ta-cen-ters net-works time-stamp}

\newif\ifincludeappendix
\includeappendixtrue

\newcommand{\evalto}{\;\Longrightarrow\;}

% Placeholder character like \textvisiblespace, but works in math mode
\newcommand{\placeholder}{%
  \makebox[0.7em]{%
    \kern.07em
    \vrule height.3ex
    \hrulefill
    \vrule height.3ex
    \kern.07em
  }%
}

% Span multiple columns within an alignat math environment
\newcommand{\multialign}[2]{%
  \multispan{#1}\mbox{$\displaystyle{}#2$}%
}

\begin{document}
\sloppy
\title{A Conflict-Free Replicated JSON Datatype}
\author{Martin Kleppmann and Alastair R. Beresford
\thanks{M. Kleppmann and A.R. Beresford are with the University of Cambridge Computer Laboratory, Cambridge, UK.\protect\\Email: \url{mk428@cl.cam.ac.uk}, \url{arb33@cl.cam.ac.uk}.}}

\IEEEtitleabstractindextext{%
\begin{abstract}
% abstract word limit: 100-200 words
Many applications model their data in a general-purpose storage format such as JSON. This data structure is modified by the application as a result of user input. Such modifications are well understood if performed sequentially on a single copy of the data, but if the data is replicated and modified concurrently on multiple devices, it is unclear what the semantics should be. In this paper we present an algorithm and formal semantics for a JSON data structure that automatically resolves concurrent modifications such that no updates are lost, and such that all replicas converge towards the same state (a conflict-free replicated datatype or CRDT). It supports arbitrarily nested list and map types, which can be modified by insertion, deletion and assignment. The algorithm performs all merging client-side and does not depend on ordering guarantees from the network, making it suitable for deployment on mobile devices with poor network connectivity, in peer-to-peer networks, and in messaging systems with end-to-end encryption.
\end{abstract}

\begin{IEEEkeywords}
CRDTs, Collaborative Editing, P2P, JSON, Optimistic Replication, Operational Semantics, Eventual Consistency.
\end{IEEEkeywords}}
\maketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{U}{sers} of mobile devices, such as smartphones, expect applications to continue working while the device is offline or has poor network connectivity, and to synchronize its state with the user's other devices when the network is available. Examples of such applications include calendars, address books, note-taking tools, to-do lists, and password managers. Similarly, collaborative work often requires several people to simultaneously edit the same text document, spreadsheet, presentation, graphic, and other kinds of document, with each person's edits reflected on the other collaborators' copies of the document with minimal delay.

What these applications have in common is that the application state needs to be replicated to several devices, each of which may modify the state locally. The traditional approach to concurrency control, serializability, would cause the application to become unusable at times of poor network connectivity~\cite{Davidson:1985hv}. If we require that applications work regardless of network availability, we must assume that users can make arbitrary modifications concurrently on different devices, and that any resulting conflicts must be resolved.

The simplest way to resolve conflicts is to discard some modifications when a conflict occurs, for example using a ``last writer wins'' policy. However, this approach is undesirable as it incurs data loss. An alternative is to let the user manually resolve the conflict, which is tedious and error-prone, and therefore should be avoided whenever possible.

Current applications solve this problem with a range of ad-hoc and application-specific mechanisms. In this paper we present a general-purpose datatype that provides the full expressiveness of the JSON data model, and supports concurrent modifications without loss of information. As we shall see later, our approach typically supports the automatic merging of concurrent modifications into a JSON data structure. We introduce a single, general mechanism (a multi-value register) into our model to record conflicting updates to leaf nodes in the JSON data structure. This mechanism then provides a consistent basis on which applications can resolve any remaining conflicts through programmatic means, or via further user input.  We expect that implementations of this datatype will drastically simplify the development of collaborative and state-synchronizing applications for mobile devices.

\subsection{JSON Data Model}

JSON is a popular general-purpose data encoding format, used in many databases and web services. It has similarities to XML, and we compare them in Section~\ref{sec:json-xml}. The structure of a JSON document can optionally be constrained by a schema; however, for simplicity, this paper discusses only untyped JSON without an explicit schema.

A JSON document is a tree containing two types of branch node:

\begin{description}
\item[Map:] A node whose children have no defined order, and where each child is labelled with a string \emph{key}. A key uniquely identifies one of the children. We treat keys as immutable, but values as mutable, and key-value mappings can be added and removed from the map. A JSON map is also known as an \emph{object}.
\item[List:] A node whose children have an order defined by the application. The list can be mutated by inserting or deleting list elements. A JSON list is also known as an \emph{array}.
\end{description}

A child of a branch node can be either another branch node, or a leaf node. A leaf of the tree contains a primitive value (string, number, boolean, or null). We treat primitive values as immutable, but allow the value of a leaf node to be modified by treating it as a \emph{register} that can be assigned a new value.

This model is sufficient to express the state of a wide range of applications. For example, a text document can be represented by a list of single-character strings; character-by-character edits are then expressed as insertions and deletions of list elements. In Section~\ref{sec:examples} we describe four more complex examples of using JSON to model application data.

\subsection{Replication and Conflict Resolution}\label{sec:intro-replication}

We consider systems in which a full copy of the JSON document is replicated on several devices. Those devices could be servers in datacenters, but we focus on mobile devices such as smartphones and laptops, which have intermittent network connectivity. We do not distinguish between devices owned by the same user and different users. Our model allows each device to optimistically modify its local replica of the document, and to asynchronously propagate those edits to other replicas.

Our only requirement of the network is that messages sent by one replica are eventually delivered to all other replicas, by retrying if delivery fails. We assume the network may arbitrarily delay, reorder and duplicate messages.

Our algorithm works client-side and does not depend on any server to transform or process messages. This approach allows messages to be delivered via a peer-to-peer network as well as a secure messaging protocol with end-to-end encryption~\cite{Unger:2015kg}. The details of the network implementation and cryptographic protocols are outside of the scope of this paper.

In Section~\ref{sec:semantics} we define formal semantics describing how conflicts are resolved when a JSON document is concurrently modified on different devices. Our design is based on three simple principles:
\begin{enumerate}
\item All replicas of the data structure should automatically converge towards the same state (a requirement known as \emph{strong eventual consistency}~\cite{Shapiro:2011un}).
\item No user input should be lost due to concurrent modifications.
\item If all sequential permutations of a set of updates lead to the same state, then concurrent execution of those updates also leads to the same state~\cite{Bieniusa:2012gt}.
\end{enumerate}

\subsection{Our Contributions}

Our main contribution in this work is to define an algorithm and formal semantics for collaborative, concurrent editing of JSON data structures with automatic conflict resolution. Although similar algorithms have previously been defined for lists, maps and registers individually (see Section~\ref{sec:related}), to our knowledge this paper is the first to integrate all of these structures into an arbitrarily composable datatype that can be deployed on any network topology.

A key requirement of conflict resolution is that after any sequence of concurrent modifications, all replicas eventually converge towards the same state. In Section~\ref{sec:convergence} and the appendix we prove a theorem to show that our algorithm satisfies this requirement.

Composing maps and lists into arbitrarily nested structures opens up subtle challenges that do not arise in flat structures, due to the possibility of concurrent edits at different levels of the tree. We illustrate some of those challenges by example in Section~\ref{sec:examples}. Nested structures are an important requirement for many applications. Consequently, the long-term goal of our work is to simplify the development of applications that use optimistic replication by providing a general algorithm for conflict resolution whose details can largely be hidden inside an easy-to-use software library.

\section{Related Work}\label{sec:related}

In this section we discuss existing approaches to optimistic replication, collaborative editing and conflict resolution.

\subsection{Operational Transformation}\label{sec:related-ot}

Algorithms based on \emph{operational transformation} (OT) have long been used for collaborative editing applications~\cite{Ellis:1989ue,Ressel:1996wx,Sun:1998vf,Nichols:1995fd}. Most of them treat a document as a single ordered list (of characters, for example) and do not support nested tree structures that are required by many applications. Some algorithms generalize OT to editing XML documents~\cite{Davis:2002iv,Ignat:2003jy,Wang:2015vo}, which provides nesting of ordered lists, but these algorithms do not support key-value maps as defined in this paper (see Section~\ref{sec:json-xml}). The performance of OT algorithms degrades rapidly as the number of concurrent operations increases~\cite{Li:2006kd,Mehdi:2011ke}.

Most deployed OT collaboration systems, including Google Docs~\cite{DayRichter:2010tt}, Etherpad~\cite{Etherpad:2011um}, Novell Vibe~\cite{Spiewak:2010vw} and Apache Wave (formerly Google Wave~\cite{Wang:2015vo}), rely on a single server to decide on a total ordering of operations~\cite{Lemonik:2016wh}, a design decision inherited from the Jupiter system~\cite{Nichols:1995fd}. This approach has the advantage of making the transformation functions simpler and less error-prone~\cite{Imine:2003ks}, but it does not meet our requirements, since we want to support peer-to-peer collaboration without requiring a single server.

Many secure messaging protocols, which we plan to use for encrypted collaboration, do not guarantee that different recipients will see messages in the same order~\cite{Unger:2015kg}. Although it is possible to decide on a total ordering of operations without using a single server by using an atomic broadcast protocol~\cite{Defago:2004ji}, such protocols are equivalent to consensus~\cite{Chandra:1996cp}, so they can only safely make progress if a quorum of participants are reachable. We expect that in peer-to-peer systems of mobile devices participants will frequently be offline, and so any algorithm requiring atomic broadcast would struggle to reach a quorum and become unavailable. Without quorums, the strongest guarantee a system can give is causal ordering~\cite{Attiya:2015dm}.

The Google Realtime API~\cite{Google:2015vk} is to our knowledge the only implementation of OT that supports arbitrary nesting of lists and maps. Like Google Docs, it relies on a single server~\cite{Lemonik:2016wh}. As a proprietary product, details of its algorithms have not been published.

\subsection{CRDTs}\label{sec:related-crdts}

Conflict-free replicated datatypes (CRDTs) are a family of data structures that support concurrent modification and guarantee convergence of concurrent updates. They work by attaching additional metadata to the data structure, making modification operations commutative by construction. The JSON datatype described in this paper is a CRDT.

CRDTs for registers, counters, maps, and sets are well-known~\cite{Shapiro:2011un,Shapiro:2011wy}, and have been implemented in various deployed systems such as Riak~\cite{Brown:2014hs,Brown:2013wy}. For ordered lists, various algorithms have been proposed, including WOOT~\cite{Oster:2006wj}, RGA~\cite{Roh:2011dw}, Treedoc~\cite{Preguica:2009fz}, Logoot~\cite{Weiss:2010hx}, and LSEQ~\cite{Nedelec:2013ky}. Attiya et al.~\cite{Attiya:2016kh} analyze the metadata overhead of collaboratively edited lists, and provide a correctness proof of the RGA algorithm. However, none of them support nesting: all of the aforementioned algorithms assume that each of their elements is a primitive value, not another CRDT.

The problem of nesting one CRDT inside another (also known as \emph{composition} or \emph{embedding}) has only been studied more recently. Riak allows nesting of counters and registers inside maps, and of maps within other maps~\cite{Brown:2014hs,Brown:2013wy}. Embedding counters inside maps raises questions of semantics, which have been studied by Baquero, Almeida and Lerche~\cite{Baquero:2016iv}. Almeida et al.~\cite{Almeida:2016tk} also define delta mutations for nested maps, and Baquero et al.~\cite{Baquero:2015tm} define a theoretical framework for composition of state-based CRDTs, based on lattices. None of this work integrates CRDTs for ordered lists, but the treatment of causality in these datatypes forms a basis for the semantics developed in this paper.

Burckhardt et al.~\cite{Burckhardt:2012jy} define \emph{cloud types}, which are similar to CRDTs and can be composed. They define \emph{cloud arrays}, which behave similarly to our map datatype, and \emph{entities}, which are like unordered sets or relations; ordered lists are not defined in this framework.

On the other hand, Martin et al.~\cite{Martin:2010ih} generalize Logoot~\cite{Weiss:2010hx} to support collaborative editing of XML documents~-- that is, a tree of nested ordered lists without nested maps. As discussed in Section~\ref{sec:json-xml}, such a structure does not capture the expressiveness of JSON.

Although CRDTs for registers, maps and ordered lists have existed for years in isolation, we are not aware of any prior work that allows them all to be composed into an arbitrarily nested CRDT with a JSON-like structure.

\subsection{Other Approaches}\label{sec:related-other}

Many replicated data systems need to deal with the problem of concurrent, conflicting modifications, but the solutions are often ad-hoc. For example, in Dynamo~\cite{DeCandia:2007ui} and CouchDB, if several values are concurrently written to the same key, the database preserves all of these values, and leaves conflict resolution to application code -- in other words, the only datatype it supports is a multi-value register. Naively chosen merge functions often exhibit anomalies such as deleted items reappearing~\cite{DeCandia:2007ui}. We believe that conflict resolution is not a simple matter that can reasonably be left to application programmers.

Another frequently-used approach to conflict resolution is \emph{last writer wins} (LWW), which arbitrarily chooses one among several concurrent writes as ``winner'' and discards the others. LWW is used in Apache Cassandra, for example. It does not meet our requirements, since we want no user input to be lost due to concurrent modifications.

Resolving concurrent updates on tree structures has been studied in the context of file synchronization~\cite{Balasubramaniam:1998jh,Ramsey:2001ce}.

Finally, systems such as Bayou~\cite{Terry:1995dn} allow offline nodes to execute transactions tentatively, and confirm them when they are next online. This approach relies on all servers executing transactions in the same serial order, and deciding whether a transaction was successful depending on its preconditions. Bayou has the advantage of being able to express global invariants such as uniqueness constraints, which require serialization and cannot be expressed using CRDTs~\cite{Bailis:2014th}. Bayou's downside is that tentative transactions may be rolled back, requiring explicit handling by the application, whereas CRDTs are defined such that operations cannot fail after they have been performed on one replica.


\section{Composing Data Structures}\label{sec:composing}

In this section we informally introduce our approach to collaborative editing of JSON data structures, and illustrate some peculiarities of concurrent nested data structures. A formal presentation of the algorithm follows in Section~\ref{sec:semantics}.

\subsection{Concurrent Editing Examples}\label{sec:examples}

\begin{figure*}[p]
\centering
\begin{tikzpicture}[auto,scale=0.8]
\path [draw,dotted] (4,-0.5) -- (4,6.5);
\node (leftR)  at (0,6) {Replica $p$:};
\node (rightR) at (8,6) {Replica $q$:};
\node (left0)  at (0,5) [rectangle,draw] {\{``key'': ``A''\}};
\node (right0) at (8,5) [rectangle,draw] {\{``key'': ``A''\}};
\node (left1)  at (0,3) [rectangle,draw] {\{``key'': ``B''\}};
\node (right1) at (8,3) [rectangle,draw] {\{``key'': ``C''\}};
\node (left2)  at (0,0) [rectangle,draw] {\{``key'': \{``B'', ``C''\}\}};
\node (right2) at (8,0) [rectangle,draw] {\{``key'': \{``B'', ``C''\}\}};
\node (comms)  at (4,1.6) [text=blue] {\footnotesize network communication};
\draw [thick,->] (left0) to node [left,inner sep=8pt] {doc.get(``key'') := ``B'';} (left1);
\draw [thick,->] (right0) to node [right,inner sep=8pt] {doc.get(``key'') := ``C'';} (right1);
\draw [thick,->] (left1) -- (left2);
\draw [thick,dashed,blue,->] (left1.south)  to [out=270,in=135] (right2.north west);
\draw [thick,dashed,blue,->] (right1.south) to [out=270,in=45]  (left2.north east);
\draw [thick,->] (right1) -- (right2);
\end{tikzpicture}
\caption{Concurrent assignment to the register at doc.get(``key'') by replicas $p$ and $q$.}\label{fig:register-assign}
\end{figure*}

The sequential semantics of editing a JSON data structure are well-known, and the semantics of concurrently editing a flat map or list data structure have been thoroughly explored in the literature (see Section~\ref{sec:related}). However, when defining a CRDT for JSON data, difficulties arise due to the interactions between concurrency and nested data structures.

In the following examples we show some situations that might occur when JSON documents are concurrently modified, demonstrate how they are handled by our algorithm, and explain the rationale for our design decisions. In all examples we assume two replicas, labelled $p$ (drawn on the left-hand side) and $q$ (right-hand side). Local state for a replica is drawn in boxes, and modifications to local state are shown with labelled solid arrows; time runs down the page. Since replicas only mutate local state, we make communication of state changes between replicas explicit in our model. Network communication is depicted with dashed arrows.

Our first example is shown in Figure~\ref{fig:register-assign}. In a document that maps ``key'' to a register with value ``A'', replica $p$ sets the value of the register to ``B'', while replica $q$ concurrently sets it to ``C''. As the replicas subsequently exchange edits via network communication, they detect the conflict. Since we do not want to simply discard one of the edits, and the strings ``B'' and ``C'' cannot be meaningfully merged, the system must preserve both concurrent updates. This datatype is known as a \emph{multi-value register}: although a replica can only assign a single value to the register, reading the register may return a set of multiple values that were concurrently written.

A multi-value register is hardly an impressive CRDT, since it does not actually perform any conflict resolution. We use it only for primitive values for which no automatic merge function is defined. Other CRDTs could be substituted in its place: for example, a counter CRDT for a number that can only be incremented and decremented, or an ordered list of characters for a collaboratively editable string (see also Figure~\ref{fig:text-edit}).

\begin{figure*}[p]
\centering
\begin{tikzpicture}[auto,scale=0.8]
\path [draw,dotted] (4,-1) -- (4,8);
\node (left0)  at (0,7.5) [rectangle,draw] {\{``colors'': \{``blue'': ``\#0000ff''\}\}};
\node (right0) at (8,7.5) [rectangle,draw] {\{``colors'': \{``blue'': ``\#0000ff''\}\}};
\node [matrix] (left1) at (0,4) [rectangle,draw] {
    \node {\{``colors'': \{``blue'': ``\#0000ff'',}; \\
    \node {``red'': ``\#ff0000''\}\}}; \\
};
\node (right1) at (8,5.5) [rectangle,draw] {\{``colors'': \{\}\}};
\node (right2) at (8,3) [rectangle,draw] {\{``colors'': \{``green'': ``\#00ff00''\}\}};
\node [matrix] (left2) at (0,0) [rectangle,draw] {
    \node {\{``colors'': \{``red'': ``\#ff0000'',}; \\
    \node {``green'': ``\#00ff00''\}\}}; \\
};
\node [matrix] (right3) at (8,0) [rectangle,draw] {
    \node {\{``colors'': \{``red'': ``\#ff0000'',}; \\
    \node {``green'': ``\#00ff00''\}\}}; \\
};
\node (comms) at (4,2.1) [text=blue] {\footnotesize network communication};
\draw [thick,->] (left0)  -- (left1)
    node [left,text width=4.2cm,text centered,midway]  {doc.get(``colors'').get(``red'') := ``\#ff0000'';};
\draw [thick,->] (right0) to node [right] {doc.get(``colors'') := \{\};} (right1);
\draw [thick,->] (right1) -- (right2)
    node [right,text width=4.2cm,text centered,midway] {doc.get(``colors'').get(``green'') := ``\#00ff00'';};
\draw [thick,->] (left1)  to (left2);
\draw [thick,->] (right2) to (right3);
\draw [thick,dashed,blue,->] (left1.south)  to [out=270,in=135] (right3.north west);
\draw [thick,dashed,blue,->] (right2.south) to [out=270,in=45]  (left2.north east);
\end{tikzpicture}
\caption{Modifying the contents of a nested map while concurrently the entire map is overwritten.}\label{fig:map-remove}
\end{figure*}

\begin{figure*}[p]
\centering
\begin{tikzpicture}[auto,scale=0.8]
\path [draw,dotted] (5,-0.5) -- (5,10);
\node (left0)  at  (0,9.5) [rectangle,draw] {\{\}};
\node (right0) at (10,9.5) [rectangle,draw] {\{\}};
\node (left1)  at  (0,7.5) [rectangle,draw] {\{``grocery'': []\}};
\node (right1) at (10,7.5) [rectangle,draw] {\{``grocery'': []\}};
\node (left2)  at  (0,5.0) [rectangle,draw] {\{``grocery'': [``eggs'']\}};
\node (left3)  at  (0,2.5) [rectangle,draw] {\{``grocery'': [``eggs'', ``ham'']\}};
\node (right2) at (10,5.0) [rectangle,draw] {\{``grocery'': [``milk'']\}};
\node (right3) at (10,2.5) [rectangle,draw] {\{``grocery'': [``milk'', ``flour'']\}};
\node (left4)  at  (0,0.0) [rectangle,draw] {\{``grocery'': [``eggs'', ``ham'', ``milk'', ``flour'']\}};
\node (right4) at (10,0.0) [rectangle,draw] {\{``grocery'': [``eggs'', ``ham'', ``milk'', ``flour'']\}};
\node (comms)  at  (5,1.4) [text=blue] {\footnotesize network communication};
\draw [thick,->] (left0)  to node [left]  {doc.get(``grocery'') := [];} (left1);
\draw [thick,->] (right0) to node [right] {doc.get(``grocery'') := [];} (right1);
\draw [thick,->] (left1)  -- (left2)
    node [left,text width=4cm,text centered,midway]  {doc.get(``grocery'').idx(0) .insertAfter(``eggs'');};
\draw [thick,->] (right1) -- (right2)
    node [right,text width=4cm,text centered,midway] {doc.get(``grocery'').idx(0) .insertAfter(``milk'');};
\draw [thick,->] (left2)  -- (left3)
    node [left,text width=4cm,text centered,midway]  {doc.get(``grocery'').idx(1) .insertAfter(``ham'');};
\draw [thick,->] (right2) -- (right3)
    node [right,text width=4cm,text centered,midway] {doc.get(``grocery'').idx(1) .insertAfter(``flour'');};
\draw [thick,->] (left3)  to (left4);
\draw [thick,->] (right3) to (right4);
\draw [thick,dashed,blue,->] (left3.south)  to [out=270,in=135] (right4.north west);
\draw [thick,dashed,blue,->] (right3.south) to [out=270,in=45]  (left4.north east);
\end{tikzpicture}
\caption{Two replicas concurrently create ordered lists under the same map key.}\label{fig:two-lists}
\end{figure*}

\begin{figure*}[p]
\centering
\begin{tikzpicture}[auto,scale=0.8]
\path [draw,dotted] (4,-0.5) -- (4,8.5);
\node (leftR)  at (0,8) {Replica $p$:};
\node (rightR) at (8,8) {Replica $q$:};
\node (left1)  at (0,7) [rectangle,draw] {[``a'', ``b'', ``c'']};
\node (left2)  at (0,5) [rectangle,draw] {[``a'', ``c'']};
\node (left3)  at (0,3) [rectangle,draw] {[``a'', ``x'', ``c'']};
\node (left4)  at (0,0) [rectangle,draw] {[``y'', ``a'', ``x'', ``z'', ``c'']};
\node (right1) at (8,7) [rectangle,draw] {[``a'', ``b'', ``c'']};
\node (right2) at (8,5) [rectangle,draw] {[``y'', ``a'', ``b'', ``c'']};
\node (right3) at (8,3) [rectangle,draw] {[``y'', ``a'', ``z'', ``b'', ``c'']};
\node (right4) at (8,0) [rectangle,draw] {[``y'', ``a'', ``x'', ``z'', ``c'']};
\node (comms)  at (4, 1.5) [text=blue] {\footnotesize network communication};
\draw [thick,->] (left1)  to node [left]  {doc.idx(2).delete;} (left2);
\draw [thick,->] (left2)  to node [left]  {doc.idx(1).insertAfter(``x'');} (left3);
\draw [thick,->] (right1) to node [right] {doc.idx(0).insertAfter(``y'');} (right2);
\draw [thick,->] (right2) to node [right] {doc.idx(2).insertAfter(``z'');} (right3);
\draw [thick,->] (left3)  to (left4);
\draw [thick,->] (right3) to (right4);
\draw [thick,dashed,blue,->] (left3.south)  to [out=270,in=135] (right4.north west);
\draw [thick,dashed,blue,->] (right3.south) to [out=270,in=45]  (left4.north east);
\end{tikzpicture}
\caption{Concurrent editing of an ordered list of characters (i.e., a text document).}\label{fig:text-edit}
\end{figure*}

\begin{figure*}[p]
\centering
\begin{tikzpicture}[auto,scale=0.8]
\path [draw,dotted] (4,-1) -- (4,7.5);
\node (left1)  at (0,7) [rectangle,draw] {\{\}};
\node (left2)  at (0,5) [rectangle,draw] {\{``a'': \{\}\}};
\node (left3)  at (0,3) [rectangle,draw] {\{``a'': \{``x'': ``y''\}\}};
\node [matrix] (left4) at (0,0) [rectangle,draw] {
    \node {\{mapT(``a''): \{``x'': ``y''\},}; \\
    \node {listT(``a''): [``z'']\}}; \\
};
\node (right1) at (8,7) [rectangle,draw] {\{\}};
\node (right2) at (8,5) [rectangle,draw] {\{``a'': []\}};
\node (right3) at (8,3) [rectangle,draw] {\{``a'': [``z'']\}};
\node [matrix] (right4) at (8,0) [rectangle,draw] {
    \node {\{mapT(``a''): \{``x'': ``y''\},}; \\
    \node {listT(``a''): [``z'']\}}; \\
};
\node (comms)  at (4,2.0) [text=blue] {\footnotesize network communication};
\draw [thick,->] (left1)  to node [left]  {doc.get(``a'') := \{\};} (left2);
\draw [thick,->] (left2)  to node [left]  {doc.get(``a'').get(``x'') := ``y'';} (left3);
\draw [thick,->] (right1) to node [right] {doc.get(``a'') := [];} (right2);
\draw [thick,->] (right2) to node [right] {doc.get(``a'').idx(0).insertAfter(``z'');} (right3);
\draw [thick,->] (left3)  to (left4);
\draw [thick,->] (right3) to (right4);
\draw [thick,dashed,blue,->] (left3.south)  to [out=270,in=135] (right4.north west);
\draw [thick,dashed,blue,->] (right3.south) to [out=270,in=45]  (left4.north east);
\end{tikzpicture}
\caption{Concurrently assigning values of different types to the same map key.}\label{fig:type-clash}
\end{figure*}

\begin{figure*}[p]
\centering
\begin{tikzpicture}[auto,scale=0.8]
\path [draw,dotted] (4,-0.5) -- (4,7.0);
\node [matrix] (left0) at (0,6) [rectangle,draw] {
    \node {\{``todo'': [\{``title'': ``buy milk'',}; \\
    \node {``done'': false\}]\}}; \\
};
\node [matrix] (right0) at (8,6) [rectangle,draw] {
    \node {\{``todo'': [\{``title'': ``buy milk'',}; \\
    \node {``done'': false\}]\}}; \\
};
\node (left1)  at (0,3) [rectangle,draw] {\{``todo'': []\}};
\node [matrix] (right1) at (8,3) [rectangle,draw] {
    \node {\{``todo'': [\{``title'': ``buy milk'',}; \\
    \node {``done'': true\}]\}}; \\
};
\node (left2)  at (0,0) [rectangle,draw] {\{``todo'': [\{``done'': true\}]\}};
\node (right2) at (8,0) [rectangle,draw] {\{``todo'': [\{``done'': true\}]\}};
\node (comms)  at (4,1.6) [text=blue] {\footnotesize network communication};
\draw [thick,->] (left0)  to node [left]  {doc.get(``todo'').idx(1).delete;} (left1);
\draw [thick,->] (right0) to node [right] {doc.get(``todo'').idx(1).get(``done'') := true;} (right1);
\draw [thick,->] (left1)  to (left2);
\draw [thick,->] (right1) to (right2);
\draw [thick,dashed,blue,->] (left1.south)  to [out=270,in=135] (right2.north west);
\draw [thick,dashed,blue,->] (right1.south) to [out=270,in=45]  (left2.north east);
\end{tikzpicture}
\caption{One replica removes a list element, while another concurrently updates its contents.}\label{fig:todo-item}
\end{figure*}

Figure~\ref{fig:map-remove} gives an example of concurrent edits at different levels of the JSON tree. Here, replica $p$ adds ``red'' to a map of colors, while replica $q$ concurrently blanks out the entire map of colors and then adds ``green''. Instead of assigning an empty map, $q$ could equivalently remove the entire key ``colors'' from the outer map, and then assign a new empty map to that key. The difficulty in this example is that the addition of ``red'' occurs at a lower level of the tree, while concurrently the removal of the map of colors occurs at a higher level of the tree.

One possible way of handling such a conflict would be to let edits at higher levels of the tree always override concurrent edits within that subtree. In this case, that would mean the addition of ``red'' would be discarded, since it would be overridden by the blanking-out of the entire map of colors. However, that behavior would violate our requirement that no user input should be lost due to concurrent modifications. Instead, we define merge semantics that preserve all changes, as shown in Figure~\ref{fig:map-remove}: ``blue'' must be absent from the final map, since it was removed by blanking out the map, while ``red'' and ``green'' must be present, since they were explicitly added. This behavior matches that of CRDT maps in Riak~\cite{Brown:2014hs,Brown:2013wy}.

Figure~\ref{fig:two-lists} illustrates another problem with maps: two replicas can concurrently insert the same map key. Here, $p$ and $q$ each independently create a new shopping list under the same map key ``grocery'', and add items to the list. In the case of Figure~\ref{fig:register-assign}, concurrent assignments to the same map key were left to be resolved by the application, but in Figure~\ref{fig:two-lists}, both values are lists and so they can be merged automatically. We preserve the ordering and adjacency of items inserted at each replica, so ``ham'' appears after ``eggs'', and ``flour'' appears after ``milk'' in the merged result. There is no information on which replica's items should appear first in the merged result, so the algorithm can make an arbitrary choice between ``eggs, ham, milk, flour'' and ``milk, flour, eggs, ham'', provided that all replicas end up with the items in the same order.

Figure~\ref{fig:text-edit} shows how a collaborative text editor can be implemented, by treating the document as a list of characters. All changes are preserved in the merged result: ``y'' is inserted before ``a''; ``x'' and ``z'' are inserted between ``a'' and ``c''; and ``b'' is deleted.

Figure~\ref{fig:type-clash} demonstrates a variant of the situation in Figure~\ref{fig:two-lists}, where two replicas concurrently insert the same map key, but they do so with different datatypes as values: $p$ inserts a nested map, whereas $q$ inserts a list. These datatypes cannot be meaningfully merged, so we preserve both values separately. We do this by tagging each map key with a type annotation (\textsf{mapT}, \textsf{listT}, or \textsf{regT} for a map, list, or register value respectively), so each type inhabits a separate namespace.

Finally, Figure~\ref{fig:todo-item} shows a limitation of the principle of preserving all user input. In a to-do list application, one replica removes a to-do item from the list, while another replica concurrently marks the same item as done. As the changes are merged, the update of the map key ``done'' effectively causes the list item to be resurrected on replica $p$, leaving a to-do item without a title (since the title was deleted as part of deleting the list item). This behavior is consistent with the example in Figure~\ref{fig:map-remove}, but it is perhaps surprising. In this case it may be more desirable to discard one of the concurrent updates, and thus preserve the implicit schema that a to-do item has both a ``title'' and a ``done'' field. We leave the analysis of developer expectations and the development of a schema language for future work.

\subsection{JSON Versus XML}\label{sec:json-xml}

The most common alternative to JSON is XML, and collaborative editing of XML documents has been previously studied~\cite{Davis:2002iv,Ignat:2003jy,Wang:2015vo}. Besides the superficial syntactical differences, the tree structure of XML and JSON appears quite similar. However, there is an important difference that we should highlight.

JSON has two collection constructs that can be arbitrarily nested: maps for unordered key-value pairs, and lists for ordered sequences. In XML, the children of an element form an ordered sequence, while the attributes of an element are unordered key-value pairs. However, XML does not allow nested elements inside attributes -- the value of an attribute can only be a primitive datatype. Thus, XML supports maps within lists, but not lists within maps. In this regard, XML is less expressive than JSON: the scenarios in Figures~\ref{fig:two-lists} and~\ref{fig:type-clash} cannot occur in XML.

Some applications may attach map-like semantics to the children of an XML document, for example by interpreting the child element name as key. However, this key-value structure is not part of XML itself, and would not be enforced by existing collaborative editing algorithms for XML. If multiple children with the same key are concurrently created, existing algorithms would create duplicate children with the same key rather than merging them like in Figure~\ref{fig:two-lists}.

\subsection{Document Editing API}\label{sec:editing-api}

\begin{figure}
\centering
\begin{tabular}{rcll}
CMD & ::= & \texttt{let} $x$ \texttt{=} EXPR & $x \in \mathrm{VAR}$ \\
& $|$ & $\mathrm{EXPR}$ \texttt{:=} $v$ & $v \in \mathrm{VAL}$ \\
& $|$ & $\mathrm{EXPR}$\texttt{.insertAfter(}$v$\texttt{)} & $v \in \mathrm{VAL}$ \\
& $|$ & $\mathrm{EXPR}$\texttt{.delete} \\
& $|$ & \texttt{yield} \\
& $|$ & CMD\texttt{;} CMD \vspace{0.5em}\\
EXPR & ::= & \texttt{doc} \\
& $|$ & $x$ & $x \in \mathrm{VAR}$ \\
& $|$ & EXPR\texttt{.get(}$\mathit{key}$\texttt{)} & $\mathit{key} \in \mathrm{String}$ \\
& $|$ & EXPR\texttt{.idx(}$i$\texttt{)} & $i \ge 0$ \\
& $|$ & EXPR\texttt{.keys} \\
& $|$ & EXPR\texttt{.values} \vspace{0.5em}\\
VAR & ::= & $x$ & $x \in \mathrm{VarString}$\\
VAL & ::= & $n$ & $n \in \mathrm{Number}$ \\
& $|$ & $\mathit{str}$ & $\mathit{str} \in \mathrm{String}$ \\
& $|$ & \texttt{true} $|$ \texttt{false} $|$ \texttt{null} \\
& $|$ & \verb|{}| $|$ \verb|[]|
\end{tabular}
\caption{Syntax of command language for querying and modifying a document.}\label{fig:local-syntax}
\end{figure}

\begin{figure}
\centering
\begin{verbatim}
doc := {};
doc.get("shopping") := [];
let head = doc.get("shopping").idx(0);
head.insertAfter("eggs");
let eggs = doc.get("shopping").idx(1);
head.insertAfter("cheese");
eggs.insertAfter("milk");

// Final state:
{"shopping": ["cheese", "eggs", "milk"]}
\end{verbatim}
\caption{Example of programmatically constructing a JSON document.}\label{fig:make-doc}
\end{figure}

To define the semantics for collaboratively editable data structures, we first define a simple command language that is executed locally at any of the replicas, and which allows that replica's local copy of the document to be queried and modified. Performing read-only queries has no side-effects, but modifying the document has the effect of producing \emph{operations} describing the mutation. Those operations are immediately applied to the local copy of the document, and also enqueued for asynchronous broadcasting to other replicas.

The syntax of the command language is given in Figure~\ref{fig:local-syntax}. It is not a full programming language, but rather an API through which the document state is queried and modified. We assume that the program accepts user input and issues a (possibly infinite) sequence of commands to the API. We model only the semantics of those commands, and do not assume anything about the program in which the command language is embedded. The API differs slightly from the JSON libraries found in many programming languages, in order to allow us to define consistent merge semantics.

We first explain the language informally, before giving its formal semantics. The expression construct EXPR is used to construct a \emph{cursor} which identifies a position in the document. An expression starts with either the special token \texttt{doc}, identifying the root of the JSON document tree, or a variable $x$ that was previously defined in a \texttt{let} command. The expression defines, left to right, the path the cursor takes as it navigates through the tree towards the leaves: the operator \texttt{.get(}$\mathit{key}$\texttt{)} selects a key within a map, and \texttt{.idx(}$n$\texttt{)} selects the $n$th element of an ordered list. Lists are indexed starting from 1, and \texttt{.idx(0)} is a special cursor indicating the head of a list (a virtual position before the first list element).

The expression construct EXPR can also query the state of the document: \texttt{keys} returns the set of keys in the map at the current cursor, and \texttt{values} returns the contents of the multi-value register at the current cursor. (\texttt{values} is not defined if the cursor refers to a map or list.)

A command CMD either sets the value of a local variable (\texttt{let}), performs network communication (\texttt{yield}), or modifies the document. A document can be modified by writing to a register (the operator \texttt{:=} assigns the value of the register identified by the cursor), by inserting an element into a list (\texttt{insertAfter} places a new element after the existing list element identified by the cursor, and \texttt{.idx(0).insertAfter} inserts at the head of a list), or by deleting an element from a list or a map (\texttt{delete} removes the element identified by the cursor).

Figure~\ref{fig:make-doc} shows an example sequence of commands that constructs a new document representing a shopping list. First \texttt{doc} is set to \verb|{}|, the empty map literal, and then the key \verb|"shopping"| within that map is set to the empty list \verb|[]|. The third line navigates to the key \verb|"shopping"| and selects the head of the list, placing the cursor in a variable called \texttt{head}. The list element ``eggs'' is inserted at the head of the list. In line 5, the variable \texttt{eggs} is set to a cursor pointing at the list element ``eggs''. Then two more list elements are inserted: ``cheese'' at the head, and ``milk'' after ``eggs''.

Note that the cursor \texttt{eggs} identifies the list element by identity, not by its index: after the insertion of ``cheese'', the element ``eggs'' moves from index 1 to 2, but ``milk'' is nevertheless inserted after ``eggs''. As we shall see later, this feature is helpful for achieving desirable semantics in the presence of concurrent modifications.


\begin{figure*}
\begin{center}
\AxiomC{$\mathit{cmd}_1 \mathbin{:} \mathrm{CMD}$}
\AxiomC{$A_p,\, \mathit{cmd}_1 \evalto A_p'$}
\LeftLabel{\textsc{Exec}}
\BinaryInfC{$A_p,\, \langle \mathit{cmd}_1 \mathbin{;} \mathit{cmd}_2 \mathbin{;} \dots \rangle
    \evalto A_p',\, \langle \mathit{cmd}_2 \mathbin{;} \dots \rangle$}
\DisplayProof\hspace{4em}
%
\AxiomC{}
\LeftLabel{\textsc{Doc}}
\UnaryInfC{$A_p,\, \mathsf{doc} \evalto \mathsf{cursor}(\langle\rangle,\, \mathsf{doc})$}
\DisplayProof\proofSkipAmount
\end{center}

\begin{center}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathit{cur}$}
\LeftLabel{\textsc{Let}}
\UnaryInfC{$A_p,\, \mathsf{let}\; x = \mathit{expr} \evalto A_p[\,x \,\mapsto\, \mathit{cur}\,]$}
\DisplayProof\hspace{3em}
%
\AxiomC{$x \in \mathrm{dom}(A_p)$}
\LeftLabel{\textsc{Var}}
\UnaryInfC{$A_p,\, x \evalto A_p(x)$}
\DisplayProof\proofSkipAmount
\end{center}

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathsf{cursor}(\langle k_1, \dots, k_{n-1} \rangle,\, k_n)$}
\AxiomC{$k_n \not= \mathsf{head}$}
\LeftLabel{\textsc{Get}}
\BinaryInfC{$A_p,\, \mathit{expr}.\mathsf{get}(\mathit{key}) \evalto
    \mathsf{cursor}(\langle k_1, \dots, k_{n-1}, \mathsf{mapT}(k_n) \rangle,\, \mathit{key})$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathsf{cursor}(\langle k_1, \dots, k_{n-1} \rangle,\, k_n)$}
\AxiomC{$A_p,\, \mathsf{cursor}(\langle k_1, \dots, k_{n-1}, \mathsf{listT}(k_n) \rangle,\,
    \mathsf{head}).\mathsf{idx}(i) \evalto \mathit{cur}'$}
\LeftLabel{$\textsc{Idx}_1$}
\BinaryInfC{$A_p,\, \mathit{expr}.\mathsf{idx}(i) \evalto \mathit{cur}'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$k_1 \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{ctx}(k_1),\, \mathsf{cursor}(\langle k_2, \dots, k_{n-1} \rangle,\, k_n).\mathsf{idx}(i)
    \evalto \mathsf{cursor}(\langle k_2, \dots, k_{n-1} \rangle,\, k_n')$}
\LeftLabel{$\textsc{Idx}_2$}
\BinaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle k_1, k_2, \dots, k_{n-1} \rangle,\, k_n).\mathsf{idx}(i)
    \evalto \mathsf{cursor}(\langle k_1, k_2, \dots, k_{n-1} \rangle,\, k_n')$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$i>0 \,\wedge\, \mathit{ctx}(\mathsf{next}(k)) = k' \,\wedge\, k' \not= \mathsf{tail}$}
\AxiomC{$\mathit{ctx}(\mathsf{pres}(k')) \not= \{\}$}
\AxiomC{$\mathit{ctx},\, \mathsf{cursor}(\langle\rangle,\, k').\mathsf{idx}(i-1) \evalto \mathit{ctx}'$}
\LeftLabel{$\textsc{Idx}_3$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle\rangle,\, k).\mathsf{idx}(i) \evalto \mathit{ctx}'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$i>0 \,\wedge\, \mathit{ctx}(\mathsf{next}(k)) = k' \,\wedge\, k' \not= \mathsf{tail}$}
\AxiomC{$\mathit{ctx}(\mathsf{pres}(k')) = \{\}$}
\AxiomC{$\mathit{ctx},\, \mathsf{cursor}(\langle\rangle,\, k').\mathsf{idx}(i) \evalto \mathit{cur}'$}
\LeftLabel{$\textsc{Idx}_4$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle\rangle,\, k).\mathsf{idx}(i) \evalto \mathit{cur}'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$i=0$}
\LeftLabel{$\textsc{Idx}_5$}
\UnaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle\rangle,\, k).\mathsf{idx}(i) \evalto
    \mathsf{cursor}(\langle\rangle,\, k)$}
\end{prooftree}

\[ \mathrm{keys}(\mathit{ctx}) = \{\; k \mid
    \mathsf{mapT}(k)  \in \mathrm{dom}(\mathit{ctx}) \,\vee\,
    \mathsf{listT}(k) \in \mathrm{dom}(\mathit{ctx}) \,\vee\,
    \mathsf{regT}(k)  \in \mathrm{dom}(\mathit{ctx})
\;\} \]

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathit{cur}$}
\AxiomC{$A_p,\, \mathit{cur}.\mathsf{keys} \evalto \mathit{keys}$}
\LeftLabel{$\textsc{Keys}_1$}
\BinaryInfC{$A_p,\, \mathit{expr}.\mathsf{keys} \evalto \mathit{keys}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathit{map} = \mathit{ctx}(\mathsf{mapT}(k))$}
\AxiomC{$\mathit{keys} = \{\; k \mid k \in \mathrm{keys}(\mathit{map}) \,\wedge\,
    \mathit{map}(\mathsf{pres}(k)) \not= \{\} \;\}$}
\LeftLabel{$\textsc{Keys}_2$}
\BinaryInfC{$A_p,\, \mathsf{cursor}(\langle\rangle,\, k).\mathsf{keys} \evalto \mathit{keys}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$k_1 \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{ctx}(k_1),\, \mathsf{cursor}(\langle k_2, \dots, k_{n-1} \rangle,\, k_n).\mathsf{keys}
    \evalto \mathit{keys}$}
\LeftLabel{$\textsc{Keys}_3$}
\BinaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle k_1, k_2, \dots, k_{n-1} \rangle,\, k_n).\mathsf{keys}
    \evalto \mathit{keys}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathit{cur}$}
\AxiomC{$A_p,\, \mathit{cur}.\mathsf{values} \evalto \mathit{val}$}
\LeftLabel{$\textsc{Val}_1$}
\BinaryInfC{$A_p,\, \mathit{expr}.\mathsf{values} \evalto \mathit{val}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathsf{regT}(k) \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{val} = \mathrm{range}(\mathit{ctx}(\mathsf{regT}(k)))$}
\LeftLabel{$\textsc{Val}_2$}
\BinaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle\rangle,\, k).\mathsf{values} \evalto \mathit{val}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$k_1 \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{ctx}(k_1),\, \mathsf{cursor}(\langle k_2, \dots, k_{n-1} \rangle,\, k_n).\mathsf{values}
    \evalto \mathit{val}$}
\LeftLabel{$\textsc{Val}_3$}
\BinaryInfC{$\mathit{ctx},\, \mathsf{cursor}(\langle k_1, k_2, \dots, k_{n-1} \rangle,\, k_n).\mathsf{values}
    \evalto \mathit{val}$}
\end{prooftree}
\caption{Rules for evaluating expressions.}\label{fig:expr-rules}
\end{figure*}

\section{Formal Semantics}\label{sec:semantics}

We now explain formally how to achieve the concurrent semantics outlined in Section~\ref{sec:composing}. The state of replica $p$ is described by $A_p$, a finite partial function. The evaluation rules of the command language inspect and modify this local state $A_p$, and they do not depend on $A_q$ (the state of any other replica $q \neq p$). The only communication between replicas occurs in the evaluation of the \textsf{yield} command, which we discuss later. For now, we concentrate on the execution of commands at a single replica $p$.

\subsection{Expression Evaluation}

Figure~\ref{fig:expr-rules} gives the rules for evaluating EXPR expressions in the command language, which are evaluated in the context of the local replica state $A_p$. The \textsc{Exec} rule formalizes the assumption that commands are executed sequentially. The \textsc{Let} rule allows the program to define a local variable, which is added to the local state (the notation $A_p[\,x \,\mapsto\, \mathit{cur}\,]$ denotes a partial function that is the same as $A_p$, except that $A_p(x) = \mathit{cur}$). The corresponding \textsc{Var} rule allows the program to retrieve the value of a previously defined variable.

The following rules in Figure~\ref{fig:expr-rules} show how an expression is evaluated to a cursor, which unambiguously identifies a particular position in a JSON document by describing a path from the root of the document tree to some branch or leaf node. A cursor consists only of immutable keys and identifiers, so it can be sent over the network to another replica, where it can be used to locate the same position in the document.

For example,
\[ \mathsf{cursor}(\langle \mathsf{mapT}(\mathsf{doc}), \mathsf{listT}(\text{``shopping''}) \rangle,\, \mathit{id}_1) \]
is a cursor representing the list element \verb|"eggs"| in Figure~\ref{fig:make-doc}, assuming that $\mathit{id}_1$ is the unique identifier of the operation that inserted this list element (we will discuss these identifiers in Section~\ref{sec:lamport-ts}). The cursor can be interpreted as a path through the local replica state structure $A_p$, read from left to right: starting from the \textsf{doc} map at the root, it traverses through the map entry ``shopping'' of type \textsf{listT}, and finishes with the list element that has identifier $\mathit{id}_1$.

In general, $\mathsf{cursor}(\langle k_1, \dots, k_{n-1} \rangle,\, k_n)$ consists of a (possibly empty) vector of keys $\langle k_1, \dots, k_{n-1} \rangle$, and a final key $k_n$ which is always present. $k_n$ can be thought of as the final element of the vector, with the distinction that it is not tagged with a datatype, whereas the elements of the vector are tagged with the datatype of the branch node being traversed, either \textsf{mapT} or \textsf{listT}.

The \textsc{Doc} rule in Figure~\ref{fig:expr-rules} defines the simplest cursor $\mathsf{cursor}(\langle\rangle,\, \mathsf{doc})$, referencing the root of the document using the special atom \textsf{doc}. The \textsc{Get} rule navigates a cursor to a particular key within a map. For example, the expression \verb|doc.get("shopping")| evaluates to $\mathsf{cursor}(\langle \mathsf{mapT}(\mathsf{doc}) \rangle,\, \text{``shopping''})$ by applying the \textsc{Doc} and \textsc{Get} rules. Note that the expression \verb|doc.get(...)| implicitly asserts that \textsf{doc} is of type \textsf{mapT}, and this assertion is encoded in the cursor.

The rules $\textsc{Idx}_{1 \dots 5}$ define how to evaluate the expression \verb|.idx(|$n$\verb|)|, moving the cursor to a particular element of a list. $\textsc{Idx}_1$ constructs a cursor representing the head of the list, and delegates to the subsequent rules to scan over the list. $\textsc{Idx}_2$ recursively descends the local state according to the vector of keys in the cursor. When the vector of keys is empty, the context $\mathit{ctx}$ is the subtree of $A_p$ that stores the list in question, and the rules $\textsc{Idx}_{3,4,5}$ iterate over that list until the desired element is found.

$\textsc{Idx}_5$ terminates the iteration when the index reaches zero, while $\textsc{Idx}_3$ moves to the next element and decrements the index, and $\textsc{Idx}_4$ skips over list elements that are marked as deleted. The structure resembles a linked list: each list element has a unique identifier $k$, and the partial function representing local state maps $\mathsf{next}(k)$ to the ID of the list element that follows $k$.

Deleted elements are never removed from the linked list structure, but only marked as deleted (they become so-called \emph{tombstones}). To this end, the local state maintains a \emph{presence set} $\mathsf{pres}(k)$ for the list element with ID $k$, which is the set of all operations that have asserted the existence of this list element. When a list element is deleted, the presence set is set to the empty set, which marks it as deleted; however, a concurrent operation that references the list element can cause the presence set to be come non-empty again (leading to the situations in Figures~\ref{fig:map-remove} and~\ref{fig:todo-item}). Rule $\textsc{Idx}_4$ handles list elements with an empty presence set by moving to the next list element without decrementing the index (i.e., not counting them as list elements).

The $\textsc{Keys}_{1,2,3}$ rules allow the application to inspect the set of keys in a map. This set is determined by examining the local state, and excluding any keys for which the presence set is empty (indicating that the key has been deleted).

Finally, the $\textsc{Val}_{1,2,3}$ rules allow the application to read the contents of a register at a particular cursor position, using a similar recursive rule structure as the \textsc{Idx} rules. A register is expressed using the \textsf{regT} type annotation in the local state. Although a replica can only assign a single value to a register, a register can nevertheless contain multiple values if multiple replicas concurrently assign values to it.

\subsection{Generating Operations}

When commands mutate the state of the document, they generate \emph{operations} that describe the mutation. In our semantics, a command never directly modifies the local replica state $A_p$, but only generates an operation. That operation is then immediately applied to $A_p$ so that it takes effect locally, and the same operation is also asynchronously broadcast to the other replicas.

\subsubsection{Lamport Timestamps}\label{sec:lamport-ts}

Every operation in our model is given a unique identifier, which is used in the local state and in cursors. Whenever an element is inserted into a list, or a value is assigned to a register, the new list element or register value is identified by the identifier of the operation that created it.

In order to generate globally unique operation identifiers without requiring synchronous coordination between replicas we use Lamport timestamps~\cite{Lamport:1978jq}. A Lamport timestamp is a pair $(c, p)$ where $p \in \mathrm{ReplicaID}$ is the unique identifier of the replica on which the edit is made (for example, a hash of its public key), and $c \in \mathbb{N}$ is a counter that is stored at each replica and incremented for every operation. Since each replica generates a strictly monotonically increasing sequence of counter values $c$, the pair $(c, p)$ is unique.

If a replica receives an operation with a counter value $c$ that is greater than the locally stored counter value, the local counter is increased to the value of the incoming counter. This ensures that if operation $o_1$ causally happened before $o_2$ (that is, the replica that generated $o_2$ had received and processed $o_1$ before $o_2$ was generated), then $o_2$ must have a greater counter value than $o_1$. Only concurrent operations can have equal counter values.

We can thus define a total ordering $<$ for Lamport timestamps:
\[ (c_1, p_1) < (c_2, p_2) \;\text{ iff }\; (c_1 < c_2) \vee (c_1 = c_2 \wedge p_1 < p_2). \]
If one operation happened before another, this ordering is consistent with causality (the earlier operation has a lower timestamp). If two operations are concurrent, their order according to $<$ is arbitrary but deterministic. This ordering property is important for our definition of the semantics of ordered lists.

\subsubsection{Operation Structure}

An operation is a tuple of the form
\begin{alignat*}{3}
& \mathsf{op}( \\
&& \mathit{id} &: \mathbb{N} \times \mathrm{ReplicaID}, \\
&& \mathit{deps} &: \mathcal{P}(\mathbb{N} \times \mathrm{ReplicaID}), \\
&& \mathit{cur} &: \mathsf{cursor}(\langle k_1, \dots, k_{n-1} \rangle,\, k_n), \\
&& \mathit{mut} &: \mathsf{insert}(v) \mid \mathsf{delete} \mid \mathsf{assign}(v) && \quad v: \mathrm{VAL} \\
& )
\end{alignat*}
where $\mathit{id}$ is the Lamport timestamp that uniquely identifies the operation, $\mathit{cur}$ is the cursor describing the position in the document being modified, and $\mathit{mut}$ is the mutation that was requested at the specified position.

$\mathit{deps}$ is the set of \emph{causal dependencies} of the operation. It is defined as follows: if operation $o_2$ was generated by replica $p$, then a causal dependency of $o_2$ is any operation $o_1$ that had already been applied on $p$ at the time when $o_2$ was generated. In this paper, we define $\mathit{deps}$ as the set of Lamport timestamps of all causal dependencies. In a real implementation, this set would become impracticably large, so a compact representation of causal history would be used instead -- for example, version vectors~\cite{ParkerJr:1983jb}, state vectors~\cite{Ellis:1989ue}, or dotted version vectors~\cite{Preguica:2012fx}. However, to avoid ambiguity in our semantics we give the dependencies as a simple set of operation IDs.

The purpose of the causal dependencies $\mathit{deps}$ is to impose a partial ordering on operations: an operation can only be applied after all operations that ``happened before'' it have been applied. In particular, this means that the sequence of operations generated at one particular replica will be applied in the same order at every other replica. Operations that are concurrent (i.e., where there is no causal dependency in either direction) can be applied in any order.

\subsubsection{Semantics of Generating Operations}

\begin{figure*}
\centering
\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathit{cur}$}
\AxiomC{$\mathit{val}: \mathrm{VAL}$}
\AxiomC{$A_p,\, \mathsf{makeOp}(\mathit{cur}, \mathsf{assign}(\mathit{val})) \evalto A_p'$}
\LeftLabel{\textsc{Make-Assign}}
\TrinaryInfC{$A_p,\, \mathit{expr} \,\text{ := }\, \mathit{val} \evalto A_p'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathit{cur}$}
\AxiomC{$\mathit{val}: \mathrm{VAL}$}
\AxiomC{$A_p,\, \mathsf{makeOp}(\mathit{cur}, \mathsf{insert}(\mathit{val})) \evalto A_p'$}
\LeftLabel{\textsc{Make-Insert}}
\TrinaryInfC{$A_p,\, \mathit{expr}.\mathsf{insertAfter}(\mathit{val}) \evalto A_p'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{expr} \evalto \mathit{cur}$}
\AxiomC{$A_p,\, \mathsf{makeOp}(\mathit{cur}, \mathsf{delete}) \evalto A_p'$}
\LeftLabel{\textsc{Make-Delete}}
\BinaryInfC{$A_p,\, \mathit{expr}.\mathsf{delete} \evalto A_p'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathit{ctr} = \mathrm{max}(\{0\} \,\cup\, \{ c_i \mid (c_i, p_i) \in A_p(\mathsf{ops}) \}$}
\AxiomC{$A_p,\, \mathsf{apply}(\mathsf{op}((\mathit{ctr} + 1, p), A_p(\mathsf{ops}),
    \mathit{cur}, \mathit{mut})) \evalto A_p'$}
\LeftLabel{\textsc{Make-Op}}
\BinaryInfC{$A_p,\, \mathsf{makeOp}(\mathit{cur}, \mathit{mut}) \evalto A_p'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$A_p,\, \mathit{op} \evalto A_p'$}
\LeftLabel{\textsc{Apply-Local}}
\UnaryInfC{$A_p,\, \mathsf{apply}(\mathit{op}) \evalto A_p'[\,
    \mathsf{queue} \,\mapsto\, A_p'(\mathsf{queue}) \,\cup\, \{\mathit{op}\},\;
    \mathsf{ops} \,\mapsto\, A_p'(\mathsf{ops}) \,\cup\, \{\mathit{op.id}\}\,]$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathit{op} \in A_p(\mathsf{recv})$}
\AxiomC{$\mathit{op.id} \notin A_p(\mathsf{ops})$}
\AxiomC{$\mathit{op.deps} \subseteq A_p(\mathsf{ops})$}
\AxiomC{$A_p,\, \mathit{op} \evalto A_p'$}
\LeftLabel{\textsc{Apply-Remote}}
\QuaternaryInfC{$A_p,\, \mathsf{yield} \evalto
    A_p'[\,\mathsf{ops} \,\mapsto\, A_p'(\mathsf{ops}) \,\cup\, \{\mathit{op.id}\}\,]$}
\end{prooftree}

\begin{prooftree}
\AxiomC{}
\LeftLabel{\textsc{Send}}
\UnaryInfC{$A_p,\, \mathsf{yield} \evalto
    A_p[\,\mathsf{send} \,\mapsto\, A_p(\mathsf{send}) \,\cup\, A_p(\mathsf{queue})\,]$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$q: \mathrm{ReplicaID}$}
\LeftLabel{\textsc{Recv}}
\UnaryInfC{$A_p,\, \mathsf{yield} \evalto
    A_p[\,\mathsf{recv} \,\mapsto\, A_p(\mathsf{recv}) \,\cup\, A_q(\mathsf{send})\,]$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$A_p,\, \mathsf{yield} \evalto A_p'$}
\AxiomC{$A_p',\, \mathsf{yield} \evalto A_p''$}
\LeftLabel{\textsc{Yield}}
\BinaryInfC{$A_p,\, \mathsf{yield} \evalto A_p''$}
\end{prooftree}
\caption{Rules for generating, sending, and receiving operations.}
\label{fig:send-recv}
\end{figure*}

The evaluation rules for commands are given in Figure~\ref{fig:send-recv}. The \textsc{Make-Assign}, \textsc{Make-Insert} and \textsc{Make-Delete} rules define how these respective commands mutate the document: all three delegate to the \textsc{Make-Op} rule to generate and apply the operation. \textsc{Make-Op} generates a new Lamport timestamp by choosing a counter value that is 1 greater than any existing counter in $A_p(\mathsf{ops})$, the set of all operation IDs that have been applied to replica $p$.

\textsc{Make-Op} constructs an \textsf{op()} tuple of the form described above, and delegates to the \textsc{Apply-Local} rule to process the operation. \textsc{Apply-Local} does three things: it evaluates the operation to produce a modified local state $A_p'$, it adds the operation to the queue of generated operations $A_p(\mathsf{queue})$, and it adds the ID to the set of processed operations $A_p(\mathsf{ops})$.

The \textsf{yield} command, inspired by Burckhardt et al.~\cite{Burckhardt:2012jy}, performs network communication: sending and receiving operations to and from other replicas, and applying operations from remote replicas. The rules \textsc{Apply-Remote}, \textsc{Send}, \textsc{Recv} and \textsc{Yield} define the semantics of \textsf{yield}. Since any of these rules can be used to evaluate \textsf{yield}, their effect is nondeterministic, which models the asynchronicity of the network: a message sent by one replica arrives at another replica at some arbitrarily later point in time, and there is no message ordering guarantee in the network.

The \textsc{Send} rule takes any operations that were placed in $A_p(\mathsf{queue})$ by \textsc{Apply-Local} and adds them to a send buffer $A_p(\mathsf{send})$. Correspondingly, the \textsc{Recv} rule takes operations in the send buffer of replica $q$ and adds them to the receive buffer $A_p(\mathsf{recv})$ of replica $p$. This is the only rule that involves more than one replica, and it models all network communication.

Once an operation appears in the receive buffer $A_p(\mathsf{recv})$, the rule \textsc{Apply-Remote} may apply. Under the preconditions that the operation has not already been processed and that its causal dependencies are satisfied, \textsc{Apply-Remote} applies the operation in the same way as \textsc{Apply-Local}, and adds its ID to the set of processed operations $A_p(\mathsf{ops})$.

The actual document modifications are performed by applying the operations, which we discuss next.

\subsection{Applying Operations}

\begin{sidewaysfigure*}
\begin{prooftree}
\AxiomC{$\mathit{ctx},\, k_1 \evalto \mathit{child}$}
\AxiomC{$\mathit{child},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle k_2, \dots, k_{n-1} \rangle,\, k_n), \mathit{mut}) \evalto \mathit{child}'$}
\AxiomC{$\mathit{ctx},\, \mathsf{addId}(k_1, \mathit{id}, \mathit{mut}) \evalto \mathit{ctx}'$}
\LeftLabel{\textsc{Descend}}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle k_1, k_2, \dots, k_{n-1} \rangle,\, k_n), \mathit{mut}) \evalto
    \mathit{ctx}' [\, k_1 \,\mapsto\, \mathit{child}' \,]$}
\end{prooftree}\vspace{6pt}

\begin{center}
\AxiomC{$k \in \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{\textsc{Child-Get}}
\UnaryInfC{$\mathit{ctx},\, k \evalto \mathit{ctx}(k)$}
\DisplayProof\hspace{3em}
%
\AxiomC{$\mathsf{mapT}(k) \notin \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{\textsc{Child-Map}}
\UnaryInfC{$\mathit{ctx},\, \mathsf{mapT}(k) \evalto \{\}$}
\DisplayProof\hspace{3em}
%
\AxiomC{$\mathsf{listT}(k) \notin \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{\textsc{Child-List}}
\UnaryInfC{$\mathit{ctx},\, \mathsf{listT}(k) \evalto
    \{\,\mathsf{next}(\mathsf{head}) \,\mapsto\, \mathsf{tail} \,\}$}
\DisplayProof\proofSkipAmount
\end{center}\vspace{6pt}

\begin{center}
\AxiomC{$\mathsf{regT}(k) \notin \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{\textsc{Child-Reg}}
\UnaryInfC{$\mathit{ctx},\, \mathsf{regT}(k) \evalto \{\}$}
\DisplayProof\hspace{3em}
%
\AxiomC{$\mathsf{pres}(k) \in \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{$\textsc{Presence}_1$}
\UnaryInfC{$\mathit{ctx},\, \mathsf{pres}(k) \evalto \mathit{ctx}(\mathsf{pres}(k))$}
\DisplayProof\hspace{3em}
%
\AxiomC{$\mathsf{pres}(k) \notin \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{$\textsc{Presence}_2$}
\UnaryInfC{$\mathit{ctx},\, \mathsf{pres}(k) \evalto \{\}$}
\DisplayProof\proofSkipAmount
\end{center}\vspace{6pt}

\begin{center}
\AxiomC{$\mathit{mut} \not= \mathsf{delete}$}
\AxiomC{$k_\mathit{tag} \in \{\mathsf{mapT}(k), \mathsf{listT}(k), \mathsf{regT}(k)\}$}
\AxiomC{$\mathit{ctx},\, \mathsf{pres}(k) \evalto \mathit{pres}$}
\LeftLabel{$\textsc{Add-ID}_1$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{addId}(k_\mathit{tag}, \mathit{id}, \mathit{mut}) \evalto
    \mathit{ctx}[\, \mathsf{pres}(k) \,\mapsto\, \mathit{pres} \,\cup\, \{\mathit{id}\} \,]$}
\DisplayProof\hspace{3em}
%
\AxiomC{$\mathit{mut} = \mathsf{delete}$}
\LeftLabel{$\textsc{Add-ID}_2$}
\UnaryInfC{$\mathit{ctx},\, \mathsf{addId}(k_\mathit{tag}, \mathit{id}, \mathit{mut}) \evalto \mathit{ctx}$}
\DisplayProof\proofSkipAmount
\end{center}\vspace{6pt}

\begin{prooftree}
\AxiomC{$\mathit{val} \not= \texttt{[]} \,\wedge\, \mathit{val} \not= \texttt{\string{\string}}$}
\AxiomC{$\mathit{ctx},\, \mathsf{clear}(\mathit{deps}, \mathsf{regT}(k)) \evalto \mathit{ctx}',\, \mathit{pres}$}
\AxiomC{$\mathit{ctx}',\, \mathsf{addId}(\mathsf{regT}(k), \mathit{id}, \mathsf{assign}(\mathit{val}))
    \evalto \mathit{ctx}''$}
\AxiomC{$\mathit{ctx}'',\, \mathsf{regT}(k) \evalto \mathit{child}$}
\LeftLabel{\textsc{Assign}}
\QuaternaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, k), \mathsf{assign}(\mathit{val})) \evalto
    \mathit{ctx}''[\, \mathsf{regT}(k) \,\mapsto\,
    \mathit{child}[\, \mathit{id} \,\mapsto\, \mathit{val} \,]\,]$}
\end{prooftree}\vspace{6pt}

\begin{prooftree}
\AxiomC{$\mathit{val} = \texttt{\string{\string}}$}
\AxiomC{$\mathit{ctx},\, \mathsf{clearElem}(\mathit{deps}, k) \evalto \mathit{ctx}',\, \mathit{pres}$}
\AxiomC{$\mathit{ctx}',\, \mathsf{addId}(\mathsf{mapT}(k), \mathit{id}, \mathsf{assign}(\mathit{val}))
    \evalto \mathit{ctx}''$}
\AxiomC{$\mathit{ctx}'',\, \mathsf{mapT}(k) \evalto \mathit{child}$}
\LeftLabel{\textsc{Empty-Map}}
\QuaternaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, k), \mathsf{assign}(\mathit{val})) \evalto
    \mathit{ctx}''[\, \mathsf{mapT}(k) \,\mapsto\, \mathit{child} \,]$}
\end{prooftree}\vspace{6pt}

\begin{prooftree}
\AxiomC{$\mathit{val} = \texttt{[]}$}
\AxiomC{$\mathit{ctx},\, \mathsf{clearElem}(\mathit{deps}, k) \evalto \mathit{ctx}',\, \mathit{pres}$}
\AxiomC{$\mathit{ctx}',\, \mathsf{addId}(\mathsf{listT}(k), \mathit{id}, \mathsf{assign}(\mathit{val}))
    \evalto \mathit{ctx}''$}
\AxiomC{$\mathit{ctx}'',\, \mathsf{listT}(k) \evalto \mathit{child}$}
\LeftLabel{\textsc{Empty-List}}
\QuaternaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, k), \mathsf{assign}(\mathit{val})) \evalto
    \mathit{ctx}''[\, \mathsf{listT}(k) \,\mapsto\, \mathit{child} \,]$}
\end{prooftree}\vspace{6pt}

\begin{prooftree}
\AxiomC{$\mathit{ctx}(\mathsf{next}(\mathit{prev})) = \mathit{next}$}
\AxiomC{$\mathit{next} < \mathit{id} \,\vee\, \mathit{next} = \mathsf{tail}$}
\AxiomC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, \mathit{id}), \mathsf{assign}(\mathit{val})) \evalto \mathit{ctx}'$}
\LeftLabel{$\textsc{Insert}_1$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, \mathit{prev}), \mathsf{insert}(\mathit{val})) \evalto
    \mathit{ctx}'[\,\mathsf{next}(\mathit{prev}) \,\mapsto\, \mathit{id},\;
    \mathsf{next}(\mathit{id}) \,\mapsto\, \mathit{next}\,]$}
\end{prooftree}\vspace{6pt}

\begin{prooftree}
\AxiomC{$\mathit{ctx}(\mathsf{next}(\mathit{prev})) = \mathit{next}$}
\AxiomC{$\mathit{id} < \mathit{next}$}
\AxiomC{$ctx,\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, \mathit{next}), \mathsf{insert}(\mathit{val})) \evalto \mathit{ctx}'$}
\LeftLabel{$\textsc{Insert}_2$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, \mathit{prev}), \mathsf{insert}(\mathit{val})) \evalto \mathit{ctx}'$}
\end{prooftree}
\caption{Rules for applying insertion and assignment operations to update the state of a replica.}\label{fig:operation-rules}
\end{sidewaysfigure*}

Figure~\ref{fig:operation-rules} gives the rules that apply an operation $\mathsf{op}$ to a context $\mathit{ctx}$, producing an updated context $\mathit{ctx}'$. The context is initially the replica state $A_p$, but may refer to subtrees of the state as rules are recursively applied. These rules are used by \textsc{Apply-Local} and \textsc{Apply-Remote} to perform the state updates on a document.

When the operation cursor's vector of keys is non-empty, the \textsc{Descend} rule first applies. It recursively descends the document tree by following the path described by the keys. If the tree node already exists in the local replica state, \textsc{Child-Get} finds it, otherwise \textsc{Child-Map} and \textsc{Child-List} create an empty map or list respectively.

The \textsc{Descend} rule also invokes $\textsc{Add-ID}_{1,2}$ at each tree node along the path described by the cursor, adding the operation ID to the presence set $\mathsf{pres}(k)$ to indicate that the subtree includes a mutation made by this operation.

The remaining rules in Figure~\ref{fig:operation-rules} apply when the vector of keys in the cursor is empty, which is the case when descended to the context of the tree node to which the mutation applies. The \textsc{Assign} rule handles assignment of a primitive value to a register, \textsc{Empty-Map} handles assignment where the value is the empty map literal \verb|{}|, and \textsc{Empty-List} handles assignment of the empty list \verb|[]|. These three rules for \textsf{assign} have a similar structure: first clearing the prior value at the cursor (as discussed in the next section), then adding the operation ID to the presence set, and finally incorporating the new value into the tree of local state.

The $\textsc{Insert}_{1,2}$ rules handle insertion of a new element into an ordered list. In this case, the cursor refers to the list element $\mathit{prev}$, and the new element is inserted after that position in the list. $\textsc{Insert}_1$ performs the insertion by manipulating the linked list structure. $\textsc{Insert}_2$ handles the case of multiple replicas concurrently inserting list elements at the same position, and uses the ordering relation $<$ on Lamport timestamps to consistently determine the insertion point. Our approach for handling insertions is based on the RGA algorithm~\cite{Roh:2011dw}. We show later that these rules ensure all replicas converge towards the same state.

\subsubsection{Clearing Prior State}

\begin{figure*}
\begin{prooftree}
\AxiomC{$\mathit{ctx},\, \mathsf{clearElem}(\mathit{deps}, k) \evalto \mathit{ctx}',\, \mathit{pres}$}
\LeftLabel{\textsc{Delete}}
\UnaryInfC{$\mathit{ctx},\, \mathsf{op}(\mathit{id}, \mathit{deps},
    \mathsf{cursor}(\langle\rangle,\, k), \mathsf{delete}) \evalto \mathit{ctx}'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathit{ctx},\, \mathsf{clearAny}(\mathit{deps}, k) \evalto \mathit{ctx}', \mathit{pres}_1$}
\AxiomC{$\mathit{ctx}',\, \mathsf{pres}(k) \evalto \mathit{pres}_2$}
\AxiomC{$\mathit{pres}_3 = \mathit{pres}_1 \,\cup\, \mathit{pres}_2 \setminus \mathit{deps}$}
\LeftLabel{\textsc{Clear-Elem}}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{clearElem}(\mathit{deps}, k) \evalto
    \mathit{ctx}' [\, \mathsf{pres}(k) \,\mapsto\, \mathit{pres}_3 \,],\, \mathit{pres}_3$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\begin{matrix}
    \mathit{ctx},\, \mathsf{clear}(\mathit{deps}, \mathsf{mapT}(k)) \\
    \evalto \mathit{ctx}_1,\, \mathit{pres}_1 \end{matrix}$}
\AxiomC{$\begin{matrix}
    \mathit{ctx}_1,\, \mathsf{clear}(\mathit{deps}, \mathsf{listT}(k)) \\
    \evalto \mathit{ctx}_2,\, \mathit{pres}_2 \end{matrix}$}
\AxiomC{$\begin{matrix}
    \mathit{ctx}_2,\, \mathsf{clear}(\mathit{deps}, \mathsf{regT}(k)) \\
    \evalto \mathit{ctx}_3,\, \mathit{pres}_3 \end{matrix}$}
\LeftLabel{\textsc{Clear-Any}}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{clearAny}(\mathit{deps}, k) \evalto
    \mathit{ctx}_3,\, \mathit{pres}_1 \,\cup\, \mathit{pres}_2 \,\cup\, \mathit{pres}_3$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$k \notin \mathrm{dom}(\mathit{ctx})$}
\LeftLabel{\textsc{Clear-None}}
\UnaryInfC{$\mathit{ctx},\, \mathsf{clear}(\mathit{deps}, k) \evalto \mathit{ctx},\, \{\}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathsf{regT}(k) \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{concurrent} = \{ \mathit{id} \mapsto v \mid
    (\mathit{id} \mapsto v) \in \mathit{ctx}(\mathsf{regT}(k))
    \,\wedge\, \mathit{id} \notin \mathit{deps} \}$}
\LeftLabel{\textsc{Clear-Reg}}
\BinaryInfC{$\mathit{ctx},\, \mathsf{clear}(\mathit{deps}, \mathsf{regT}(k)) \evalto
    \mathit{ctx}[\, \mathsf{regT}(k) \,\mapsto\, \mathit{concurrent} \,],\, \mathrm{dom}(\mathit{concurrent})$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathsf{mapT}(k) \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{ctx}(\mathsf{mapT}(k)),\, \mathsf{clearMap}(\mathit{deps}, \{\}) \evalto
    \mathit{cleared},\, \mathit{pres}$}
\LeftLabel{$\textsc{Clear-Map}_1$}
\BinaryInfC{$\mathit{ctx},\, \mathsf{clear}(\mathit{deps}, \mathsf{mapT}(k)) \evalto
    \mathit{ctx} [\, \mathsf{mapT}(k) \,\mapsto\, \mathit{cleared} \,],\, \mathit{pres}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\begin{matrix}
    k \in \mathrm{keys}(\mathit{ctx}) \\
    \wedge\, k \notin \mathit{done} \end{matrix}$}
\AxiomC{$\begin{matrix}
    \mathit{ctx},\, \mathsf{clearElem}(\mathit{deps}, k) \\
    \evalto \mathit{ctx}', \mathit{pres}_1 \end{matrix}$}
\AxiomC{$\begin{matrix}
    \mathit{ctx}',\, \mathsf{clearMap}(\mathit{deps}, \mathit{done} \cup \{k\}) \\
    \evalto \mathit{ctx}'',\, \mathit{pres}_2 \end{matrix}$}
\LeftLabel{$\textsc{Clear-Map}_2$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{clearMap}(\mathit{deps}, \mathit{done})
    \evalto \mathit{ctx}'',\, \mathit{pres}_1 \,\cup\, \mathit{pres}_2$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathit{done} = \mathrm{keys}(\mathit{ctx})$}
\LeftLabel{$\textsc{Clear-Map}_3$}
\UnaryInfC{$\mathit{ctx},\, \mathsf{clearMap}(\mathit{deps}, \mathit{done}) \evalto \mathit{ctx},\, \{\}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\mathsf{listT}(k) \in \mathrm{dom}(\mathit{ctx})$}
\AxiomC{$\mathit{ctx}(\mathsf{listT}(k)),\, \mathsf{clearList}(\mathit{deps}, \mathsf{head})
    \evalto \mathit{cleared},\, \mathit{pres}$}
\LeftLabel{$\textsc{Clear-List}_1$}
\BinaryInfC{$\mathit{ctx},\, \mathsf{clear}(\mathit{deps}, \mathsf{listT}(k)) \evalto
    \mathit{ctx}[\, \mathsf{listT}(k) \,\mapsto\, \mathit{cleared} \,],\, \mathit{pres}$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\begin{matrix}
    k \not= \mathsf{tail} \,\wedge\\
    \mathit{ctx}(\mathsf{next}(k)) = \mathit{next} \end{matrix}$}
\AxiomC{$\begin{matrix}
    \mathit{ctx},\, \mathsf{clearElem}(\mathit{deps}, k) \\
    \evalto \mathit{ctx}', \mathit{pres}_1 \end{matrix}$}
\AxiomC{$\begin{matrix}
    \mathit{ctx}',\, \mathsf{clearList}(\mathit{deps}, \mathit{next}) \\
    \evalto \mathit{ctx}'', \mathit{pres}_2 \end{matrix}$}
\LeftLabel{$\textsc{Clear-List}_2$}
\TrinaryInfC{$\mathit{ctx},\, \mathsf{clearList}(\mathit{deps}, k) \evalto
    \mathit{ctx}'',\, \mathit{pres}_1 \,\cup\, \mathit{pres}_2$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$k = \mathsf{tail}$}
\LeftLabel{$\textsc{Clear-List}_3$}
\UnaryInfC{$\mathit{ctx},\, \mathsf{clearList}(\mathit{deps}, k) \evalto \mathit{ctx},\, \{\}$}
\end{prooftree}
\caption{Rules for applying deletion operations to update the state of a replica.}\label{fig:clear-rules}
\end{figure*}

Assignment and deletion operations require that prior state (the value being overwritten or deleted) is cleared, while also ensuring that concurrent modifications are not lost, as illustrated in Figure~\ref{fig:map-remove}. The rules to handle this clearing process are given in Figure~\ref{fig:clear-rules}. Intuitively, the effect of clearing something is to reset it to its empty state by undoing any operations that causally precede the current operation, while leaving the effect of any concurrent operations untouched.

A \textsf{delete} operation can be used to delete either an element from an ordered list or a key from a map, depending on what the cursor refers to. The \textsc{Delete} rule shows how this operation is evaluated by delegating to \textsc{Clear-Elem}. In turn, \textsc{Clear-Elem} uses \textsc{Clear-Any} to clear out any data with a given key, regardless of whether it is of type \textsf{mapT}, \textsf{listT} or \textsf{regT}, and also updates the presence set to include any nested operation IDs, but exclude any operations in $\mathit{deps}$.

The premises of \textsc{Clear-Any} are satisfied by $\textsc{Clear-Map}_1$, $\textsc{Clear-List}_1$ and \textsc{Clear-Reg} if the respective key appears in $\mathit{ctx}$, or by \textsc{Clear-None} (which does nothing) if the key is absent.

As defined by the \textsc{Assign} rule, a register maintains a mapping from operation IDs to values. \textsc{Clear-Reg} updates a register by removing all operation IDs that appear in $\mathit{deps}$ (i.e., which causally precede the clearing operation), but retaining all operation IDs that do not appear in $\mathit{deps}$ (from assignment operations that are concurrent with the clearing operation).

Clearing maps and lists takes a similar approach: each element of the map or list is recursively cleared using \textsf{clearElem}, and presence sets are updated to exclude $\mathit{deps}$. Thus, any list elements or map entries whose modifications causally precede the clearing operation will end up with empty presence sets, and thus be considered deleted. Any map or list elements containing operations that are concurrent with the clearing operation are preserved.

\subsection{Convergence}\label{sec:convergence}

As outlined in Section~\ref{sec:intro-replication}, we require that all replicas automatically converge towards the same state -- a key requirement of a CRDT. We now formalize this notion, and show that the rules in Figures~\ref{fig:expr-rules} to~\ref{fig:clear-rules} satisfy this requirement.

\begin{definition}[valid execution]\label{def:valid-exec}
A \emph{valid execution} is a set of operations generated by a set of replicas $\{p_1, \dots, p_k\}$, each reducing a sequence of commands $\langle \mathit{cmd}_1 \mathbin{;} \dots \mathbin{;} \mathit{cmd}_n \rangle$ without getting stuck.
\end{definition}

A reduction gets stuck if there is no application of rules in which all premises are satisfied. For example, the $\textsc{Idx}_{3,4}$ rules in Figure~\ref{fig:expr-rules} get stuck if $\mathsf{idx}(n)$ tries to iterate past the end of a list, which would happen if $n$ is greater than the number of non-deleted elements in the list; in a real implementation this would be a runtime error. By constraining valid executions to those that do not get stuck, we ensure that operations only refer to list elements that actually exist.

Note that it is valid for an execution to never perform any network communication, either because it never invokes the \textsf{yield} command, or because the nondeterministic execution of \textsf{yield} never applies the \textsc{Recv} rule. We need only a replica's local state to determine whether reduction gets stuck.

\begin{definition}[history]\label{def:history}
A \emph{history} is a sequence of operations in the order it was applied at one particular replica $p$ by application of the rules \textsc{Apply-Local} and \textsc{Apply-Remote}.
\end{definition}

Since the evaluation rules sequentially apply one operation at a time at a given replica, the order is well-defined. Even if two replicas $p$ and $q$ applied the same set of operations, i.e. if $A_p(\mathsf{ops}) = A_q(\mathsf{ops})$, they may have applied any concurrent operations in a different order. Due to the premise $\mathit{op.deps} \subseteq A_p(\mathsf{ops})$ in \textsc{Apply-Remote}, histories are consistent with causality: if an operation has causal dependencies, it appears at some point after those dependencies in the history.

\begin{definition}[document state]\label{def:doc-state}
The \emph{document state} of a replica $p$ is the subtree of $A_p$ containing the document: that is, $A_p(\mathsf{mapT}(\mathsf{doc}))$ or $A_p(\mathsf{listT}(\mathsf{doc}))$ or $A_p(\mathsf{regT}(\mathsf{doc}))$, whichever is defined.
\end{definition}

$A_p$ contains variables defined with \textsf{let}, which are local to one replica, and not part of the replicated state. The definition of document state excludes these variables.

\begin{convergence-thm}
For any two replicas $p$ and $q$ that participated in a valid execution, if $A_p(\mathsf{ops}) = A_q(\mathsf{ops})$, then $p$ and $q$ have the same document state.
\end{convergence-thm}

This theorem is proved in the appendix. It formalizes the safety property of convergence: if two replicas have processed the same set of operations, possibly in a different order, then they are in the same state. In combination with a liveness property, namely that every replica eventually processes all operations, we obtain the desired notion of convergence: all replicas eventually end up in the same state.

The liveness property depends on assumptions of replicas invoking \textsf{yield} sufficiently often, and all nondeterministic rules for \textsf{yield} being chosen fairly. We will not formalize the liveness property in this paper, but assert that it can usually be provided in practice, as network interruptions are usually of finite duration.

\section{Conclusions and Further Work}

In this paper we demonstrated how to compose CRDTs for ordered lists, maps and registers into a compound CRDT with a JSON data model. It supports arbitrarily nested lists and maps, and it allows replicas to make arbitrary changes to the data without waiting for network communication. Replicas asynchronously send mutations to other replicas in the form of operations. Concurrent operations are commutative, which ensures that replicas converge towards the same state without requiring application-specific conflict resolution logic.

This work focused on the formal semantics of the JSON CRDT, represented as a mathematical model. We are also working on a practical implementation of the algorithm, and will report on its performance characteristics in follow-on work.

Our principle of not losing input due to concurrent modifications appears at first glance to be reasonable, but as illustrated in Figure~\ref{fig:todo-item}, it leads to merged document states that may be surprising to application programmers who are more familiar with sequential programs. Further work will be needed to understand the expectations of application programmers, and to design data structures that are minimally surprising under concurrent modification. It may turn out that a schema language will be required to support more complex applications. A schema language could also support semantic annotations, such as indicating that a number should be treated as a counter rather than a register.

The CRDT defined in this paper supports insertion, deletion and assignment operations. In addition to these, it would be useful to support a \emph{move} operation (to change the order of elements in an ordered list, or to move a subtree from one position in a document to another) and an \emph{undo} operation. Moreover, garbage collection (tombstone removal) is required in order to prevent unbounded growth of the data structure. We plan to address these missing features in future work.

\section*{Acknowledgements}

This research was supported by a grant from The Boeing Company. Thank you to Dominic Orchard, Diana Vasile, and the anonymous reviewers for comments that improved this paper.

\bibliographystyle{IEEEtran}
\bibliography{references}{}
\vfill

\begin{IEEEbiography}[{\includegraphics[width=1in]{mk428.jpg}}]{Martin Kleppmann}
is a Research Associate in the Computer Laboratory at the University of Cambridge. His current research project, TRVE Data, is working towards better security and privacy in cloud applications by applying end-to-end encryption to collaboratively editable application data. His book \emph{Designing Data-Intensive Applications} was published by O'Reilly Media in 2017. Previously, he worked as a software engineer and entrepreneur at several internet companies, including Rapportive and LinkedIn.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in]{arb33.jpg}}]{Alastair R. Beresford}
is a Senior Lecturer in the Computer Laboratory at the University of Cambridge. His research work explores the security and privacy of large-scale distributed systems, with a particular focus on networked mobile devices such as smartphones, tablets and laptops. He looks at the security and privacy of the devices themselves, as well as the security and privacy problems induced by the interaction between mobile devices and cloud-based Internet services.
\end{IEEEbiography}

\ifincludeappendix
\clearpage
\appendix[Proof of Convergence]\label{sec:proof}

\begin{theorem}\label{thm:convergence}
For any two replicas $p$ and $q$ that participated in a valid execution, if $A_p(\mathsf{ops}) = A_q(\mathsf{ops})$, then $p$ and $q$ have the same document state.
\end{theorem}

\begin{proof}
Consider the histories $H_p$ and $H_q$ at $p$ and $q$ respectively (see Definition~\ref{def:history}). The rules \textsc{Apply-Local} and \textsc{Apply-Remote} maintain the invariant that an operation is added to $A_p(\mathsf{ops})$ or $A_q(\mathsf{ops})$ if and only if it was applied to the document state at $p$ or $q$. Thus, $A_p(\mathsf{ops}) = A_q(\mathsf{ops})$ iff $H_p$ and $H_q$ contain the same set of operations (potentially ordered differently).

The history $H_p$ at replica $p$ is a sequence of $n$ operations: $H_p = \langle o_1, \dots, o_n \rangle$, and the document state at $p$ is derived from $H_p$ by starting in the empty state and applying the operations in order. Likewise, the document state at $q$ is derived from $H_q$, which is a permutation of $H_p$. Both histories must be consistent with causality, i.e. for all $i$ with $1 \le i \le n$, we require $o_i.\mathit{deps} \subseteq \{o_j.\mathit{id} \mid 1 \le j < i\}$. The causality invariant is maintained by the \textsc{Apply-*} rules.

We can prove the theorem by induction over the length of history $n$.

\emph{Base case:} An empty history with $n=0$ describes the empty document state. The empty document is always the same, and so any two replicas that have not executed any operations are by definition in the same state.

\emph{Induction step:} Given histories $H_p$ and $H_q$ of length $n$, such that $H_p = \langle o_1, \dots, o_n \rangle$ and $H_q$ is a permutation of $H_p$, and such that applying $H_p$ results in the same document state as applying $H_q$, we can construct new histories $H_p'$ and $H_q'$ of length $n+1$ by inserting a new operation $o_{n+1}$ at any causally ready position in $H_p$ or $H_q$ respectively. We must then show that for all the histories $H_p'$ and $H_q'$ constructed this way, applying the sequence of operations in order results in the same document state.

In order to prove the induction step, we examine the insertion of $o_{n+1}$ into $H_p$ and $H_q$. Each history can be split into a prefix, which is the minimal subsequence $\langle o_1, \dots, o_j \rangle$ such that $o_{n+1}.\mathit{deps} \subseteq \{o_1.\mathit{id}, \dots, o_j.\mathit{id}\}$, and a suffix, which is the remaining subsequence $\langle o_{j+1}, \dots, o_n \rangle$. The prefix contains all operations that causally precede $o_{n+1}$, and possibly some operations that are concurrent with $o_{n+1}$; the suffix contains only operations that are concurrent with $o_{n+1}$. The earliest position where $o_{n+1}$ can be inserted into the history is between the prefix and the suffix; the latest position is at the end of the suffix; or it could be inserted at any point within the suffix.

We need to show that the effect on the document state is the same, regardless of the position at which $o_{n+1}$ is inserted, and regardless of whether it is inserted into $H_p$ or $H_q$. We do this in Lemma~\ref{lem:op-commute} by showing that $o_{n+1}$ is commutative with respect to all operations in the suffix, i.e. with respect to any operations that are concurrent to $o_{n+1}$.
\end{proof}

Before we can prove the commutativity of operations, we must first define some more terms and prove some preliminary lemmas.

\begin{definition}[appearing after]
In the ordered list $\mathit{ctx}$, list element $k_j$ \emph{appears after} list element $k_1$ if there exists a (possibly empty) sequence of list elements $k_2, \dots, k_{j-1}$ such that for all $i$ with $1 \le i < j$, $\mathit{ctx}(\mathsf{next}(k_i)) = k_{i+1}$. Moreover, we say $k_j$ appears \emph{immediately after} $k_1$ if that sequence is empty, i.e. if $\mathit{ctx}(\mathsf{next}(k_1)) = k_j$.
\end{definition}

The definition of \emph{appearing after} corresponds to the order in which the \textsc{Idx} rules iterate over the list.

\begin{lemma}\label{lem:list-after}
If $k_2$ appears after $k_1$ in an ordered list, and the list is mutated according to the evaluation rules, $k_2$ also appears after $k_1$ in all later document states.
\end{lemma}

\begin{proof}
The only rule that modifies the \textsf{next} pointers in the context is $\textsc{Insert}_1$, and it inserts a new list element between two existing list elements (possibly \textsf{head} and/or \textsf{tail}). This modification preserves the appears-after relationship between any two existing list elements. Since no other rule affects the list order, appears-after is always preserved.
\end{proof}

Note that deletion of an element from a list does not remove it from the sequence of \textsf{next} pointers, but only clears its presence set $\mathsf{pres}(k)$.

\begin{lemma}\label{lem:insert-between}
If one replica inserts a list element $k_\mathit{new}$ between $k_1$ and $k_2$, i.e. if $k_\mathit{new}$ appears after $k_1$ in the list and $k_2$ appears after $k_\mathit{new}$ in the list on the source replica after applying \textsc{Apply-Local}, then $k_\mathit{new}$ appears after $k_1$ and $k_2$ appears after $k_\mathit{new}$ on every other replica where that operation is applied.
\end{lemma}

\begin{proof}
The rules for generating list operations ensure that $k_1$ is either \textsf{head} or an operation identifier, and $k_2$ is either \textsf{tail} or an operation identifier.

When the insertion operation is generated using the \textsc{Make-Op} rule, its operation identifier is given a counter value $\mathit{ctr}$ that is greater than the counter of any existing operation ID in $A_p(\mathsf{ops})$. If $k_2$ is an operation identifier, we must have $k_2 \in A_p(\mathsf{ops})$, since both \textsc{Apply-Local} and \textsc{Apply-Remote} add operation IDs to $A_p(\mathsf{ops})$ when applying an insertion. Thus, either $k_2 < k_\mathit{new}$ under the ordering relation $<$ for Lamport timestamps, or $k_2 = \mathsf{tail}$.

When the insertion operation is applied on another replica using \textsc{Apply-Remote} and $\textsc{Insert}_{1,2}$, $k_2$ appears after $k_1$ on that replica (by Lemma~\ref{lem:list-after} and causality). The cursor of the operation is $\mathsf{cursor}(\langle\dots\rangle, k_1)$, so the rules start iterating the list at $k_1$, and therefore $k_\mathit{new}$ is inserted at some position after $k_1$.

If other concurrent insertions occurred between $k_1$ and $k_2$, their operation ID may be greater than or less than $k_\mathit{new}$, and thus either $\textsc{Insert}_1$ or $\textsc{Insert}_2$ may apply. In particular, $\textsc{Insert}_2$ skips over any list elements whose Lamport timestamp is greater than $k_\mathit{new}$. However, we know that $k_2 < k_\mathit{new} \vee k_2 = \mathsf{tail}$, and so $\textsc{Insert}_1$ will apply with $\mathit{next}=k_2$ at the latest. The $\textsc{Insert}_{1,2}$ rules thus never iterate past $k_2$, and thus $k_\mathit{new}$ is never inserted at a list position that appears after $k_2$.
\end{proof}

\begin{definition}[common ancestor]\label{def:common-ancestor}
In a history $H$, the \emph{common ancestor} of two concurrent operations $o_r$ and $o_s$ is the latest document state that causally precedes both $o_r$ and $o_s$.
\end{definition}

The common ancestor of $o_r$ and $o_s$ can be defined more formally as the document state resulting from applying a sequence of operations $\langle o_1, \dots, o_j \rangle$ that is the shortest prefix of $H$ that satisfies $(o_r.\mathit{deps} \cap o_s.\mathit{deps}) \subseteq \{o_1.\mathit{id}, \dots, o_j.\mathit{id}\}$.

\begin{definition}[insertion interval]\label{def:insert-interval}
Given two concurrent operations $o_r$ and $o_s$ that insert into the same list, the \emph{insertion interval} of $o_r$ is the pair of keys $(k_r^\mathrm{before}, k_r^\mathrm{after})$ such that $o_r.\mathit{id}$ appears after $k_r^\mathrm{before}$ when $o_r$ has been applied, $k_r^\mathrm{after}$ appears after $o_r.\mathit{id}$ when $o_r$ has been applied, and $k_r^\mathrm{after}$ appears immediately after $k_r^\mathrm{before}$ in the common ancestor of $o_r$ and $o_s$. The insertion interval of $o_s$ is the pair of keys $(k_s^\mathrm{before}, k_s^\mathrm{after})$ defined similarly.
\end{definition}

It may be the case that $k_r^\mathrm{before}$ or $k_s^\mathrm{before}$ is \textsf{head}, and that $k_r^\mathrm{after}$ or $k_s^\mathrm{after}$ is \textsf{tail}.

\begin{lemma}\label{lem:insert-conflict}
For any two concurrent insertion operations $o_r, o_s$ in a history $H$, if $o_r.\mathit{cur} = o_s.\mathit{cur}$, then the order at which the inserted elements appear in the list after applying $H$ is deterministic and independent of the order of $o_r$ and $o_s$ in $H$.
\end{lemma}

\begin{proof}
Without loss of generality, assume that $o_r.\mathit{id} < o_s.\mathit{id}$ according to the ordering relation on Lamport timestamps. (If the operation ID of $o_r$ is greater than that of $o_s$, the two operations can be swapped in this proof.) We now distinguish the two possible orders of applying the operations:

\begin{enumerate}
\item $o_r$ is applied before $o_s$ in $H$. Thus, at the time when $o_s$ is applied, $o_r$ has already been applied. When applying $o_s$, since $o_r$ has a lesser operation ID, the rule $\textsc{Insert}_1$ applies with $\mathit{next} = o_r.\mathit{id}$ at the latest, so the insertion position of $o_s$ must appear before $o_r$. It is not possible for $\textsc{Insert}_2$ to skip past $o_r$.

\item $o_s$ is applied before $o_r$ in $H$. Thus, at the time when $o_r$ is applied, $o_s$ has already been applied. When applying $o_r$, the rule $\textsc{Insert}_2$ applies with $\mathit{next} = o_s.\mathit{id}$, so the rule skips past $o_s$ and inserts $o_r$ at a position after $o_s$. Moreover, any list elements that appear between $o_s.\mathit{cur}$ and $o_s$ at the time of inserting $o_r$ must have a Lamport timestamp greater than $o_s.\mathit{id}$, so $\textsc{Insert}_2$ also skips over those list elements when inserting $o_r$. Thus, the insertion position of $o_r$ must be after $o_s$.
\end{enumerate}

Thus, the insertion position of $o_r$ appears after the insertion position of $o_s$, regardless of the order in which the two operations are applied. The ordering depends only on the operation IDs, and since these IDs are fixed at the time the operations are generated, the list order is determined by the IDs.
\end{proof}

\begin{lemma}\label{lem:insert-reorder}
In an operation history $H$, an insertion operation is commutative with respect to concurrent insertion operations to the same list.
\end{lemma}

\begin{proof}
Given any two concurrent insertion operations $o_r, o_s$ in $H$, we must show that the document state does not depend on the order in which $o_r$ and $o_s$ are applied.

Either $o_r$ and $o_s$ have the same insertion interval as defined in Definition~\ref{def:insert-interval}, or they have different insertion intervals. If the insertion intervals are different, then by Lemma~\ref{lem:insert-between} the operations cannot affect each other, and so they have the same effect regardless of their order. So we need only analyze the case in which they have the same insertion interval $(k^\mathrm{before}, k^\mathrm{after})$.

If $o_r.\mathit{cur} = o_s.\mathit{cur}$, then by Lemma~\ref{lem:insert-conflict}, the operation with the greater operation ID appears first in the list, regardless of the order in which the operations are applied. If $o_r.\mathit{cur} \not= o_s.\mathit{cur}$, then one or both of the cursors must refer to a list element that appears between $k^\mathrm{before}$ and $k^\mathrm{after}$, and that did not yet exist in the common ancestor (Definition~\ref{def:common-ancestor}).

Take a cursor that differs from $k^\mathrm{before}$: the list element it refers to was inserted by a prior operation, whose cursor in turn refers to another prior operation, and so on. Following this chain of cursors for a finite number of steps leads to an operation $o_\mathrm{first}$ whose cursor refers to $k^\mathrm{before}$ (since an insertion operation always inserts at a position after the cursor).

Note that all of the operations in this chain are causally dependent on $o_\mathrm{first}$, and so they must have a Lamport timestamp greater than $o_\mathrm{first}$. Thus, we can apply the same argument as in Lemma~\ref{lem:insert-conflict}: if $\textsc{Insert}_2$ skips over the list element inserted by $o_\mathrm{first}$, it will also skip over all of the list elements that are causally dependent on it; if $\textsc{Insert}_1$ inserts a new element before $o_\mathrm{first}$, it is also inserted before the chain of operations that is based on it.

Therefore, the order of $o_r$ and $o_s$ in the final list is determined by the Lamport timestamps of the first insertions into the insertion interval after their common ancestor, in the chains of cursor references of the two operations. Since the argument above applies to all pairs of concurrent operations $o_r, o_s$ in $H$, we deduce that the final order of elements in the list depends only on the operation IDs but not the order of application, which shows that concurrent insertions to the same list are commutative.
\end{proof}

\begin{lemma}\label{lem:delete-commute}
In a history $H$, a deletion operation is commutative with respect to concurrent operations.
\end{lemma}

\begin{proof}
Given a deletion operation $o_d$ and any other concurrent operation $o_c$, we must show that the document state after applying both operations does not depend on the order in which $o_d$ and $o_c$ were applied.

The rules in Figure~\ref{fig:clear-rules} define how a deletion operation $o_d$ is applied: starting at the cursor in the operation, they recursively descend the subtree, removing $o_d.\mathit{deps}$ from the presence set $\mathsf{pres}(k)$ at all branch nodes in the subtree, and updating all registers to remove any values written by operations in $o_d.\mathit{deps}$.

If $o_c$ is an assignment or insertion operation, the \textsc{Assign} rule adds $o_c.\mathit{id}$ to the mapping from operation ID to value for a register, and the \textsc{Descend}, \textsc{Assign}, \textsc{Empty-Map} and \textsc{Empty-List} rules add $o_c.\mathit{id}$ to the presence sets $\mathsf{pres}(k)$ along the path through the document tree described by the cursor.

If $o_d.\mathit{cur}$ is not a prefix of $o_c.\mathit{cur}$, the operations affect disjoint subtrees of the document, and so they are trivially commutative. Any state changes by \textsc{Descend} and $\textsc{Add-ID}_1$ along the shared part of the cursor path are applied using the set union operator $\cup$, which is commutative.

Now consider the case where $o_d.\mathit{cur}$ is a prefix of $o_c.\mathit{cur}$. Since $o_c$ is concurrent with $o_d$, we know that $o_c.\mathit{id} \notin o_d.\mathit{deps}$. Therefore, if $o_c$ is applied before $o_d$ in the history, the \textsc{Clear-*} rules evaluating $o_d$ will leave any occurrences of $o_c.\mathit{id}$ in the document state undisturbed, while removing any occurrences of operations in $o_d.\mathit{deps}$.

If $o_d$ is applied before $o_c$, the effect on presence sets and registers is the same as if they had been applied in the reverse order. Moreover, $o_c$ applies in the same way as if $o_d$ had not been applied previously, because applying a deletion only modifies presence sets and registers, without actually removing map keys or list elements, and because the rules for applying an operation are not conditional on the previous content of presence sets and registers.

Thus, the effect of applying $o_c$ before $o_d$ is the same as applying $o_d$ before $o_c$, so the operations commute.
\end{proof}

\begin{lemma}\label{lem:assign-commute}
In a history $H$, an assignment operation is commutative with respect to concurrent operations.
\end{lemma}

\begin{proof}
Given an assignment $o_a$ and any other concurrent operation $o_c$, we must show that the document state after applying both operations does not depend on the order in which $o_a$ and $o_c$ were applied.

The rules \textsc{Assign}, \textsc{Empty-Map} and \textsc{Empty-List} define how an assignment operation $o_a$ is applied, depending on the value being assigned. All three rules first clear any causally prior state from the cursor at which the assignment is occurring; by Lemma~\ref{lem:delete-commute}, this clearing operation is commutative with concurrent operations, and leaves updates by concurrent operations untouched.

The rules also add $o_a.\mathit{id}$ to the presence set identified by the cursor, and \textsc{Descend} adds $o_a.\mathit{id}$ to the presence sets on the path from the root of the document tree described by the cursor. These state changes are applied using the set union operator $\cup$, which is commutative.

Finally, in the case where value assigned by $o_a$ is a primitive and the \textsc{Assign} rule applies, the mapping from operation ID to value is added to the register by the expression $\mathit{child}[\,\mathit{id} \mapsto \mathit{val}\,]$. If $o_c$ is not an assignment operation or if $o_a.\mathit{cursor} \not= o_c.\mathit{cursor}$, the operations are independent and thus trivially commutative.

If $o_a$ and $o_c$ are assignments to the same cursor, we use the commutativity of updates to a partial function: $\mathit{child}[\,\mathit{id}_1 \mapsto \mathit{val}_1\,]\,[\,\mathit{id}_2 \mapsto \mathit{val}_2\,] = \mathit{child}[\,\mathit{id}_2 \mapsto \mathit{val}_2\,]\,[\,\mathit{id}_1 \mapsto \mathit{val}_1\,]$ provided that $\mathit{id}_1 \not= \mathit{id}_2$. Since operation IDs (Lamport timestamps) are unique, two concurrent assignments add two different keys to the mapping, and their order is immaterial.

Thus, all parts of the process of applying $o_a$ have the same effect on the document state, regardless of whether $o_c$ is applied before or after $o_a$, so the operations commute.
\end{proof}

\begin{lemma}\label{lem:op-commute}
Given an operation history $H=\langle o_1, \dots, o_n \rangle$ from a valid execution, a new operation $o_{n+1}$ from that execution can be inserted at any point in $H$ after $o_{n+1}.\mathit{deps}$ have been applied. For all histories $H'$ that can be constructed this way, the document state resulting from applying the operations in $H'$ in order is the same, and independent of the ordering of any concurrent operations in $H$.
\end{lemma}

\begin{proof}
$H$ can be split into a prefix and a suffix, as described in the proof of Theorem~\ref{thm:convergence}. The suffix contains only operations that are concurrent with $o_{n+1}$, and we allow $o_{n+1}$ to be inserted at any point after the prefix. We then prove the lemma case-by-case, depending on the type of mutation in $o_{n+1}$. 

If $o_{n+1}$ is a deletion, by Lemma~\ref{lem:delete-commute} it is commutative with all operations in the suffix, and so $o_{n+1}$ can be inserted at any point within, before, or after the suffix without changing its effect on the final document state. Similarly, if $o_{n+1}$ is an assignment, by Lemma~\ref{lem:assign-commute} it is commutative with all operations in the suffix.

If $o_{n+1}$ is an insertion, let $o_c$ be any operation in the suffix, and consider the cases of $o_{n+1}$ being inserted before and after $o_c$ in the history. If $o_c$ is a deletion or assignment, it is commutative with $o_{n+1}$ by Lemma~\ref{lem:delete-commute} or Lemma~\ref{lem:assign-commute} respectively. If $o_c$ is an insertion into the same list as $o_{n+1}$, then by Lemma~\ref{lem:insert-reorder} the operations are commutative. If $o_c$ is an insertion into a different list in the document, its effect is independent from $o_{n+1}$ and so the two operations can be applied in any order.

Thus, $o_{n+1}$ is commutative with respect to any concurrent operation in $H$. Therefore, $o_{n+1}$ can be inserted into $H$ at any point after its causal dependencies, and the effect on the final document state is independent of the position at which the operation is inserted.
\end{proof}

This completes the induction step in the proof of Theorem~\ref{thm:convergence}, and thus proves convergence of our datatype.

\fi % includeappendix
\end{document}
/// This is an implementation of a general purpose skip list. It was originally
/// ported from a version of skiplists intended for efficient string handling
/// found here - https://github.com/josephg/rustrope

/// This implementation is not optimized for strings (there's some string
/// specific features like unicode handling which have been intentionally
/// removed for simplicity). But it does have another somewhat unusual feature -
/// users can specify their own size function, and lookups, inserts and deletes
/// can use their custom length property to specify offsets.


use std::{mem, ptr};
use std::mem::MaybeUninit;
use std::ptr::NonNull;
use std::alloc::{alloc, dealloc, Layout};
use std::cmp::min;
use std::marker::PhantomData;
use std::iter;

use std::fmt;

use rand::{RngCore, Rng, SeedableRng};
use rand::rngs::SmallRng;

/// The likelyhood a node will have height (n+1) instead of n
const BIAS: u8 = 100; // out of 256.

/// The number of items in each node. Must fit in a u8 thanks to Node.
#[cfg(debug_assertions)]
const NODE_NUM_ITEMS: usize = 2;

#[cfg(not(debug_assertions))]
const NODE_NUM_ITEMS: usize = 100;

/// List operations will move to linear time after NODE_STR_SIZE * 2 ^
/// MAX_HEIGHT length. (With a smaller constant the higher this is). On the flip
/// side, cursors grow linearly with this number; so smaller is marginally
/// better when the contents are smaller.
#[cfg(debug_assertions)]
const MAX_HEIGHT: usize = 5;

#[cfg(not(debug_assertions))]
const MAX_HEIGHT: usize = 10;


const MAX_HEIGHT_U8: u8 = MAX_HEIGHT as u8; // convenience.

pub struct ItemMarker<Item: ListItem> {
    pub(super) ptr: *mut Node<Item>,
    // _phantom: PhantomData<&'a SkipList<C>>
}

// Derive traits don't work here.
impl<Item: ListItem> Clone for ItemMarker<Item> {
    fn clone(&self) -> Self { *self }
}
impl<Item: ListItem> Copy for ItemMarker<Item> {}
impl<Item: ListItem> PartialEq for ItemMarker<Item> {
    fn eq(&self, other: &Self) -> bool { self.ptr == other.ptr }
}
impl<Item: ListItem> Eq for ItemMarker<Item> {}

impl<Item: ListItem> ItemMarker<Item> {
    pub fn null() -> ItemMarker<Item> {
        ItemMarker { ptr: ptr::null_mut() }
    }

    pub fn is_null(self) -> bool {
        self.ptr.is_null()
    }
}

impl<Item: ListItem> Default for ItemMarker<Item> {
    fn default() -> Self { Self::null() }
}

impl<Item: ListItem> fmt::Debug for ItemMarker<Item> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.pad("ItemMarker")
    }
}



pub trait ListItem: Sized {
    /// Applications which have custom sizes (or do their own
    /// run-length-encoding) can define their own size function for items. When
    /// items are inserted or replaced, the position is specified using the
    /// custom size defined here.
    fn get_usersize(&self) -> usize { 1 }

    /// An optimized method to calculate the userlen of a slice of ListItems.
    /// The default implementation simply calls [`get_usersize`] in a loop.
    fn userlen_of_slice(items: &[Self]) -> usize {
        items.iter().fold(0, |acc, item| {
            acc + Self::get_usersize(item)
        })
    }

    /// Split the passed item into a pair of items at some offset.
    fn split_item(&self, _at: usize) -> (Self, Self) {
    // fn split_item(self, _at: usize) -> (Self, Self) {
        unimplemented!("Cannot insert in the middle of an item - split_item is not defined in trait");
    }

    fn merge_from(&mut self, _other: &Self) -> bool {
        false
    }
}

// Blanket implementations for some common builtin types, because its impossible
// to add these later. These make every item have a size of 1.
impl ListItem for () {}
impl<X, Y> ListItem for (X, Y) {}
impl<X, Y, Z> ListItem for (X, Y, Z) {}
impl<V> ListItem for Option<V> {}
impl<T, E> ListItem for Result<T, E> {}

impl<X, Y> ListItem for &(X, Y) {}
impl<X, Y, Z> ListItem for &(X, Y, Z) {}
impl<V> ListItem for &Option<V> {}
impl<T, E> ListItem for &Result<T, E> {}

impl ListItem for bool {}
impl ListItem for char {}
impl ListItem for u8 {}
impl ListItem for i8 {}
impl ListItem for u16 {}
impl ListItem for i16 {}
impl ListItem for u32 {}
impl ListItem for i32 {}
impl ListItem for usize {}
impl ListItem for isize {}
impl ListItem for f32 {}
impl ListItem for f64 {}

impl ListItem for &bool {}
impl ListItem for &char {}
impl ListItem for &u8 {}
impl ListItem for &i8 {}
impl ListItem for &u16 {}
impl ListItem for &i16 {}
impl ListItem for &u32 {}
impl ListItem for &i32 {}
impl ListItem for &usize {}
impl ListItem for &isize {}
impl ListItem for &f32 {}
impl ListItem for &f64 {}

pub trait NotifyTarget<Item: ListItem> {
    const USED: bool = true;

    fn on_set(&mut self, items: &[Item], at_marker: ItemMarker<Item>);
    fn on_delete(&mut self, items: &[Item]);
}

impl<Item: ListItem> NotifyTarget<Item> for () {
    const USED: bool = false;
    fn on_set(&mut self, _items: &[Item], _at_marker: ItemMarker<Item>) {}
    fn on_delete(&mut self, _items: &[Item]) {}
}

/// This represents a single entry in either the nexts pointers list or in an
/// iterator.
#[derive(Debug, PartialEq, Eq)]
pub(super) struct SkipEntry<Item: ListItem> {
    /// The node being pointed to.
    node: *mut Node<Item>,

    /// The number of *items* between the start of the current node and the
    /// start of the next node. That means nexts entry 0 contains the length of
    /// the current node.
    skip_usersize: usize,
}

// We can't use #[derive()] here for Copy and Clone due to a bug in the rust
// compiler: https://github.com/rust-lang/rust/issues/26925
impl<Item: ListItem> Copy for SkipEntry<Item> {}
impl<Item: ListItem> Clone for SkipEntry<Item> {
    fn clone(&self) -> Self { *self }
}

impl<Item: ListItem> SkipEntry<Item> {
    fn new_null() -> Self {
        SkipEntry { node: ptr::null_mut(), skip_usersize: 0 }
    }
}



/// The node structure is designed in a very fancy way which would be more at
/// home in C or something like that. The basic idea is that the node structure
/// is fixed size in memory, but the proportion of that space taken up by
/// characters and by the height differ depending on a node's height. This
/// results in a lot of `unsafe` blocks. I think the tradeoff is worth it but I
/// could be wrong here. You probably wouldn't lose much performance in practice
/// by replacing the inline structure with a smallvec - but that would waste
/// memory in small nodes, and require extra pointer indirection on large nodes.
/// It also wouldn't remove all the unsafe here.
///
/// A different representation (which might be better or worse - I can't tell)
/// would be to have the nodes all be the same size in memory and change the
/// *proportion* of the node's memory that is used by the string field vs the
/// next pointers. That might be lighter weight for the allocator because the
/// struct itself would be a fixed size; but I'm not sure if it would be better.
#[repr(C)] // Prevent parameter reordering.
pub(super) struct Node<Item: ListItem> {
    /// We start with the items themselves. Only the first `num_items` of this
    /// list is in use. The user specified length of the items in the node is
    /// stored in nexts[0].skip_items. This is initialized with
    /// Default::default() for the type, but when MaybeUninit completely lands,
    /// it will be possible to make this a tiny bit faster by leaving the list
    /// initially uninitialized.
    items: [MaybeUninit<Item>; NODE_NUM_ITEMS],

    /// Number of items in `items` in use / filled.
    num_items: u8,

    /// Height of nexts array.
    height: u8,

    /// With the heads array as is, we have no way to go from a marker back to a
    /// cursor (which is required to insert at that location in the list). For
    /// that we need to be able to figure out at each level of the nexts
    /// pointers which object points to us, and the offset from that element to
    /// the current element. Anyway, for markers to work we need this.
    parent: *mut Node<Item>,

    // #[repr(align(std::align_of::<SkipEntry>()))]
    
    /// In reality this array has the size of height, allocated using more or
    /// less direct calls to malloc() at runtime based on the randomly generated
    /// size. The size is always at least 1.
    nexts: [SkipEntry<Item>; 0],
}

// Make sure nexts uses correct alignment. This should be guaranteed by repr(C)
// This test will fail if this ever stops being true.
#[test]
fn test_align() {
    struct Item(u8);
    impl ListItem for Item {}
    #[repr(C)] struct Check([SkipEntry<Item>; 0]);
    assert!(mem::align_of::<Check>() >= mem::align_of::<SkipEntry<Item>>());
    // TODO: It'd be good to also check the alignment of the nexts field in Node.
}

fn random_height<R: RngCore>(rng: &mut R) -> u8 {
    let mut h: u8 = 1;
    // Should I use a csrng here? Does it matter?
    while h < MAX_HEIGHT_U8 && rng.gen::<u8>() < BIAS { h+=1; }
    h
}

#[repr(C)]
pub struct SkipList<Item: ListItem, N: NotifyTarget<Item> = ()> {
    // TODO: Consider putting the head item on the heap. For the use case here
    // its almost certainly fine either way. The code feels a bit cleaner if its
    // on the heap (and then iterators will be able to outlast a move of the
    // skiplist parent). But its also very nice having the code run fast for
    // small lists. Most lists are small, and it makes sense to optimize for
    // that.

    // TODO: For safety, pointers in to this structure should be Pin<> if we
    // ever want to hold on to iterators.

    /// The total number of items in the skip list. This is not used internally -
    /// just here for bookkeeping.
    pub(super) num_items: usize,
    /// Size of the list in user specified units.
    pub(super) num_usercount: usize,

    /// The RNG we use to generate node heights. Specifying it explicitly allows
    /// unit tests and randomizer runs to be predictable, which is very helpful
    /// during debugging. I'm still not sure how the type of this should be
    /// specified. Should it be a generic parameter? Box<dyn *>?
    /// ??
    rng: Option<SmallRng>,

    /// The first node is inline. The height is 1 more than the max height we've
    /// ever used. The highest next entry points to {null, total usersize}.
    head: Node<Item>,

    /// This is so dirty. The first node is embedded in SkipList; but we need to
    /// allocate enough room for height to get arbitrarily large. I could insist
    /// on SkipList always getting allocated on the heap, but for small lists its
    /// much better to be on the stack.
    ///
    /// So this struct is repr(C) and I'm just padding out the struct directly.
    /// All accesses should go through head because otherwise I think we violate
    /// aliasing rules.
    _nexts_padding: [SkipEntry<Item>; MAX_HEIGHT],

    _phantom: PhantomData<N>
}


impl<Item: ListItem> Node<Item> {
    // Do I need to be explicit about the lifetime of the references being tied
    // to the lifetime of the node?
    fn nexts(&self) -> &[SkipEntry<Item>] {
        unsafe {
            std::slice::from_raw_parts(self.nexts.as_ptr(), self.height as usize)
        }
    }

    fn nexts_mut(&mut self) -> &mut [SkipEntry<Item>] {
        unsafe {
            std::slice::from_raw_parts_mut(self.nexts.as_mut_ptr(), self.height as usize)
        }
    }

    fn layout_with_height(height: u8) -> Layout {
        Layout::from_size_align(
            mem::size_of::<Node<Item>>() + mem::size_of::<SkipEntry<Item>>() * (height as usize),
            mem::align_of::<Node<Item>>()).unwrap()
    }

    fn alloc_with_height(height: u8) -> *mut Node<Item> {
        assert!(height >= 1 && height <= MAX_HEIGHT_U8);

        unsafe {
            let node = alloc(Self::layout_with_height(height)) as *mut Node<Item>;
            node.write(Node {
                items: uninit_items_array(),
                num_items: 0,
                height,
                parent: ptr::null_mut(),
                nexts: [],
            });

            for next in (*node).nexts_mut() {
                *next = SkipEntry::new_null();
            }

            node
        }
    }

    fn alloc<R: RngCore>(rng: &mut R) -> *mut Node<Item> {
        Self::alloc_with_height(random_height(rng))
    }

    unsafe fn free(p: *mut Node<Item>) {
        ptr::drop_in_place(p); // We could just implement drop here, but this is cleaner.
        dealloc(p as *mut u8, Self::layout_with_height((*p).height));
    }

    fn content_slice(&self) -> &[Item] {
        let slice = &self.items[..self.num_items as usize];
        unsafe { maybeinit_slice_get_ref(slice) }
    }

    // The height is at least 1, so this is always valid.
    fn first_skip_entry<'a>(&self) -> &'a SkipEntry<Item> {
        unsafe { &*self.nexts.as_ptr() }
    }

    fn first_skip_entry_mut<'a>(&mut self) -> &'a mut SkipEntry<Item> {
        unsafe { &mut *self.nexts.as_mut_ptr() }
    }

    // TODO: Rename to len() ?
    fn get_userlen(&self) -> usize {
        self.first_skip_entry().skip_usersize
    }
    
    fn get_next_ptr(&self) -> *mut Node<Item> {
        self.first_skip_entry().node
    }

    pub(crate) fn iter(&self, local_index: usize) -> ListItemIter<Item> {
        ListItemIter {
            node: Some(&self),
            index: local_index,
            remaining_items: None
        }
    }
}

impl<Item: ListItem> Drop for Node<Item> {
    fn drop(&mut self) {
        for item in &mut self.items[0..self.num_items as usize] {
            // Could instead call assume_init() on each item but this is
            // friendlier to the optimizer.
            unsafe { ptr::drop_in_place(item.as_mut_ptr()); }
        }
    }
}

struct NodeIter<'a, Item: ListItem>(Option<&'a Node<Item>>);
impl<'a, Item: ListItem> Iterator for NodeIter<'a, Item> {
    type Item = &'a Node<Item>;

    fn next(&mut self) -> Option<&'a Node<Item>> {
        let prev = self.0;
        if let Some(n) = self.0 {
            *self = NodeIter(unsafe { n.get_next_ptr().as_ref() });
        }
        prev
    }
}

/// This is a set of pointers with metadata into a location in the list needed
/// to skip ahead, delete and insert in items. A cursor is reasonably heavy
/// weight - we fill in and maintain as many entries as the height of the list
/// dictates.
///
/// This is not needed for simply iterating sequentially through nodes and data.
/// For that look at NodeIter.
///
/// Note most/all methods using cursors are unsafe. This is because cursors use
/// raw mutable pointers into the list, so when used the following rules have to
/// be followed:
///
/// - Whenever a write happens (insert/remove/replace), any cursor not passed to
///   the write function is invalid.
/// - While a cursor is held the SkipList struct should be considered pinned and
///   must not be moved or deleted
#[derive(Copy, Clone)]
pub(crate) struct Cursor<Item: ListItem> {
    /// The global user position of the cursor in the entire list. This is used
    /// for when the max seen height increases, so we can populate previously
    /// unused entries in the cursor and in the head node.
    ///
    /// This field isn't strictly necessary - earlier versions tacked this on to
    /// the last item in entries... I'm still not sure the cleanest way to do
    /// this.
    pub(super) userpos: usize,

    /// When the userpos of an entry is 0 (totally valid and useful), a cursor
    /// becomes ambiguous with regard to where exactly its pointing in the
    /// current entry. This is used to resolve that ambiguity.
    pub(super) local_index: usize,

    pub(super) entries: [SkipEntry<Item>; MAX_HEIGHT],

    // TODO: The cursor can't outlive the skiplist, but doing this makes it
    // tricky to pass cursors around in the Skiplist type. There's probably a
    // way out of this mess, but I'm not good enough at rust to figure it out.
    // _marker: PhantomData<&'a SkipList<C>>,
}

impl<Item: ListItem> Cursor<Item> {
    pub(super) fn update_offsets(&mut self, height: usize, by: isize) {
        for i in 0..height {
            unsafe {
                // This is weird but makes sense when you realise the nexts in
                // the cursor are pointers into the elements that have the
                // actual pointers.
                // Also adding a usize + isize is awful in rust :/
                let skip = &mut (*self.entries[i].node).nexts_mut()[i].skip_usersize;
                *skip = skip.wrapping_add(by as usize);
            }
        }
    }

    /// Move a cursor to the start of the next node. Returns the new node (or a
    /// nullptr if this is the end of the list).
    fn advance_node(&mut self) -> *mut Node<Item> {
        unsafe {
            let SkipEntry { node: e, skip_usersize: offset } = self.entries[0];
            // offset tells us how far into the current element we are (in
            // usersize). We need to increment the offsets by the entry's
            // remaining length to get to the start of the next node.
            let advance_by = (*e).get_userlen() - offset;
            let next = (*e).get_next_ptr();
            let height = (*next).height as usize;

            for i in 0..height {
                self.entries[i] = SkipEntry {
                    node: next,
                    skip_usersize: 0
                };
            }

            for i in height..self.entries.len() {
                self.entries[i].skip_usersize += advance_by;
            }

            self.userpos += advance_by;
            self.local_index = 0;

            next
        }
    }

    pub(super) fn is_at_node_end(&self) -> bool {
        self.local_index == unsafe { (*self.here_ptr()).num_items } as usize
    }

    pub(super) fn advance_item(&mut self, height: usize) {
        if self.is_at_node_end() { self.advance_node(); }
        let usersize = unsafe { self.current_item() }.unwrap().get_usersize();

        for entry in &mut self.entries[0..height] {
            entry.skip_usersize += usersize;
        }
        self.userpos += usersize;
        self.local_index += 1;
    }

    pub(super) fn advance_by_items(&mut self, num: usize, height: usize) {
        for _ in 0..num { self.advance_item(height); }
    }

    pub(super) fn move_to_item_start(&mut self, height: usize, offset: usize) {
        for entry in &mut self.entries[0..height] {
            entry.skip_usersize -= offset;
        }
        self.userpos -= offset;
    }

    pub(super) unsafe fn prev_item<'a>(&self) -> Option<&'a Item> {
        let node = &*self.here_ptr();
        if self.local_index == 0 {
            assert_eq!(self.userpos, 0, "Invalid state: Cursor at start of node");
            None
        } else {
            debug_assert!(self.local_index <= node.num_items as usize);
            Some(&*(node.items[self.local_index - 1].as_ptr()))
        }
    }

    pub(super) unsafe fn prev_item_mut<'a>(&mut self) -> Option<&'a mut Item> {
        let node = &mut *self.here_ptr();
        if self.local_index == 0 {
            assert_eq!(self.userpos, 0);
            None
        } else {
            debug_assert!(self.local_index <= node.num_items as usize);
            Some(&mut *(node.items[self.local_index - 1].as_mut_ptr()))
        }
    }

    // Could be Option<NonNull<_>>...
    unsafe fn peek_next_node_start(&self) -> Option<*mut Item> {
        let next = (*self.here_ptr()).get_next_ptr();
        if next.is_null() { None }
        else {
            debug_assert!((*next).num_items > 0);
            Some((*next).items[0].as_mut_ptr())
        }
    }

    pub(super) unsafe fn current_item<'a>(&self) -> Option<&'a Item> {
        let node = &*self.here_ptr();
        if self.local_index < node.num_items as usize {
            // Ok - just return the current item.
            Some(&*(node.items[self.local_index].as_ptr()))
        } else {
            // Peek the first item in the next node.
            self.peek_next_node_start().map(|ptr| &*ptr)
        }
    }

    // pub(super) unsafe fn take_prev<'a>(&mut self) -> Option<Item> {
    //     let node = &*self.here_ptr();
    //     if self.local_index == 0 { None }
    //     else {
    //         debug_assert!(self.local_index < node.num_items as usize
    //         // Ok - just return the current item.
    //         Some(&*(node.items[self.local_index].as_ptr()))
    //     }
    // }

    // pub(super) unsafe fn current_item_mut<'a>(&mut self) -> Option<&'a mut C::Item> {
    //     let node = &mut *self.here_ptr();
    //     if self.local_index < node.num_items as usize {
    //         // Ok - just return the current item.
    //         Some(&mut *(node.items[self.local_index].as_mut_ptr()))
    //     } else {
    //         // Peek the first item in the next node.
    //         self.peek_next_item().map(|ptr| &mut *ptr)
    //     }
    // }

    /// Get the pointer to the cursor's current node
    pub(super) fn here_ptr(&self) -> *mut Node<Item> {
        self.entries[0].node
    }
}

impl<Item: ListItem> PartialEq for Cursor<Item> {
    /// Warning: This returns false if one cursor is at the end of a node, and
    /// the other at the start of the next node. Almost all code in this library
    /// leaves cursors at the end of nodes, so this shouldn't matter too much in
    /// practice.
    fn eq(&self, other: &Self) -> bool {
        if self.userpos != other.userpos
            || self.local_index != other.local_index {return false; }

        for i in 0..MAX_HEIGHT {
            let a = &self.entries[i];
            let b = &other.entries[i];
            if a.node != b.node || a.skip_usersize != b.skip_usersize { return false; }
        }
        true
    }
}
impl<Item: ListItem> Eq for Cursor<Item> {}

impl<Item: ListItem> fmt::Debug for Cursor<Item> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_struct("Cursor")
            .field("userpos", &self.userpos)
            .field("local_index", &self.local_index)
            .finish()
    }
}

// None of the rust builtins give me what I want, which is a copy-free iterator
// to owned items in a MaybeUninit array. Eh; its easy enough to make my own.
struct UninitOwnedIter<'a, Item: ListItem, N: NotifyTarget<Item>> {
    // Based on the core slice IterMut implementation.
    ptr: NonNull<Item>,
    end: *mut Item,
    _marker: PhantomData<&'a SkipList<Item, N>>
}

impl<'a, Item: ListItem, N: NotifyTarget<Item>> UninitOwnedIter<'a, Item, N> {
    /// Make a slice we can iterate from and steal data from without dropping
    /// content. This is unsafe:
    ///
    /// - If the iterator isn't fully drained then remaining items will be
    ///   forgotten (they are not dropped).
    /// - The slice passed in here must be initialized or undefined behaviour
    ///   will hit us.
    ///
    /// After iterating, the contents are uninit memory.
    unsafe fn from_slice(slice: &[MaybeUninit<Item>]) -> Self {
        let ptr = slice.as_ptr() as *mut Item; // Safe.
        let end = ptr.add(slice.len());

        UninitOwnedIter {
            ptr: NonNull::new_unchecked(ptr),
            end,
            _marker: PhantomData
        }
    }
}

impl<'a, Item: ListItem, N: NotifyTarget<Item>> Iterator for UninitOwnedIter<'a, Item, N> {
    type Item = Item;

    fn next(&mut self) -> Option<Self::Item> {
        if self.ptr.as_ptr() == self.end {
            None
        } else {
            let ptr = self.ptr;
            self.ptr = unsafe { NonNull::new_unchecked(self.ptr.as_ptr().offset(1)) };
            Some(unsafe { ptr.as_ptr().read() })
        }
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        let size = (self.end as usize - self.ptr.as_ptr() as usize) / mem::size_of::<Item>();
        (size, Some(size))
    }
}

// TODO: Stolen from MaybeUninit::uninit_array. Replace with the real uninit_array when stable.
#[inline(always)]
fn uninit_items_array<T>() -> [MaybeUninit<T>; NODE_NUM_ITEMS] {
    unsafe { MaybeUninit::<[MaybeUninit<T>; NODE_NUM_ITEMS]>::uninit().assume_init() }
}

// TODO: Stolen from MaybeUninit::slice_get_ref. Replace when available.
#[inline(always)]
unsafe fn maybeinit_slice_get_ref<T>(slice: &[MaybeUninit<T>]) -> &[T] {
    // SAFETY: casting slice to a `*const [T]` is safe since the caller guarantees that
    // `slice` is initialized, and`MaybeUninit` is guaranteed to have the same layout as `T`.
    // The pointer obtained is valid since it refers to memory owned by `slice` which is a
    // reference and thus guaranteed to be valid for reads.
    &*(slice as *const [MaybeUninit<T>] as *const [T])
}


impl<Item: ListItem, N: NotifyTarget<Item>> SkipList<Item, N> {
    pub fn new() -> Self {
        SkipList::<Item, N> {
            num_items: 0,
            num_usercount: 0,
            rng: None,
            head: Node {
                items: uninit_items_array(),
                num_items: 0,
                height: 1, // Stores max height of list nodes
                parent: ptr::null_mut(),
                nexts: [],
            },
            _nexts_padding: [SkipEntry::new_null(); MAX_HEIGHT],
            _phantom: PhantomData
        }
    }

    pub fn init_rng_from_seed(&mut self, seed: u64) {
        self.rng = Some(SmallRng::seed_from_u64(seed));
    }

    fn get_rng(&mut self) -> &mut SmallRng {
        // I'm sure there's a nicer way to implement this.
        if self.rng.is_none() {
            // We'll use a stable RNG in debug mode so the tests are stable.
            if cfg!(debug_assertions) {
                self.init_rng_from_seed(123);
            } else {
                self.rng = Some(SmallRng::from_entropy());
            }
        }
        self.rng.as_mut().unwrap()
    }


    pub fn len_user(&self) -> usize {
        self.num_usercount
    }

    pub fn len_items(&self) -> usize {
        self.num_items as usize
    }

    fn node_iter(&self) -> NodeIter<Item> { NodeIter(Some(&self.head)) }
    
    pub fn iter(&self) -> ListItemIter<Item> {
        ListItemIter {
            node: Some(&self.head),
            index: 0,
            remaining_items: Some(self.len_items())
        }
    }

    #[inline(always)]
    pub(super) fn height(&self) -> usize {
        self.head.height as usize
    }

    fn heads_mut(&mut self) -> &mut [SkipEntry<Item>] {
        unsafe {
            std::slice::from_raw_parts_mut(self.head.nexts.as_mut_ptr(), self._nexts_padding.len())
        }
    }

    fn is_head(&self, node: *const Node<Item>) -> bool {
        node as *const _ == &self.head as *const _
    }

    #[inline(always)]
    fn use_parents() -> bool {
        cfg!(debug_assertions) || N::USED
    }

    /// Walk the list and validate internal constraints. This is used for
    /// testing the structure itself, and should generally not be called by
    /// users.
    pub fn check(&self) {
        // #[cfg(test)]
        {
            // self.print();
            assert!(self.head.height >= 1);
            assert!(self.head.height <= MAX_HEIGHT_U8);

            let head_ptr = &self.head as *const _ as *mut _;
            // let skip_over = self.get_top_entry();
            // println!("Skip over skip chars {}, num bytes {}", skip_over.skip_items, self.num_bytes);

            let mut prev: [*const Node<Item>; MAX_HEIGHT] = [ptr::null(); MAX_HEIGHT];

            let mut iter = [SkipEntry {
                // Bleh.
                node: head_ptr,
                // The skips will store the total distance travelled since the
                // start of this traversal at each height. All the entries above
                // head.height are ignored though.
                skip_usersize: 0
            }; MAX_HEIGHT];

            let mut num_items = 0;
            let mut num_usercount = 0;

            for (_i, n) in self.node_iter().enumerate() {
                // println!("visiting {:?}", n.as_str());
                if !self.is_head(n) { assert!(n.num_items > 0); }
                assert!(n.height <= MAX_HEIGHT_U8);
                assert!(n.num_items as usize <= NODE_NUM_ITEMS);

                // Make sure the number of items matches the count
                let local_count = Item::userlen_of_slice(n.content_slice());
                assert_eq!(local_count, n.get_userlen());

                if Self::use_parents() {
                    let expect_parent = if self.is_head(n) {
                        ptr::null() // The head's parent is null
                    } else if n.height == self.head.height {
                        &self.head as *const _ // Max height nodes point back to head
                    } else {
                        prev[n.height as usize]
                    };

                    assert_eq!(n.parent as *const _, expect_parent, "invalid parent");
                }
                
                for (i, entry) in iter[0..n.height as usize].iter_mut().enumerate() {
                    assert_eq!(entry.node as *const _, n as *const _);
                    assert_eq!(entry.skip_usersize, num_usercount);

                    // println!("replacing entry {:?} with {:?}", entry, n.nexts()[i].node);
                    prev[i] = n;
                    entry.node = n.nexts()[i].node;
                    entry.skip_usersize += n.nexts()[i].skip_usersize;
                }

                num_items += n.num_items as usize;
                num_usercount += n.get_userlen();

                // Check the value returned by the iterator functions matches.
                let (mut normal_iter, local_offset) = self.cursor_at_userpos(num_usercount);
                assert_eq!(local_offset, 0);
                assert_eq!(normal_iter.userpos, num_usercount);

                // Dirty hack. If n has 0-sized elements at the end, the normal
                // cursor won't be at the end...
                if Self::use_parents() {
                    while normal_iter.here_ptr() != n as *const _ as *mut _ {
                        normal_iter.advance_node();
                    }
                    normal_iter.local_index = n.num_items as usize;
                    let node_iter = unsafe { self.cursor_at_node(n, n.get_userlen(), n.num_items as usize) };
                    assert_eq!(normal_iter, node_iter);
                }
            }

            for entry in iter[0..self.height()].iter() {
                // println!("{:?}", entry);
                assert!(entry.node.is_null());
                assert_eq!(entry.skip_usersize, num_usercount);
            }
            
            // println!("self bytes: {}, count bytes {}", self.num_bytes, num_bytes);
            assert_eq!(self.num_items, num_items);
            assert_eq!(self.len_user(), num_usercount);
        }
    }
    
    
    /// Internal function for creating a cursor at a particular location in the
    /// skiplist. The returned cursor contains list of nodes which point past
    /// the specified position, as well as offsets of how far into their
    /// character lists the specified characters are.
    ///
    /// Sometimes a call to iter_at_userpos is ambiguous:
    ///
    /// - The item can contain items with zero usersize. The cursor could point
    ///   to any of them.
    /// - If the location is at the end of a node, it is equally valid to return
    ///   a position at the start of the next node.
    ///
    /// Because its impossible to move backwards in the list, iter_at_userpos
    /// returns the first admissible location with the specified userpos.
    /// 
    /// Returns (cursor, offset into the specified item).
    ///
    /// TODO: This should be Pin<&self>.
    pub(super) fn cursor_at_userpos(&self, target_userpos: usize) -> (Cursor<Item>, usize) {
        assert!(target_userpos <= self.len_user());

        let mut e: *const Node<Item> = &self.head;
        let mut height = self.height() - 1;
        
        let mut offset = target_userpos; // How many more items to skip

        // We're populating the head node pointer to simplify the case when the
        // iterator grows. We could put offset into the skip_usersize but it
        // would only be *mostly* correct, not always correct. (Since cursor
        // entries above height are not updated by insert.)
        let mut cursor = Cursor {
            entries: [SkipEntry {
                node: &self.head as *const _ as *mut _,
                skip_usersize: usize::MAX
            }; MAX_HEIGHT],
            local_index: 0,
            userpos: target_userpos,
            // _marker: PhantomData,
        };

        loop { // while height >= 0
            let en = unsafe { &*e };
            let next = en.nexts()[height];
            let skip = next.skip_usersize;
            if offset > skip {
                // Go right.
                debug_assert!(e == &self.head || en.num_items > 0);
                offset -= skip;
                e = next.node;
                assert!(!e.is_null(), "Internal constraint violation: Reached end prematurely");
            } else {
                // Record this and go down.
                cursor.entries[height] = SkipEntry {
                    skip_usersize: offset,
                    node: e as *mut Node<Item>, // This is pretty gross
                };

                if height == 0 { break; } else { height -= 1; }
            }
        };

        // We should always land within the node we're pointing to.
        debug_assert!(offset <= unsafe { &*cursor.here_ptr() }.get_userlen());

        // We've found the node. Now look for the index within the node.
        let en = unsafe { &*e };
        let mut index = 0;

        while offset > 0 {
            assert!(index < en.num_items as usize);
            
            let usersize = unsafe { &*en.items[index].as_ptr() }.get_usersize();
            if usersize > offset { break; } // We're in the middle of an item.
            offset -= usersize;
            index += 1;
        }
        cursor.local_index = index;

        (cursor, offset)
    }

    /// Create a cursor at the specified node, using the parents infrastructure
    /// to calculate offsets. The offset and local_index parameters should
    /// specify the offset into the current node. They are accepted as-is.
    /// Offset *must* be at an item boundary
    unsafe fn cursor_at_node(&self, n: *const Node<Item>, mut offset: usize, local_index: usize) -> Cursor<Item> {
        assert!(Self::use_parents(), "cursor_at_node not available if notifications are disabled");

        let mut n = n as *mut Node<Item>; // We don't mutate, but we need a mut ptr.

        let mut cursor = Cursor {
            userpos: 0, // We'll set this later.
            local_index,
            entries: [SkipEntry {
                node: &self.head as *const _ as *mut _,
                skip_usersize: usize::MAX
            }; MAX_HEIGHT],
            // _marker: PhantomData
        };

        let mut h = 0;
        loop {
            while h < (*n).height as usize {
                cursor.entries[h] = SkipEntry {
                    node: n,
                    skip_usersize: offset
                };

                h += 1;
            }

            let parent = (*n).parent;
            // Reached the head.
            if parent.is_null() { break; }

            // If we're the same height as the parent its fine.
            debug_assert!((*parent).height as usize > h
                || (self.is_head(parent) && (*parent).height as usize == h));

            // Walk from parent back to n, figuring out the offset.
            let mut c = parent;
            // let walk_height = (*parent).height as usize - 2;
            let walk_height = (*n).height as usize - 1;
            while c != n {
                let elem = (*c).nexts()[walk_height];
                offset += elem.skip_usersize;
                c = elem.node;
            }

            n = parent;
        }

        cursor.userpos = offset;
        cursor
    }

    /// SAFETY: Self must outlast the marker and not have been moved since the
    /// marker was created. Self should really be Pin<>!
    pub(super) unsafe fn cursor_at_marker<P>(&mut self, marker: ItemMarker<Item>, predicate: P) -> Option<(Cursor<Item>, usize)>
    where P: Fn(&Item) -> Option<usize> {
        // The marker gives us a pointer into a node. Find the item.
        let n = marker.ptr;

        let mut offset: usize = 0;
        let mut local_index = None;
        let mut item_offset = 0;
        for (i, item) in (*n).content_slice().iter().enumerate() {
            if let Some(item_offset_) = predicate(item) {
                // offset += item_offset;
                item_offset = item_offset_;
                local_index = Some(i);
                break;
            } else {
                offset += item.get_usersize();
            }
        }

        local_index.map(|local_index| {
            (self.cursor_at_node(n, offset, local_index), item_offset)
        })
    }

    // Internal fn to create a new node at the specified iterator filled with
    // the specified content. The passed cursor should point at the end of the
    // previous node. It will be updated to point to the end of the newly
    // inserted content.
    // unsafe fn insert_node_at(&mut self, cursor: &mut Cursor<Item>, contents: &[C::Item], new_userlen: usize, move_cursor: bool) {
    unsafe fn insert_node_at<I>(&mut self, cursor: &mut Cursor<Item>, contents: &mut I, num_items: usize, move_cursor: bool, notify: &mut N)
            where I: Iterator<Item=Item> {

        // println!("Insert_node_at {} len {}", contents.len(), self.num_bytes);
        // debug_assert_eq!(new_userlen, C::userlen_of_slice(contents));
        assert!(num_items <= NODE_NUM_ITEMS);
        debug_assert!(contents.size_hint().0 >= num_items);

        let new_node_ptr = Node::alloc(self.get_rng());
        let new_node = &mut *new_node_ptr;
        new_node.num_items = num_items as u8;

        for (slot, item) in new_node.items[..num_items].iter_mut().zip(contents) {
            (slot.as_mut_ptr() as *mut Item).write(item); // Write makes sure we don't drop the old value.
        }

        let new_userlen = Item::userlen_of_slice(new_node.content_slice());

        let new_height = new_node.height;
        let new_height_usize = new_height as usize;

        let mut head_height = self.height();
        while head_height < new_height_usize {
            // This seems weird given we're about to overwrite these values
            // below. What we're doing is retroactively setting up the cursor
            // and head pointers *as if* the height had been this high all
            // along. This way we only have to populate the higher head values
            // lazily.
            let total_userlen = self.num_usercount;
            let nexts = self.heads_mut();
            nexts[head_height].skip_usersize = total_userlen;
            cursor.entries[head_height].skip_usersize = cursor.userpos;

            head_height += 1; // This is ugly.
            self.head.height += 1;
        }

        new_node.parent = if new_height_usize == MAX_HEIGHT {
            &self.head as *const _ as *mut _
        } else { cursor.entries[new_height_usize].node };

        for i in 0..new_height_usize {
            let prev_skip = &mut (*cursor.entries[i].node).nexts_mut()[i];
            let new_nexts = new_node.nexts_mut();

            // The new node points to the successor (or null)
            new_nexts[i] = SkipEntry {
                node: prev_skip.node,
                skip_usersize: new_userlen + prev_skip.skip_usersize - cursor.entries[i].skip_usersize
            };

            // The previous node points to the new node
            *prev_skip = SkipEntry {
                node: new_node_ptr,
                skip_usersize: cursor.entries[i].skip_usersize
            };

            // Move the iterator to the end of the newly inserted node.
            if move_cursor {
                cursor.entries[i] = SkipEntry {
                    node: new_node_ptr,
                    skip_usersize: new_userlen
                };
            }
        }

        for i in new_height_usize..head_height {
            (*cursor.entries[i].node).nexts_mut()[i].skip_usersize += new_userlen;
            if move_cursor {
                cursor.entries[i].skip_usersize += new_userlen;
            }
        }

        // Update parents.
        if Self::use_parents() && new_height_usize > 1 {
            let mut n = new_node_ptr;
            let mut skip_height = 0;

            loop {
                n = (*n).nexts_mut()[skip_height].node;
                if n.is_null() || (*n).height >= new_height { break; }
                
                (*n).parent = new_node_ptr;
                skip_height = usize::max(skip_height, (*n).height as usize - 1);
            }
        }
        
        self.num_items += num_items;
        self.num_usercount += new_userlen;
        if move_cursor {
            cursor.userpos += new_userlen;
            cursor.local_index = num_items;
        }

        notify.on_set(new_node.content_slice(), ItemMarker {
            ptr: new_node_ptr,
            // _phantom: PhantomData
        });
    }

    // unsafe fn insert_at_iter(&mut self, cursor: &mut Cursor<C>, contents: &[C::Item]) {
    pub(super) unsafe fn insert_at_iter<I>(&mut self, cursor: &mut Cursor<Item>, contents: &mut I, notify: &mut N)
            where I: ExactSizeIterator<Item=Item> {
        // iter specifies where to insert.

        let mut e = cursor.here_ptr();

        // The insertion offset into the destination node.
        assert!(cursor.userpos <= self.num_usercount);
        assert!(cursor.local_index <= (*e).num_items as usize);

        // We might be able to insert the new data into the current node, depending on
        // how big it is.
        let num_inserted_items = contents.len();

        // Can we insert into the current node?
        let mut insert_here = (*e).num_items as usize + num_inserted_items <= NODE_NUM_ITEMS;

        // Can we insert into the start of the successor node?
        if !insert_here && cursor.local_index == (*e).num_items as usize && num_inserted_items <= NODE_NUM_ITEMS {
            // We can insert into the subsequent node if:
            // - We can't insert into the current node
            // - There _is_ a next node to insert into
            // - The insert would be at the start of the next node
            // - There's room in the next node
            if let Some(next) = (*e).first_skip_entry_mut().node.as_mut() {
                if next.num_items as usize + num_inserted_items <= NODE_NUM_ITEMS {
                    cursor.advance_node();
                    e = next;

                    insert_here = true;
                }
            }
        }

        let item_idx = cursor.local_index;
        let e_num_items = (*e).num_items as usize; // convenience.

        if insert_here {
            // println!("insert_here {}", contents);
            // First push the current items later in the array
            let c = &mut (*e).items;
            if item_idx < e_num_items {
                // Can't use copy_within because Item doesn't necessarily
                // implement Copy. Memmove the existing items.
                ptr::copy(
                    &c[item_idx],
                    &mut c[item_idx + num_inserted_items],
                    (*e).num_items as usize - item_idx);
            }

            // Then copy in the new items. Can't memcpy from an iterator, but
            // the optimizer should make this fast.
            let dest_content_slice = &mut c[item_idx..item_idx + num_inserted_items];
            for (slot, item) in dest_content_slice.iter_mut().zip(contents) {
                // Do not drop the old items - they were only moved.
                slot.as_mut_ptr().write(item);
            }
            let dest_content_slice = maybeinit_slice_get_ref(dest_content_slice);

            (*e).num_items += num_inserted_items as u8;
            self.num_items += num_inserted_items;
            let num_inserted_usercount = Item::userlen_of_slice(dest_content_slice);
            self.num_usercount += num_inserted_usercount;

            // .... aaaand update all the offset amounts.
            cursor.update_offsets(self.height(), num_inserted_usercount as isize);

            // Usually the cursor will be discarded after one change, but for
            // consistency of compound edits we'll update the cursor to point to
            // the end of the new content.
            for entry in cursor.entries[0..self.height()].iter_mut() {
                entry.skip_usersize += num_inserted_usercount;
            }
            cursor.userpos += num_inserted_usercount;
            cursor.local_index += num_inserted_items;

            notify.on_set(dest_content_slice, ItemMarker {
                ptr: e,
                // _phantom: PhantomData
            });
        } else {
            // There isn't room. We'll need to add at least one new node to the
            // list. We could be a bit more careful here and copy as much as
            // possible into the current node - that would decrease the number
            // of new nodes in some cases, but I don't think the performance
            // difference will be large enough to justify the complexity.

            // If we're not at the end of the current node, we'll need to remove
            // the end of the current node's data and reinsert it later.
            let num_end_items = e_num_items - item_idx;

            let (end_items, _end_usercount) = if num_end_items > 0 {
                // We'll mark the items as deleted from the node, while leaving
                // the data itself there for now to avoid a copy.

                // Note that if we wanted to, it would also be correct (and
                // slightly more space efficient) to pack some of the new
                // string's characters into this node after trimming it.
                let end_items = &(*e).items[item_idx..e_num_items];
                (*e).num_items = item_idx as u8;
                let end_usercount = (*e).get_userlen() - cursor.entries[0].skip_usersize;

                cursor.update_offsets(self.height(), -(end_usercount as isize));

                // We need to trim the size off because we'll add the characters
                // back with insert_node_at.
                self.num_usercount -= end_usercount;
                self.num_items -= num_end_items;

                (Some(end_items), end_usercount)
            } else {
                (None, 0)
            };

            // Now we insert new nodes containing the new character data. The
            // data is broken into pieces with a maximum size of NODE_NUM_ITEMS.
            // As further optimization, we could try and fit the last piece into
            // the start of the subsequent node.
            let mut items_remaining = num_inserted_items;
            while items_remaining > 0 {
                let insert_here = usize::min(items_remaining, NODE_NUM_ITEMS);
                self.insert_node_at(cursor, contents, insert_here, true, notify);
                items_remaining -= insert_here;
            }

            // TODO: Consider recursively calling insert_at_iter() here instead
            // of making a whole new node for the remaining content.
            if let Some(end_items) = end_items {
                // Passing false to indicate we don't want the cursor updated
                // after this - it should remain at the end of the newly
                // inserted content, which is *before* this end bit.
                self.insert_node_at(cursor, &mut UninitOwnedIter::<Item, N>::from_slice(end_items), end_items.len(), false, notify);
            }
        }
    }

    // unsafe fn insert_at_iter(&mut self, cursor: &mut Cursor<C>, contents: &[C::Item]) {
    //     self.insert_at_iter_and_notify(cursor, contents, Self::no_notify);
    // }

    /// Interestingly unlike the original, here we only care about specifying
    /// the number of removed items by counting them. We do not use usersize in
    /// the deleted item count.
    ///
    /// If the deleted content occurs at the start of a node, the cursor passed
    /// here must point to the end of the previous node, not the start of the
    /// current node.
    pub(super) unsafe fn del_at_iter(&mut self, cursor: &Cursor<Item>, mut num_deleted_items: usize, notify: &mut N) {
        if num_deleted_items == 0 { return; }

        let mut item_idx = cursor.local_index;
        let mut e = cursor.here_ptr();
        while num_deleted_items > 0 {
            // self.print();
            // if cfg!(debug_assertions) { self.check(); }
            if item_idx == (*e).num_items as usize {
                let entry = (*e).first_skip_entry();
                // End of current node. Skip to the start of the next one. We're
                // intentionally not updating the iterator because if we delete
                // a whole node we need the iterator to point to the previous
                // element. And if we only delete here, the iterator doesn't
                // need to be moved.
                e = entry.node;
                if e.is_null() { panic!("Cannot delete past the end of the list"); }
                item_idx = 0;
            }

            let e_num_items = (*e).num_items as usize;
            let removed_here = min(num_deleted_items, e_num_items - item_idx);
            
            let height = (*e).height as usize;
            let removed_userlen;

            if removed_here < e_num_items || e as *const _ == &self.head as *const _ {
                // Just trim the node down.
                let trailing_items = e_num_items - item_idx - removed_here;
                
                let c = &mut (*e).items;

                if N::USED {
                    notify.on_delete(maybeinit_slice_get_ref(&c[item_idx..item_idx + removed_here]));
                }

                if mem::needs_drop::<Item>() {
                    for item in &mut c[item_idx..item_idx + removed_here] {
                        ptr::drop_in_place(item.as_mut_ptr());
                    }
                }

                removed_userlen = Item::userlen_of_slice(maybeinit_slice_get_ref(&c[item_idx..item_idx + removed_here]));
                if trailing_items > 0 {
                    ptr::copy(
                        &c[item_idx + removed_here],
                        &mut c[item_idx],
                        trailing_items);
                }

                (*e).num_items -= removed_here as u8;
                self.num_items -= removed_here;
                self.num_usercount -= removed_userlen;

                for s in (*e).nexts_mut() {
                    s.skip_usersize -= removed_userlen;
                }
            } else {
                // Remove the node from the skip list entirely. e should be the
                // next node after the position of the iterator.
                assert_ne!(cursor.here_ptr(), e);

                if N::USED {
                    notify.on_delete((*e).content_slice());
                }

                removed_userlen = (*e).get_userlen();
                let next = (*e).first_skip_entry().node;

                // println!("removing {:?} contents {:?} height {}", e, (*e).content_slice(), height);

                for i in 0..height {
                    let s = &mut (*cursor.entries[i].node).nexts_mut()[i];
                    s.node = (*e).nexts_mut()[i].node;
                    s.skip_usersize += (*e).nexts()[i].skip_usersize - removed_userlen;
                }

                self.num_items -= (*e).num_items as usize;
                self.num_usercount -= removed_userlen;

                // Update parents.
                if Self::use_parents() && height > 1 {
                    let mut n = e;
                    // let new_parent = cursor.entries[height - 1].node;

                    // If you imagine this node as a big building, we need to
                    // update the parent of all the nodes we cast a shadow over.
                    // So, if our height is 3 and the next nodes have heights 1
                    // and 2, they both need new parents.
                    let mut parent_height = 1;
                    let cursor_node = cursor.here_ptr();
                    let cursor_node_height = (*cursor_node).height as usize;
                    let mut new_parent = if height >= cursor_node_height {
                        cursor.entries[parent_height].node
                    } else {
                        cursor_node
                    };

                    loop {
                        n = (*n).nexts_mut()[parent_height - 1].node;
                        if n.is_null() || (*n).height >= height as u8 { break; }
                        let n_height = (*n).height as usize;
                        
                        assert_eq!((*n).parent, e);
                        assert!(n_height >= parent_height - 1);

                        if n_height > parent_height {
                            parent_height = n_height;
                            if n_height >= cursor_node_height {
                                new_parent = cursor.entries[parent_height].node
                            }
                        }
                        
                        (*n).parent = new_parent;
                    }
                }

                Node::free(e);
                e = next;
            }

            for i in height..self.height() {
                let s = &mut (*cursor.entries[i].node).nexts_mut()[i];
                s.skip_usersize -= removed_userlen;
            }

            num_deleted_items -= removed_here;

            // if cfg!(debug_assertions) { self.check(); }
        }
    }


    pub(super) unsafe fn replace_at_iter<I>(&mut self, cursor: &mut Cursor<Item>, mut removed_items: usize, inserted_content: &mut I, notify: &mut N)
            where I: ExactSizeIterator<Item=Item> {
        if removed_items == 0 && inserted_content.len() == 0 { return; }

        // Replace as many items from removed_items as we can with inserted_content.
        let mut replaced_items = min(removed_items, inserted_content.len());
        removed_items -= replaced_items;

        while replaced_items > 0 {
            debug_assert!(inserted_content.len() >= replaced_items);
            let mut e = cursor.here_ptr();
            if cursor.local_index == (*e).num_items as usize {
                // Move to the next item.
                e = cursor.advance_node();
                if e.is_null() { panic!("Cannot replace past the end of the list"); }
            }

            let index = cursor.local_index;

            let e_num_items = (*e).num_items as usize;
            let replaced_items_here = min(replaced_items, e_num_items - index);

            let dest = &mut (*e).items[index..index + replaced_items_here];
            let old_usersize = Item::userlen_of_slice(maybeinit_slice_get_ref(dest));

            // Replace the items themselves. Everything else is commentary.
            // Would prefer to use zip() but it wants ownership of inserted_content :/
            for slot in dest.iter_mut() {
                *slot.as_mut_ptr() = inserted_content.next().unwrap();
            }

            let dest = maybeinit_slice_get_ref(dest);
            let new_usersize = Item::userlen_of_slice(dest);
            let usersize_delta = new_usersize as isize - old_usersize as isize;

            if usersize_delta != 0 {
                cursor.update_offsets(self.height(), usersize_delta);
                // I hate this.
                self.num_usercount = self.num_usercount.wrapping_add(usersize_delta as usize);
            }

            replaced_items -= replaced_items_here;
            // We'll hop to the next Node at the start of the next loop
            // iteration if needed.
            cursor.local_index += replaced_items_here;

            for i in 0..self.height() {
                cursor.entries[i].skip_usersize += new_usersize;
            }
            cursor.userpos += new_usersize;

            notify.on_set(dest, ItemMarker {
                ptr: e,
                // _phantom: PhantomData,
            });
        }

        // Ok now one of two things must be true. Either we've run out of
        // items to remove, or we've run out of items to insert.
        if inserted_content.len() > 0 {
            // Insert!
            debug_assert!(removed_items == 0);
            self.insert_at_iter(cursor, inserted_content, notify);
        } else if removed_items > 0 {
            self.del_at_iter(cursor, removed_items, notify);
        }
    }

    pub(super) unsafe fn replace_item(&mut self, cursor: &mut Cursor<Item>, new_item: Item, notify: &mut N) {
        // This could easily be optimized.
        self.replace_at_iter(cursor, 1, &mut iter::once(new_item), notify);

        // self.modify_at(start_userpos, Self::no_notify, |item, offset| {
        //     assert_eq!(offset, 0, "replace_at must modify the entire item");
        //     *item = 
        // })
    }

    // TODO: This is just for debugging. Do not export this.
    pub fn print(&self) where Item: std::fmt::Debug {
        println!("items: {}\tuserlen: {}, height: {}", self.num_items, self.len_user(), self.head.height);

        print!("HEAD:");
        for s in self.head.nexts() {
            print!(" |{} ", s.skip_usersize);
        }
        println!();

        use std::collections::HashMap;
        let mut ptr_to_id = HashMap::new();
        // ptr_to_id.insert(std::ptr::null(), usize::MAX);
        for (i, node) in self.node_iter().enumerate() {
            print!("{}:", i);
            ptr_to_id.insert(node as *const _, i);
            for s in node.nexts() {
                print!(" |{} ", s.skip_usersize);
            }
            print!("      : {:?}", node.content_slice());
            if let Some(id) = ptr_to_id.get(&(node.parent as *const _)) {
                print!(" (parent: {})", id);
            }
            print!(" (pointer: {:?})", node as *const _);

            println!();
        }
    }
}



impl<Item: ListItem, N: NotifyTarget<Item>> SkipList<Item, N> {
    pub fn eq_list<Rhs>(&self, other: &[Rhs]) -> bool where Item: PartialEq<Rhs> {
        let mut pos = 0;
        let other_len = other.len();

        for node in self.node_iter() {
            let my_data = node.content_slice();
            let my_len = my_data.len();

            if pos + my_len > other_len || my_data != &other[pos..pos + my_data.len()] {
                return false
            }
            pos += my_data.len();
        }

        pos == other_len
    }
}

impl<Item: ListItem, N: NotifyTarget<Item>> Drop for SkipList<Item, N> {
    fn drop(&mut self) {
        let mut node = self.head.first_skip_entry().node;
        unsafe {
            while !node.is_null() {
                let next = (*node).first_skip_entry().node;
                Node::free(node);
                node = next;
            }
        }
    }
}


// Only if there's no notification target.
impl<I, Item: ListItem> From<I> for SkipList<Item> where I: ExactSizeIterator<Item=Item> {
    fn from(iter: I) -> SkipList<Item> {
        SkipList::new_from_iter(iter)
    }
}

// Needs me to relax the ExactSizeIterator constraint on insert.
// impl<Item: ListItem> iter::FromIterator<Item> for SkipList<Item> {
//     fn from_iter<T: IntoIterator<Item = Item>>(iter: T) -> Self {
//         SkipList::new_from_iter(iter)
//     }
// }

impl<Item: ListItem, N: NotifyTarget<Item>> Into<Vec<Item>> for &SkipList<Item, N> where Item: Copy {
    fn into(self) -> Vec<Item> {
        let mut content: Vec<Item> = Vec::with_capacity(self.num_items);

        for node in self.node_iter() {
            content.extend(node.content_slice().iter());
        }

        content
    }
}

impl<Item: ListItem, N: NotifyTarget<Item>> fmt::Debug for SkipList<Item, N> where Item: fmt::Debug {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_list().entries(self.iter()).finish()
    }
}

impl<Item: ListItem, N: NotifyTarget<Item>> Default for SkipList<Item, N> {
    fn default() -> Self {
        SkipList::new()
    }
}


pub struct ListItemIter<'a, Item: ListItem> {
    node: Option<&'a Node<Item>>,
    index: usize,
    remaining_items: Option<usize> // For size_hint, if known.
}

impl<'a, Item: ListItem> Iterator for ListItemIter<'a, Item> {
    type Item = &'a Item;

    fn next(&mut self) -> Option<Self::Item> {
        if let Some(node) = self.node {
            let current = &node.items[self.index];
            self.index += 1;
            if self.index == node.num_items as usize {
                self.index = 0;
                self.node = unsafe { node.get_next_ptr().as_ref() };
            }

            Some(unsafe { &*current.as_ptr() })
        } else { None }
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        if let Some(r) = self.remaining_items {
            (r, Some(r))
        } else {
            (0, None)
        }
    }
}


// impl<T: Default + Copy, F: Fn(&T) -> usize> PartialEq for SkipList<T, F> {
//     // This is quite complicated. It would be cleaner to just write a bytes
//     // iterator, then iterate over the bytes of both strings comparing along the
//     // way.
//     // However, this should be faster because it can memcmp().

//     // Another way to implement this would be to rewrite it as a comparison with
//     // an iterator over &str. Then the list vs list comparison would be trivial,
//     // but also we could add comparison functions with a single &str and stuff
//     // very easily.
//     fn eq(&self, other: &SkipList<T, F>) -> bool {
//         if self.num_items != other.num_items
//                 || self.num_chars() != other.num_chars() {
//             return false
//         }

//         let mut other_iter = other.iter().map(|n| { n.as_str() });

//         let mut os = other_iter.next();
//         let mut opos: usize = 0; // Byte offset in os.
//         for n in self.iter() {
//             let s = n.as_str();
//             let mut pos: usize = 0; // Current byte offset in s
//             debug_assert_eq!(s.len(), n.num_bytes as usize);

//             // Walk s.len() bytes through the other list
//             while pos < n.num_bytes as usize {
//                 if let Some(oss) = os {
//                     let amt = min(s.len() - pos, oss.len() - opos);
//                     // println!("iter slen {} pos {} osslen {} amt {}", s.len(), pos, oss.len(), amt);

//                     if &s[pos..pos+amt] != &oss[opos..opos+amt] {
//                         return false
//                     }

//                     pos += amt;
//                     opos += amt;
//                     debug_assert!(opos <= oss.len());

//                     if opos == oss.len() {
//                         os = other_iter.next();
//                         opos = 0;
//                     }
//                 } else {
//                     panic!("Internal string length does not match");
//                 }
//             }
//         }

//         true
//     }
// }
// impl<T: Default + Copy, F: Fn(&T) -> usize> Eq for SkipList<T, F> {}

// impl<T: Default + Copy, F> Clone for SkipList<T, F> where F: Fn(&T) -> usize {
//     fn clone(&self) -> Self {
//         let mut r = SkipList::new(self.get_usersize);
//         r.num_items = self.num_items;
//         let head_str = self.head.as_str();
//         r.head.items[..head_str.len()].copy_from_slice(head_str.as_bytes());
//         r.head.num_bytes = self.head.num_bytes;
//         r.head.height = self.head.height;
        
//         {
//             // I could just edit the overflow memory directly, but this is safer
//             // because of aliasing rules.
//             let head_nexts = r.head.nexts_mut();
//             for i in 0..self.height() {
//                 head_nexts[i].skip_items = self.nexts[i].skip_items;
//             }
//         }

//         let mut nodes = [&mut r.head as *mut Node; MAX_HEIGHT];

//         // The first node the iterator will return is the head. Ignore it.
//         let mut iter = self.iter();
//         iter.next();
//         for other in iter {
//             // This also sets height.
//             let height = other.height;
//             let node = Node::alloc_with_height(height);
//             unsafe {
//                 (*node).num_bytes = other.num_bytes;
//                 let len = other.num_bytes as usize;
//                 (*node).items[..len].copy_from_slice(&other.items[..len]);

//                 let other_nexts = other.nexts();
//                 let nexts = (*node).nexts_mut();
//                 for i in 0..height as usize {
//                     nexts[i].skip_items = other_nexts[i].skip_items;
//                     (*nodes[i]).nexts_mut()[i].node = node;
//                     nodes[i] = node;
//                 }
//             }
//         }

//         r
//     }
// }
# 5000x faster CRDTs: An Adventure in Optimization

<span class=post-meta>July 31 2021</span>

A few years ago I was really bothered by an academic paper.

Some researchers in France put together a comparison showing lots of ways you could implement realtime collaborative editing (like Google Docs). They implemented lots of algorithms - CRDTs and OT algorithms and stuff. And they benchmarked them all to see how they perform. (Cool!!) Some algorithms worked reasonably well. But others took upwards of 3 seconds to process simple paste operations from their editing sessions. Yikes!

Which algorithm was that? Well, this is awkward but .. it was mine. I mean, I didn't invent it - but it was the algorithm I was using for ShareJS. The algorithm we used for Google Wave. The algorithm which - hang on - I knew for a fact didn't take 3 seconds to process large paste events. Whats going on here?

I took a closer look at the paper. In their implementation when a user pasted a big chunk of text (like 1000 characters), instead of creating 1 operation with 1000 characters, their code split the insert into 1000 individual operations. And each of those operations needed to be processed separately. Do'h - of course it'll be slow if you do that! This isn't a problem with the operational transformation algorithm. This is just a problem with *their particular implementation*.

The infuriating part was that several people sent me links to the paper and (pointedly) asked me what I think about it. Written up as a Published Science Paper, these speed comparisons seemed like a Fact About The Universe. And not what they really were - implementation details of some java code, written by a probably overstretched grad student. One of a whole bunch of implementations that they needed to code up.

"Nooo! The peer reviewed science isn't right everybody! Please believe me!". But I didn't have a published paper justifying my claims. I had working code but it felt like none of the smart computer science people cared about that. Who was I? I was nobody.

---

Even talking about this stuff we have a language problem. We describe each system as an "algorithm". Jupiter is an Algorithm. RGA is an Algorithm. But really there are two very separate aspects:

1. The black-box *behaviour* of concurrent edits. When two clients edit the same region of text at the same time, what happens? Are they merged, and if so in what order? What are the rules?
2. The white-box *implementation* of the system. What programming language are we using? What data structures? How well optimized is the code?

If some academic's code runs slowly, what does that actually teach us? Maybe it's like tests. A passing test suite *suggests*, but can never *prove* that there are no bugs. Likewise a slow implementation suggests, but can never prove that every implementation of the system will be slow. If you wait long enough, somebody will find more bugs. And, maybe, someone out there can design a faster implementation.

Years ago I translated my old text OT code into C, Javascript, Go, Rust and Swift. Each implementation has the same behaviour, and the same algorithm. But the performance is not even close. In javascript my transform function ran about 100 000 times per second. Not bad! But the same function in C does 20M iterations per second. That's 200x faster. Wow!

Were the academics testing a slow version or the fast version of this code? Maybe, without noticing, they had fast versions of some algorithms and slow versions of others. It's impossible to tell from the paper!


## Making CRDTs fast

So as you may know, I've been getting interested in CRDTs lately. For the uninitiated, CRDTs [(Conflict-Free Replicated Data types)](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) are fancy programming tools which let multiple users edit the same data at the same time. They let you work locally with no lag. (You don't even have to be online). And when you do sync up with other users & devices, everything just magically syncs up and becomes eventually consistent. The best part of CRDTs is that they can do all that without even needing a centralized computer in the cloud to monitor and control everything.

I want Google Docs without google. I want my apps to seamlessly share data between all my devices, without me needing to rely on some [flakey startup](https://ourincrediblejourney.tumblr.com/)'s servers to still be around in another decade. I think they're the [future of collaborative editing](https://josephg.com/blog/crdts-are-the-future/). And maybe the future of all software - but I'm not ready to talk about that yet.

But most CRDTs you read about in academic papers are crazy slow. A decade ago I decided to stop reading academic papers and dismissed them. I assumed CRDTs had some inherent problem. A GUID for every character? Nought but madness comes from those strange lands! But - and this is awkward to admit - I think I've been making the same mistake as those researchers. I was reading papers which described the *behaviour* of different systems. And I assumed that meant we knew how the best way to *implement* those systems. And wow, I was super wrong.

How wrong? Well. Running [this editing trace](https://github.com/automerge/automerge-perf/), [Automerge](https://github.com/automerge/automerge/) (a popular CRDT, written by [a popular researcher](https://martin.kleppmann.com/)) takes nearly 5 minutes to run. I have a [new implementation](https://github.com/josephg/diamond-types) that can process the same editing trace in 56 milliseconds. Thats 0.056 seconds, which is over 5000x faster. It's the largest speed up I've ever gotten from optimization work - and I'm utterly delighted by it.

Lets talk about why automerge is currently slow, and I'll take you through all the steps toward making it super fast.

Wait, no. First we need to start with:


### What is automerge?

Automerge is a library to help you do collaborative editing. It's written by Martin Kleppmann, who's a little bit famous from his book and [excellent talks](https://martin.kleppmann.com/2020/07/06/crdt-hard-parts-hydra.html). Automerge is based on an algorithm called RGA, which you can read about in an academic paper if you're into that sort of thing.

Martin explains automerge far better than I will in this talk from 2020:

<iframe class="youtube" src="https://www.youtube-nocookie.com/embed/x7drE24geUw?start=1237" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Automerge (and Yjs and other CRDTs) think of a shared document as a list of characters. Each character in the document gets a unique ID, and whenever you insert into the document, you name what you're inserting after.

Imagine I type "abc" into an empty document. Automerge creates 3 items:

- Insert *'a'* id `(seph, 0)` after `ROOT`
  - Insert *'b'* id `(seph, 1)` after `(seph, 0)`
    - Insert *'c'* id `(seph, 2)` after `(seph, 1)`

We can draw this as a tree!

![tree with "abc" inserts](automerge1.drawio.svg)

Lets say Mike inserts an 'X' between *a* and *b*, so we get "aXbc". Then we have:

- Insert *'a'* id `(seph, 0)` after `ROOT`
  - Insert *'X'* id `(mike, 0)` after `(seph, 0)`
  - Insert *'b'* id `(seph, 1)` after `(seph, 0)`
    - Insert *'c'* id `(seph, 2)` after `(seph, 1)`

![tree with "aXbc"](automerge2.drawio.svg)

Note the 'X' and 'b' both share the same parent. This will happen when users type concurrently in the same location in the document. But how do we figure out which character goes first? We could just sort using their agent IDs or something. But argh, if we do that the document could end up as *abcX*, even though Mike inserted *X* before the *b*. That would be really confusing.

Automerge (RGA) solves this with a neat hack. It adds an extra integer to each item called a *sequence number*. Whenever you insert something, you set the new item's sequence number to be 1 bigger than the biggest sequence number you've ever seen:

- Insert *'a'* id `(seph, 0)` after `ROOT`, seq: *0*
  - Insert *'X'* id `(mike, 0)` after `(seph, 0)`, seq: *3*
  - Insert *'b'* id `(seph, 1)` after `(seph, 0)`, seq: *1*
    - Insert *'c'* id `(seph, 2)` after `(seph, 1)`, seq: *2*

This is the algorithmic version of "Wow I saw a sequence number, and it was *this big!*" "Yeah? Mine is *even bigger!*"

The rule is that children are sorted first based on their sequence numbers (bigger sequence number first). If the sequence numbers match, the changes must be concurrent. In that case we can sort them arbitrarily based on their agent IDs. (We do it this way so all peers end up with the same resulting document.)

Yjs - which we'll see more of later - implements a CRDT called YATA. YATA is identical to RGA, except that it solves this problem with a slightly different hack. But the difference isn't really important here.

Automerge (RGA)'s *behaviour* is defined by this algorithm:

- Build the tree, connecting each item to its parent
- When an item has multiple children, sort them by sequence number then by their ID.
- The resulting list (or text document) can be made by flattening the tree with a depth-first traversal.

So how should you *implement* automerge? The automerge library does it in the obvious way, which is to store all the data as a tree. (At least I think so - after typing "abc" [this is automerge's internal state](https://gist.github.com/josephg/0522c4aec5021cc1dddb60e778828dbe). Uh, uhm, I have no idea whats going on here. And what are all those Uint8Arrays doing all over the place? Whatever.) The automerge library works by building a tree of items.

For a simple benchmark, I'm going to test automerge using [an editing trace Martin himself made](https://github.com/automerge/automerge-perf/). This is a character by character recording of Martin typing up an academic paper. There aren't any concurrent edits in this trace, but users almost never actually put their cursors at exactly the same place and type anyway, so I'm not too worried about that. I'm also only counting the time taken to apply this trace *locally*, which isn't ideal but it'll do. Kevin Jahns (Yjs's author) has a much more [extensive benchmarking suite here](https://github.com/dmonad/crdt-benchmarks) if you're into that sort of thing. All the benchmarks here are done on my chonky ryzen 5800x workstation, with Nodejs v16.1 and rust 1.52 when that becomes appropriate. (Spoilers!)

The editing trace has 260 000 edits, and the final document size is about 100 000 characters.

As I said above, automerge takes a little under 5 minutes to process this trace. Thats just shy of 900 edits per second, which is probably fine. But by the time it's done, automerge is using 880 MB of RAM. Whoa! That's 10kb of ram *per key press*. At peak, automerge was using 2.6 GB of RAM!

To get a sense of how much overhead there is, I'll compare this to [a baseline benchmark](https://gist.github.com/josephg/13efc1444660c07870fcbd0b3e917638#file-js_baseline-js) where we just splice all the edits directly into a javascript string. This throws away all the information we need to do collaborative editing, but it gives us a sense of how fast javascript is capable of going. It turns out javascript running on V8 is *fast*:

| Test                              | Time taken | RAM usage |
|:--------------------------        | ----------:| ---------:|
| **automerge (v1.0.0-preview2)**   |  291s      | 880 MB    |
| *Plain string edits in JS*        | 0.61s      | 0.1 MB    |

This is a chart showing the time taken to process each operation throughout the test, averaged in groups of 1000 operations. I think those spikes are V8's garbage collector trying to free up memory.

![automerge performance chart](am_perf1.svg)

In the slowest spike near the end, a single edit took *1.8 seconds* to process. Oof. In a real application, the whole app (or browser tab) would freeze up for a couple of seconds sometimes while you're in the middle of typing.

The chart is easier to read when we average everything out a bit and zoom the Y axis. We can see the average performance gets gradually (roughly linearly) worse over time.

![automerge performance chart smoothed out](am_perf1_smooth.svg)

### Why is automerge slow though?

Automerge is slow for a whole slew of reasons:

1. Automerge's core tree based data structure gets big and slow as the document grows.
2. Automerge makes heavy use of [Immutablejs](https://immutable-js.github.io/). Immutablejs is a library which gives you clojure-like copy-on-write semantics for javascript objects. This is a cool set of functionality, but the V8 optimizer & GC struggles to optimize code that uses immutablejs. As a result, it increases memory usage and decreases performance.
3. Automerge treats each inserted character as a separate item. Remember that paper I talked about earlier, where copy+paste operations are slow? Automerge does that too!

Automerge was just never written with performance in mind. Their team is working on a replacement [rust implementation of the algorithm](https://github.com/automerge/automerge-rs/) to run through wasm, but at the time of writing it hasn't landed yet. I got the master branch working, but they have some kinks to work out before it's ready. Switching to the automerge-rs backend doesn't make average performance in this test any faster. (Although it does halve memory usage and smooth out performance.)

---

There's an old saying with performance tuning:

> You can't make the computer faster. You can only make it do less work.

How do we make the computer do less work here? There's lots of performance wins to be had from going through the code and improving lots of small things. But the automerge team has the right approach. It's always best to start with macro optimizations. Fix the core algorithm and data structures before moving to optimizing individual methods. There's no point optimizing a function when you're about to throw it away in a rewrite.

By far, Automerge's biggest problem is its complex tree based data structure. And we can replace it with something faster.


## Improving the data structure

Luckily, there's a better way to implement CRDTs, pioneered in [Yjs](https://github.com/yjs/yjs). Yjs is another (competing) opensource CRDT implementation made by Kevin Jahns. It's fast, well documented and well made. If I were going to build software which supports collaborative editing today, I'd use Yjs.

Yjs doesn't need a whole blog post talking about how to make it fast because it's already pretty fast, as we'll see soon. It got there by using a clever, obvious data structure "trick" that I don't think anyone else in the field has noticed. Instead of implementing the CRDT as a tree like automerge does:

```javascript
state = {
  { item: 'a', id: ['seph', 0], seq: 0, children: [
    { item: 'X', id, seq, children: []},
    { item: 'b', id, seq, children: [
      { item: 'c', id, seq, children: []}
    ]}
  ]}
}
```

Yjs just puts all the items in a single flat list:

```javascript
state = [
  { item: 'a', id: ['seph', 0], seq: 0, parent: null },
  { item: 'X', id, seq, parent: ['seph', 0] },
  { item: 'b', id, seq, parent: ['seph', 0] },
  { item: 'c', id, seq, parent: [..] }
]
```

That looks simple, but how do you insert a new item into a list? With automerge it's easy:

1. Find the parent item
2. Insert the new item into the right location in the parents' list of children

But with this list approach it's more complicated:

1. Find the parent item
2. Starting right after the parent item, iterate through the list until we find the location where the new item should be inserted (?)
3. Insert it there, splicing into the array

Essentially, this approach is just a fancy insertion sort. We're implementing a list CRDT with a list. Genius!

This sounds complicated - how do you figure out where the new item should go? But it's complicated in the same way *math* is complicated. It's hard to understand, but once you understand it, you can implement the whole insert function in about 20 lines of code:

(But don't be alarmed if this looks confusing - we could probably fit everyone on the planet who understands this code today into a small meeting room.)

```javascript
const automergeInsert = (doc, newItem) => {
  const parentIdx = findItem(doc, newItem.parent) // (1)

  // Scan to find the insert location
  let i
  for (i = parentIdx + 1; i < doc.content.length; i++) {
    let o = doc.content[i]
    if (newItem.seq > o.seq) break // Optimization.
    let oparentIdx = findItem(doc, o.parent)

    // Should we insert here? (Warning: Black magic part)
    if (oparentIdx < parentIdx
      || (oparentIdx === parentIdx
        && newItem.seq === o.seq
        && newItem.id[0] < o.id[0])
    ) break
  }
  // We've found the position. Insert at position *i*.
  doc.content.splice(i, 0, newItem) // (2)

  // .. And do various bookkeeping.
}
```

I implemented both Yjs's CRDT (YATA) and Automerge using this approach in my experimental [*reference-crdts*](https://github.com/josephg/reference-crdts/blob/main/crdts.ts) codebase. [Here's the insert function, with a few more comments](https://github.com/josephg/reference-crdts/blob/fed747255df9d457e11f36575de555b39f07e909/crdts.ts#L401-L459). The Yjs version of this function is in the same file, if you want to have a look. Despite being very different papers, the logic for inserting is almost identical. And even though my code is very different, this approach is *semantically* identical to the actual automerge, and Yjs and sync9 codebases. ([Fuzzer verified (TM)](https://github.com/josephg/reference-crdts/blob/main/reference_test.ts)).

If you're interested in going deeper on this, I gave [a talk about this approach](https://invisiblecollege.s3-us-west-1.amazonaws.com/braid-meeting-10.mp4#t=300) at a [braid](https://braid.org/) meeting a few weeks ago.

The important point is this approach is better:

1. We can use a flat array to store everything, rather than an unbalanced tree. This makes everything smaller and faster for the computer to process.
2. The code is really simple. Being faster *and* simpler moves the [Pareto efficiency frontier](https://en.wikipedia.org/wiki/Pareto_efficiency). Ideas which do this are rare and truly golden.
3. You can implement lots of CRDTs like this. Yjs, Automerge, Sync9 and others work. You can implement many list CRDTs in the same codebase. In my reference-crdts codebase I have an implementation of both RGA (automerge) and YATA (Yjs). They share most of their code (everything except this one function) and their performance in this test is identical.

Theoretically this algorithm can slow down when there are concurrent inserts in the same location in the document. But that's really rare in practice - you almost always just insert right after the parent item.

Using this approach, my implementation of automerge's algorithm is about 10x faster than the real automerge. And it's 30x more memory-efficient:

| Test                              | Time taken | RAM usage |
|:--------------------------        | ----------:| ---------:|
| automerge (v1.0.0-preview2)       |  291s      | 880 MB    |
| **reference-crdts (automerge / Yjs)** |   31s      |  28 MB    |
| *Plain string edits in JS*        | 0.61s      | 0.1 MB    |

I wish I could attribute *all* of that difference to this sweet and simple data structure. But a lot of the difference here is probably just immutablejs gumming automerge up.

It's a lot faster than automerge:

![Automerge is much slower than reference-crdts](ref_vs_am_perf.svg)


## Death by 1000 scans

We're using a clean and fast core data abstraction now, but the implementation is still not *fast*. There are two big performance bottlenecks in this codebase we need to fix:

1. Finding the location to insert, and
2. Actually inserting into the array

(These lines are marked *(1)* and *(2)* in the code listing above).

To understand why this code is necessary, lets say we have a document, which is a list of items.

```javascript
state = [
  { item: 'a', isDeleted: false, id: ['seph', 0], seq, parent: null },
  { item: 'X', isDeleted: false, id, seq, parent: ['seph', 0] },
  { item: 'b', isDeleted: true,  id, seq, parent: ['seph', 0] },
  { item: 'c', isDeleted: false, id, seq, parent: ['seph', 1] },
  ...
]
```

And some of those items might have been deleted. I've added an `isDeleted` flag to mark which ones. (Unfortunately we can't just remove them from the array because other inserts might depend on them. Drat! But that's a problem for another day.)

Imagine the document has 150 000 array items in it, representing 100 000 characters which haven't been deleted. If the user types an 'a' in the middle of the document (at *document position* 50 000), what index does that correspond to in our array? To find out, we need to scan through the document (skipping deleted items) to figure out the right array location.

So if the user inserts at position 50 000, we'll probably have to linearly scan past 75 000 items or something to find the insert position. Yikes!

And then when we actually insert, the code does this, which is double yikes:

```javascript
doc.content.splice(destIdx, 0, newItem)
```

If the array currently has 150 000 items, javascript will need to move every single item *after* the new item once space forward in the array. This part happens in native code, but it's still probably slow when we're moving so many items. (Aside: V8 is actually suspiciously fast at this part, so maybe v8 isn't using an array internally to implement Arrays? Who knows!)

But in general, inserting an item into a document with *n* items will take about *n* steps. Wait, no - it's worse than that because deleted items stick around. Inserting into a document where there have *ever been* *n* items will take *n* steps. This algorithm is reasonably fast, but it gets slower with every keystroke. Inserting *n* characters will take *O(n^2)*.

You can see this if we zoom in on the diagram above. There's a lot going on here because Martin's editing position bounced around the document. But there's a strong linear trend up and to the right, which is what we would expect when inserts take *O(n)* time:

![reference crdts implementation zoomed in](ref_perf3.svg)

And why this shape in particular? And why does performance get better near the end? If we simply graph *where* each edit happened throughout the editing trace, with the same bucketing and smoothing, the result is a very familiar curve:

![Edit position throughout document](inspos.svg)

It looks like the time spent applying changes is dominated by the time it takes to scan through the document's array.

## Changing the data structure

Can we fix this? Yes we can! And by "we", I mean Kevin fixed these problems in Yjs. How did he manage that?

So remember, there are two problems to fix:

1. How do we find a specific insert position?
2. How do we efficiently insert content at that location?

Kevin solved the first problem by thinking about how humans actually edit text documents. Usually while we're typing, we don't actually bounce around a document very much. Rather than scanning the document each time an edit happens, Yjs caches the last *(index, position)* pair where the user made an edit. The next edit will probably be pretty close to the previous edit, so Kevin just scans forwards or backwards from the last editing position. This sounds a little bit dodgy to me - I mean, thats a big assumption to make! What if edits happen randomly?! But people don't actually edit documents randomly, so it works great in practice.

(What if two users are editing different parts of a document at the same time? Yjs actually stores a whole set of cached locations, so there's almost always a cached cursor location near each user no matter where they're making changes in the document.)

Once Yjs finds the target insert location, it needs to insert efficiently, without copying all the existing items. Yjs solves that by using a bidirectional linked list instead of an array. So long as we have an insert position, linked lists allow inserts in constant time.

Yjs does one more thing to improve performance. Humans usually type in runs of characters. So when we type "hello" in a document, instead of storing:

```javascript
state = [
  { item: 'h', isDeleted: false, id: ['seph', 0], seq, parent: null },
  { item: 'e', isDeleted: false, id: ['seph', 1], seq, parent: ['seph', 0] },
  { item: 'l', isDeleted: false, id: ['seph', 2], seq, parent: ['seph', 1] },
  { item: 'l', isDeleted: false, id: ['seph', 3], seq, parent: ['seph', 2] },
  { item: 'o', isDeleted: false, id: ['seph', 4], seq, parent: ['seph', 3] },
]
```

Yjs just stores:

```javascript
state = [
  { item: 'hello', isDeleted: false, id: ['seph', 0], seq, parent: null },
]
```

Finally those pesky paste events will be fast too!

This is the same information, just stored more compactly. Unfortunately we can't collapse the whole document into a single item or something like that using this trick. The algorithm can only collapse inserts when the IDs and parents line up sequentially - but that happens whenever a user types a run of characters without moving their cursor. And that happens a lot.

In this data set, using spans reduces the number of array entries by 14x. (180k entries down to 12k).

How fast is it now? This blows me away - Yjs is 30x faster than my reference-crdts implementation in this test. And it only uses about 10% as much RAM. It's *300x faster than automerge!*.

| Test                              | Time taken | RAM usage |
|:--------------------------        | ----------:| ---------:|
| automerge (v1.0.0-preview2)       |  291s      | 880 MB    |
| reference-crdts (automerge / Yjs) |   31s      |  28 MB    |
| **Yjs (v13.5.5)**                 | 0.97s      | 3.3 MB    |
| *Plain string edits in JS*        | 0.61s      | 0.1 MB    |

Honestly I'm shocked and a little suspicious of how little ram Yjs uses in this test. I'm sure there's some wizardry in V8 making this possible. It's extremely impressive.

Kevin says he wrote and rewrote parts of Yjs 12 times in order to make this code run so fast. If there was a programmer version of the speedrunning community, they would adore Kevin. I can't even put Yjs on the same scale as the other algorithms because it's so fast:

![Yjs performance vs other algorithms](yjs_perf4.svg)

If we isolate Yjs, you can see it has *mostly* flat performance. Unlike the other algorithms, it doesn't get slower over time, as the document grows:

![Yjs performance isolated](yjs_perf5.svg)

But I have no idea what those spikes are near the end. They're pretty small in *absolute* terms, but it's still weird! Maybe they happen when the user moves their cursor around the document? Or when the user deletes chunks? I have no idea.

This is neat, but the real question is: Can we go *even faster*? Honestly I doubt I can make pure javascript run this test any faster than Kevin managed here. But maybe.. just maybe we can be...

## Faster than Javascript

When I told Kevin that I thought I could make a CRDT implementation that's way faster than Yjs, he didn't believe me. He said Yjs was already so well optimized, going a lot faster probably wasn't possible. "Maybe a little faster if you just port it to Rust. But not a lot faster! V8 is really fast these days!!"

But I knew something Kevin didn't know: I knew about memory fragmentation and caches. Rust isn't just *faster*. It's also a lower level language, and that gives us the tools we need to control allocations and memory layout.

> Kevin knows this now too, and he's working on [Yrs](https://github.com/yjs/y-crdt) to see if he can claim the performance crown back.

Imagine one of our document items in javascript:

```javascript
var item = {
  content: 'hello',
  isDeleted: false,
  id: ['seph', 10],
  seq: 5,
  parent: ['mike', 2]
}
```

This object is actually a mess like this in memory:

![javascript objects fragmented in memory](mem-frag.drawio.svg)

Bad news: *Your computer hates this.*

This is terrible because all the data is fragmented. It's all separated by pointers.

> And yes, I know, V8 tries its hardest to prevent this sort of thing when it can. But its not magic.

To arrange data like this, the computer has to allocate memory one by one for each item. This is slow. Then the garbage collector needs extra data to track all of those objects, which is also slow. Later we'll need to read that data. To read it, your computer will often need to go fetch it from main memory, which - you guessed it - is slow as well.

How slow are main memory reads? [At human scale](https://gist.github.com/hellerbarde/2843375) each L1 cache read takes 0.5 seconds. And a read from main memory takes close to 2 minutes! This is the difference between a single heartbeat, and the time it takes to brush your teeth.

Arranging memory like javascript does would be like writing a shopping list. But instead of "Cheese, Milk, Bread", your list is actually a scavenger hunt: "Under the couch", "On top of the fridge", and so on. Under the couch is a little note mentioning you need toothpaste. Needless to say, this makes doing the grocery shopping a lot of work.

To go faster, we need to squish all the data together so the computer can fetch more information with each read of main memory. (We want a single read of my grocery list to tell us everything we need to know). Linked lists are rarely used in the real world for exactly this reason - *memory fragmentation ruins performance*. I also want to move away from linked lists because the user *does* sometimes hop around the document, which in Yjs has a linear performance cost. Thats probably not a big deal in text editing, but I want this code to be fast in other use cases too. I don't want the program to *ever* need those slow scans.

We can't fix this in javascript. The problem with fancy data structures in javascript is that you end up needing a lot of exotic objects (like fixed size arrays). All those extra objects make fragmentation worse, so as a result of all your work, your programs often end up running slower anyway. This is the same limitation immutablejs has, and why its performance hasn't improved much in the decade since it was released. The V8 optimizer is very clever, but it's not magic and clever tricks only get us so far.

But we're not limited to javascript. Even when making webpages, we have WebAssembly these days. We can code this up in *anything*.

To see how fast we can *really* go, I've been quietly building a CRDT implementation in rust called [Diamond types](https://github.com/josephg/diamond-types). Diamond is almost identical to Yjs, but it uses a [range tree](https://en.wikipedia.org/wiki/Range_tree) instead of a linked list internally to store all of the items.

Under the hood, my range tree is just a slightly modified b-tree. But usually when people talk about b-trees they mean a [BTreeMap](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html). Thats not what I'm doing here. Instead of storing keys, each internal node of the b-tree stores the total number of characters (recursively) in that item's children. So we can look up any item in the document by character position, or insert or delete anywhere in the document in *log(n)* time.

This example shows the tree storing a document which currently has 1000 characters:

![b-tree diagram](btree.drawio.svg)

> This is a range tree, right? The [wikipedia article on range trees](https://en.wikipedia.org/wiki/Range_tree) is a pretty weak description of what I'm doing here.

This solves both of our linear scanning problems from earlier:

- When we want to find the item at position 200, we can just traverse across and down the tree. In the example above, the item with position 350 must be in the middle leaf node here. Trees are very tidy - we can store Martin's editing trace in just 3 levels in our tree, which means in this benchmark we can find any item in about 3 reads from main memory. In practice, most of these reads will already be in your CPU's cache.
- Updating the tree is fast too. We update a leaf, then update the character counts at its parent, and its parent, all the way up to the root. So again, after 3 or so steps we're done. Much better than shuffling everything in a javascript array.

We never merge edits from remote peers in this test, but I made that fast too anyway. When merging remote edits we also need to find items by their ID (eg *['seph', 100]*). Diamond has little index to search the b-tree by ID. That codepath doesn't get benchmarked here though. It's fast but for now you'll have to take my word for it.

I'm not using Yjs's trick of caching the last edit location - at least not yet. It might help. I just haven't tried it yet.

Rust gives us total control over the memory layout, so we can pack everything in tightly. Unlike in the diagram, each leaf node in my b-tree stores a block of 32 entries, packed in a fixed size array in memory. Inserting with a structure like this results in a little bit of memcpy-ing, but a little bit of memcpy is fine. Memcpy is always faster than I think it will be - CPUs can copy several bytes per clock cycle. Its not the epic hunt of a main memory lookup.

And why 32 entries? I ran this benchmark with a bunch of different bucket sizes and 32 worked well. I have no idea why that worked out to be the best.

Speaking of fast, how fast does it go?

If we [compile this code to webassembly](https://github.com/josephg/diamond-js) and drive it from javascript like in the other tests, we can now process the whole editing trace in 193 milliseconds. Thats 5x faster than Yjs. And remarkably 3x faster than our baseline test editing a native javascript string, despite doing all the work to support collaborative editing!

Javascript and WASM is now a bottleneck. If we skip javascript and run the benchmark [directly in rust](https://github.com/josephg/diamond-types/blob/42a8bc8fb4d44671147ccaf341eee18d77b2d532/benches/yjs.rs), we can process all 260k edits in this editing trace in just *56 milliseconds*. That's over 5000x faster than where we started with automerge. It can process 4.6 *million* operations every second.

| Test                              | Time taken | RAM usage |
|:--------------------------        | ----------:| ---------:|
| automerge (v1.0.0-preview2)       |  291s      | 880 MB    |
| reference-crdts (automerge / Yjs) |   31s      |  28 MB    |
| Yjs (v13.5.5)                     | 0.97s      | 3.3 MB    |
| *Plain string edits in JS*        | 0.61s      | 0.1 MB    |
| **Diamond (wasm via nodejs)**     | 0.19s      | ???       |
| **Diamond (native)**              | 0.056s     | 1.1 MB    |

Performance is smooth as butter. A b-tree doesn't care where edits happen. This system is uniformly fast across the whole document. Rust doesn't need a garbage collector to track memory allocations, so there's no mysterious GC spikes. And because memory is so tightly packed, processing this entire data set (all 260 000) only results in 1394 calls to malloc.

![rust implementation in wasm vs Yjs](rust_perf6.svg)

Oh, what a pity. Its so fast you can barely see it next to yjs (*fleexxxx*). Lets zoom in a bit there and bask in that flat line:

![rust implementation in wasm](rust_perf7.svg)

Well, a nearly flat line.

And remember, this chart shows the *slow* version. This chart is generated from javascript, calling into rust through WASM. If I run this benchmark natively its another ~4x faster again.

Why is WASM 4x slower than native execution? Are javascript calls to the WASM VM really that slow? Does LLVM optimize native x86 code better? Or do WASM's memory bounds checks slow it down? I'm so curious!


## Struct of arrays or Array of structs?

This implementation has another small, important change - and I'm not sure if I like it.

In rust I'm actually doing something like this:

```javascript
doc = {
  textContent: RopeyRope { 'hello' },

  clients: ['seph', 'mike'],

  items: BTree {[
    // Note: No string content!
    { len:  5, id: [0, 0], seq, parent: ROOT },
    { len: -5, id: [1, 0], seq, parent: [0, 0] }, // negative len means the content was deleted
    ...
  ]},
}
```

Notice the document's text content doesn't live in the list of items anymore. Now it's in a separate data structure. I'm using a rust library for this called [Ropey](https://crates.io/crates/ropey). Ropey implements *another* b-tree to efficiently manage just the document's text content.

This isn't universally a win. We have unfortunately arrived at the Land of Uncomfortable Engineering Tradeoffs:

- Ropey can to do text-specific byte packing. So with ropey, we use less RAM.
- When inserting we need to update 2 data structures instead of 1. This makes everything more than twice as slow, and it makes the wasm bundle twice as big  (60kb -> 120kb).
- For lots of use cases we'll end up storing the document content somewhere else anyway. For example, if you hook this CRDT up to VS Code, the editor will keep a copy of the document at all times anyway. So there's no need to store the document in my CRDT structures as well, at all. This implementation approach makes it easy to just turn that part of the code off.

So I'm still not sure whether I like this approach.

But regardless, my CRDT implementation is so fast at this point that most of the algorithm's time is spent updating the document contents in ropey. Ropey on its own takes 29ms to process this editing trace. What happens if I just ... turn ropey off? How fast can this puppy can really go?

| Test                              | Time taken | RAM usage | Data structure |
|:--------------------------        | ----------:| ---------:|:---------------|
| automerge (v1.0.0-preview2)       |  291s      | 880 MB    | Naive tree     |
| reference-crdts (automerge / Yjs) |   31s      |  28 MB    | Array          |
| Yjs (v13.5.5)                     | 0.97s      | 3.3 MB    | Linked list    |
| *Plain string edits in JS*        | 0.61s      | 0.1 MB    | *(none)*       |
| Diamond (wasm via nodejs)         | 0.20s      | ???       | B-Tree         |
| Diamond (native)                  | 0.056s     | 1.1 MB    | B-Tree         |
| *Ropey (rust) baseline*           | 0.029s     | 0.2 MB    | *(none)*       |
| **Diamond (native, no doc content)** | 0.023s  | 0.96 MB   | B-Tree         |

Boom. This is kind of useless, but it's now 14000x faster than automerge. We're processing 260 000 operations in 23ms. Thats 11 million operations per second. I could saturate my home internet connection with keystrokes and I'd still have CPU to spare.

---

We can calculate the average speed each algorithm processes edits:

![](totals.svg)

But these numbers are misleading. Remember, automerge and ref-crdts aren't steady. They're fast at first, then slow down as the document grows. Even though automerge can process about 900 edits per second *on average* (which is fast enough that users won't notice), the slowest edit during this benchmark run stalled V8 for a full 1.8 seconds.

We can put everything in a single, pretty chart if I use a log scale. It's remarkable how tidy this looks:

![all data in one chart](all_perf.svg)

> Huh - look at the bottom two lines. The jitteryness of yjs and diamond mirror each other. Periods when yjs gets slower, diamond gets faster. I wonder whats going on there!

But log scales are junk food for your intuition. On a linear scale the data looks like this:

![all data in one chart, with a linear scale](all_perf_linear.svg)

That, my friends, is how you make the computer do a lot less work.


## Conclusion

That silly academic paper I read all those years ago says some CRDTs and OT algorithms are slow. And everyone believed the paper, because it was Published Science. But the paper was wrong. As I've shown, we *can* make CRDTs fast. We can make them *crazy fast* if we get creative with our implementation strategies. With the right approach, we can make CRDTs so fast that we can compete with the performance of native strings. The performance numbers in that paper weren't just wrong. They were "a billionaire guessing a banana costs $1000" kind of wrong.

But you know what? I sort of appreciate that paper now. Their mistake is ok. It's *human*. I used to feel inadequate around academics - maybe I'll never be that smart! But this whole thing made me realise something obvious: Scientists aren't gods, sent from the heavens with the gift of Truth. No, they're beautiful, flawed *people* just like the rest of us mooks. Great at whatever we obsess over, but kind of middling everywhere else. I can optimize code pretty well, but I still get zucchini and cucumber mixed up. And, no matter the teasing I get from my friends, thats ok.

A decade ago Google Wave really needed a good quality list CRDT. I got super excited when the papers for CRDTs started to emerge. [LOGOOT](https://hal.inria.fr/inria-00432368/document) and [WOOT](https://hal.inria.fr/inria-00445975/document) seemed like a big deal! But that excitement died when I realised the algorithms were too slow and inefficient to be practically useful. And I made a big mistake - I assumed if the academics couldn't make them fast, nobody could.

But sometimes the best work comes out of a collaboration between people with different skills. I'm terrible at academic papers, I'm pretty good at making code run fast. And yet here, in my own field, I didn't even try to help. The researchers were doing their part to make P2P collaborative editing work. And I just thumbed my nose at them all and kept working on Operational Transform. If I helped out, maybe we would have had fast, workable CRDTs for text editing a decade ago. Oops! It turned out collaborative editing needed a collaboration between all of us. How ironic! Who could have guessed?!

Well, it took a decade, some hard work and some great ideas from a bunch of clever folks. The binary encoding system Martin invented for Automerge is brilliant. The system of avoiding UUIDs by using incrementing (agent id, sequence) tuples is genius. I have no idea who came up with that, but I love it. And of course, Kevin's list representation + insertion approach I describe here makes everything so much faster and simpler. I bet 100 smart people must have walked right past that idea over the last decade without any of them noticing it. I doubt I would have thought of it either. My contribution is using run-length encoded b-trees and clever indexing. And showing Kevin's fast list representation can be adapted to any CRDT algorithm. I don't think anyone noticed that before.

And now, after a decade of waiting, we finally figured out how to make fast, lightweight list CRDT implementations. Practical decentralized realtime collaborative editing? We're coming for you next.


## Appendix A: I want to use a CRDT for my application. What should I do?

If you're building a document based collaborative application today, you should use [Yjs](https://github.com/yjs/yjs). Yjs has solid performance, low memory usage and great support. If you want help implementing Yjs in your application, Kevin Jahns sometimes accepts money in exchange for help integrating Yjs into various applications. He uses this to fund working on Yjs (and adjacent work) full time. Yjs already runs fast and soon it should become even faster.

The automerge team is also fantastic. I've had some great conversations with them about these issues. They're making performance the #1 issue of 2021 and they're planning on using a lot of these tricks to make automerge fast. It might already be much faster by the time you're reading this.

Diamond is *really* fast, but there's a lot of work before I have feature parity with Yjs and Automerge. There is a lot more that goes into a good CRDT library than operation speed. CRDT libraries also need to support binary encoding, network protocols, non-list data structures, presence (cursor positions), editor bindings and so on. At the time of writing, diamond does almost none of this.

If you want database semantics instead of document semantics, as far as I know nobody has done this well on top of CRDTs yet. You can use [ShareDB](https://github.com/share/sharedb/), which uses OT. I wrote ShareDB years ago, and it's well used, well maintained and battle tested.

Looking forward, I'm excited for [Redwood](https://github.com/redwood/redwood) - which supports P2P editing and has planned full CRDT support.


## Appending B: Lies, damned lies and benchmarks

Is this for real? Yes. But performance is complicated and I'm not telling the full picture here.

First, if you want to play with any of the benchmarks I ran yourself, you can. But everything is a bit of a mess.

The benchmark code for the JS plain string editing baseline, Yjs, automerge and reference-crdts tests is all in [this github gist](https://gist.github.com/josephg/13efc1444660c07870fcbd0b3e917638). It's a mess; but messy code is better than missing code.

You'll also need `automerge-paper.json.gz` from [josephg/crdt-benchmarks](https://github.com/josephg/crdt-benchmarks) in order to run most of these tests. The reference-crdts benchmark depends on [crdts.ts from josephg/reference-crdts, at this version](https://github.com/josephg/reference-crdts/tree/fed747255df9d457e11f36575de555b39f07e909).

Diamond's benchmarks come from [josephg/diamond-types, at this version](https://github.com/josephg/diamond-types/tree/42a8bc8fb4d44671147ccaf341eee18d77b2d532). Benchmark by running ` RUSTFLAGS='-C target-cpu=native' cargo criterion yjs`. The inline rope structure updates can be enabled or disabled by editing [the constant at the top of src/list/doc.rs](https://github.com/josephg/diamond-types/blob/42a8bc8fb4d44671147ccaf341eee18d77b2d532/src/list/doc.rs#L15). You can look at memory statistics by running `cargo run --release --features memusage --example stats`.

Diamond is compiled to wasm using [this wrapper](https://github.com/josephg/diamond-js/tree/6e8a95670b651c0aaa7701a1a763778d3a486b0c), hardcoded to point to a local copy of diamond-types from git. The wasm bundle is optimized with wasm-opt.

The charts were made on [ObservableHQ](https://observablehq.com/@josephg/crdt-algorithm-performance-benchmarks).


### Are Automerge and Yjs doing the same thing?

Throughout this post I've been comparing the performance of implementations of RGA (automerge) and YATA (Yjs + my rust implementation) interchangeably.

Doing this rests on the assumption that the concurrent merging behaviour for YATA and RGA are basically the same, and that you can swap between CRDT behaviour without changing your implementation, or your implementation performance. This is a novel idea that I think nobody has looked at before.

I feel confident in this claim because I demonstrated it in my [reference CRDT implementation](https://github.com/josephg/reference-crdts), which has identical performance (and an almost identical codepath) when using Yjs or automerge's behaviour. There might be some performance differences with conflict-heavy editing traces - but that's extremely rare in practice.

I'm also confident you could modify Yjs to implement RGA's behaviour if you wanted to, without changing Yjs's performance. You would just need to:

- Change Yjs's *integrate* method (or make an alternative) which used slightly different logic for concurrent edits
- Store *seq* instead of *originRight* in each *Item*
- Store *maxSeq* in the document, and keep it up to date and
- Change Yjs's binary encoding format.

I talked to Kevin about this, and he doesn't see any point in adding RGA support into his library. It's not something anybody actually asks for. And RGA can have weird [interleaving](https://www.cl.cam.ac.uk/~arb33/papers/KleppmannEtAl-InterleavingAnomalies-PaPoC2019.pdf) when prepending items.

For diamond, I make my code accept a type parameter for switching between Yjs and automerge's behaviour. I'm not sure if I want to. Kevin is probably right - I don't think this is something people ask for.

---

Well, there is one way in which Yjs has a definite edge over automerge: Yjs doesn't record *when* each item in a document has been deleted. Only whether each item has been deleted or not. This has some weird implications:

- Storing when each delete happened has a weirdly large impact on memory usage and on-disk storage size. Adding this data doubles diamond's memory usage from 1.12mb to 2.34mb, and makes the system about 5% slower.
- Yjs doesn't store enough information to implement per-keystroke editing replays or other fancy stuff like that. (Maybe thats what people want? Is it weird to have every errant keystroke recorded?)
- Yjs needs to encode information about which items have been deleted into the *version* field. In diamond, versions are tens of bytes. In yjs, versions are ~4kb. And they grow over time as the document grows. Kevin assures me that this information is basically always small in practice. He might be right but this still makes me weirdly nervous.

For now, the master branch of diamond includes temporal deletes. But all benchmarks in this blog post use a [yjs-style branch of diamond-types](https://github.com/josephg/diamond-types/tree/yjs-style), which matches how Yjs works instead. This makes for a fairer comparison with yjs, but diamond 1.0 might have a slightly different performance profile. (There's plenty of puns here about diamond not being polished yet, but I'm not sharp enough for those right now.)


### These benchmarks measure the wrong thing

This post only measures the time taken to replay a local editing trace. And I'm measuring the resulting RAM usage. Arguably accepting incoming changes from the user only needs to happen fast *enough*. Fingers simply don't type very fast. Once a CRDT can handle any local user edit in under about 1ms, going faster probably doesn't matter much. (And automerge usually performs that well already, barring some unlucky GC pauses.)

The *actually important* metrics are:

- How many bytes does a document take on disk or over the network
- How much time does the document take to save and load
- How much time it takes to update a document stored at rest (more below)

The editing trace I'm using here also only has a single user making edits. There could be pathological performance cases lurking in the shadows when users make concurrent edits.

I did it this way because I haven't implemented a binary format in my reference-crdts implementation or diamond yet. If I did, I'd probably copy Yjs & automerge's binary formats because they're so compact. So I expect the resulting binary size would be similar between all of these implementations, except for delete operations. Performance for loading and saving will probably approximately mirror the benchmarks I showed above. Maybe. Or maybe I'm wrong. I've been wrong before. It would be fun to find out.

---

There's one other performance measure I think nobody is taking seriously enough at the moment. And that is, how we update a document at rest (in a database). Most applications aren't collaborative text editors. Usually applications are actually interacting with databases full of tiny objects. Each of those objects is very rarely written to.

If you want to update a single object in a database using Yjs or automerge today you need to:

1. Load the whole document into RAM
2. Make your change
3. Save the whole document back to disk again

This is going to be awfully slow. There are better approaches for this - but as far as I know, nobody is working on this at all. We could use your help!

> Edit: Kevin says you can adapt Yjs's providers to implement this in a reasonable way. I'd love to see that in action.

---

There's another approach to making CRDTs fast, which I haven't mentioned here at all and that is *pruning*. By default, list CRDTs like these only ever grow over time (since we have to keep tombstones for all deleted items). A lot of the performance and memory cost of CRDTs comes from loading, storing and searching that growing data set. There are some approaches which solve this problem by finding ways to shed some of this data entirely. For example, Yjs's GC algorithm, or [Antimatter](https://braid.org/antimatter). That said, git repositories only ever grow over time and nobody seems mind too much. Maybe it doesn't matter so long as the underlying system is fast enough?

But pruning is orthogonal to everything I've listed above. Any good pruning system should also work with all of the algorithms I've talked about here.


### Each step in this journey changes too many variables

Each step in this optimization journey involves changes to multiple variables and I'm not isolating those changes. For example, moving from automerge to my reference-crdts implementation changed:

- The core data structure (tree to list)
- Removed immutablejs
- Removed automerge's frontend / backend protocol. And all those Uint8Arrays that pop up throughout automerge for whatever reason are gone too, obviously.
- The javascript style is totally different. (FP javascript -> imperative)

We got 10x performance from all this. But I'm only guessing how that 10x speedup should be distributed amongst all those changes.

The jump from reference-crdts to Yjs, and from Yjs to diamond are similarly monolithic. How much of the speed difference between diamond and Yjs has nothing to do with memory layout, and everything to do with LLVM's optimizer?

The fact that automerge-rs isn't faster than automerge gives me some confidence that diamond's performance isn't just thanks to rust. But I honestly don't know.

So, yes. This is a reasonable criticism of my approach. If this problem bothers you, I'd *love* for someone to pull apart each of the performance differences between implementations I show here and tease apart a more detailed breakdown. I'd read the heck out of that. I love benchmarking stories. That's normal, right?


## Appendix C: I still don't get it - why is automerge's javascript so slow?

Because it's not trying to be fast. Look at this code [from automerge](https://github.com/automerge/automerge/blob/d2e7ca2e141de0a72f540ddd738907bcde234183/backend/op_set.js#L649-L659):

```javascript
function lamportCompare(op1, op2) {
  return opIdCompare(op1.get('opId'), op2.get('opId'))
}

function insertionsAfter(opSet, objectId, parentId, childId) {
  let childKey = null
  if (childId) childKey = Map({opId: childId})

  return opSet
    .getIn(['byObject', objectId, '_following', parentId], List())
    .filter(op => op.get('insert') && (!childKey || lamportCompare(op, childKey) < 0))
    .sort(lamportCompare)
    .reverse() // descending order
    .map(op => op.get('opId'))
}
```

This is called on each insert, to figure out how the children of an item should be sorted. I don't know how hot it is, but there are *so many things* slow about this:

- I can spot 7 allocations in this function. (Though the 2 closures should be hoisted). (Can you find them all?)
- The items are already sorted reverse-lamportCompare before this method is called. Sorting an anti-sorted list is the slowest way to sort anything. Rather than sorting, then reverse()'ing, this code should just invert the arguments in `lamportCompare` (or negate the return value).
- The goal is to insert a new item into an already sorted list. You can do that much faster with a for loop.
- This code wraps childId into an immutablejs Map, just so the argument matches `lamportCompare` - which then unwraps it again. Stop - I'm dying!

But in practice this code is going to be replaced by WASM calls through to [automerge-rs](https://github.com/automerge/automerge-rs). Maybe it already has been replaced with automerge-rs by the time you're reading this! So it doesn't matter. Try not to think about it. Definitely don't submit any PRs to fix all the low hanging fruit. *twitch.*


## Acknowledgements

This post is part of the [Braid project](https://braid.org/) and funded by the [Invisible College](https://invisible.college/). If this is the sort of work you want to contribute towards, get in touch. We're hiring.

Thankyou to everyone who gave feedback before this post went live.

And special thanks to Martin Kleppmann and Kevin Jahns for their work on Automerge and Yjs. Diamond stands on the shoulders of giants.

[Comments on Hacker News](https://news.ycombinator.com/item?id=28017204)

<footer>

[2021 Seph Gentle](https://josephg.com/)

[https://github.com/josephg/](https://github.com/josephg/)

</footer><script lang="ts">
import type { HtmlTag } from 'svelte/internal';
import type { GameConfig } from './shared';

import * as topicIcons from './topicicons.json'
import topicSpecial from './topicspecial'

export let room: string

export let connection: 'waiting' | 'connecting' | 'connected'

export let game_config: GameConfig
// export let state // loading, waiting, playing, paused.
// export let start_time
// export let topic
// export let meditate
// export let players
// export let rounds
// export let seconds_per_bead
// export let paused_progress

export let _active_sessions: number
export let _magister: true | null
export let _clock_offset: number

// let game_completed = false // Derived from other properties

let round_audio: HTMLAudioElement
let complete_audio: HTMLAudioElement
let topic_img: HTMLElement
let topic_text: HTMLElement

round_audio = new Audio()
round_audio.src = "/lo-metal-tone.mp3"
complete_audio = new Audio()
complete_audio.src = "/hi-metal-tone.mp3"
// round_audio.preload = 'auto'
	// <audio bind:this={complete_audio} src="/hi-metal-tone.mp3" preload="auto"><track kind="captions"></audio>


let state: GameConfig['state']
$: state = game_config.state

$: console.log('Game configuration changed', game_config)

	// export let state

const ARCHETOPICS = [
  'Truth', 'Human', 'Energy', 'Beauty', 'Beginning', 'End', 'Birth', 'Death',
  'Ego', 'Attention', 'Art', 'Empathy', 'Eutopia', 'Future', 'Game', 'Gift',
  'History', 'Cosmos', 'Time', 'Life', 'Addiction', 'Paradox', 'Shadow', 'Society'
]



let audio_works = true

function test_audio() {
	// This ugly monstrosity brought to you by iOS Safari. 
	// This seems to be the only way to bless the audio
	// objects to be able to play during the game. :/
	
	// let a = new Audio()
	// a.volume = 0.1
	const round_src = round_audio.src
	const complete_src = complete_audio.src
	round_audio.src = complete_audio.src = '/silence.mp3'
	// a.play().then(
	complete_audio.play()
	round_audio.play().then(
		() => {
			audio_works = true
			round_audio.src = round_src
			complete_audio.src = complete_src
			console.log('Audio works')
		},
		() => {
			audio_works = false
			round_audio.src = round_src
			complete_audio.src = complete_src
			console.log('Audio does not work')
		}
	)
}
function fix_audio() {
	console.log('fixxx')
	test_audio()
}
setTimeout(test_audio, 0)
document.onclick = () => {
	if (!audio_works) test_audio()
}

const fixed_rand = Math.random()
const randInt = (n: number) => Math.floor(fixed_rand * n)
function randItem<T>(arr: T[]) {return arr[randInt(arr.length)] }

$: {
	if (topic_img && topic_text) {
		const topic = game_config.topic.toLocaleLowerCase()
		const svgContent = topicIcons[topic as keyof typeof topicIcons]
		const textContent = topicSpecial[topic as keyof typeof topicSpecial]

		if (svgContent) {
			topic_img.innerHTML = svgContent
			topic_text.innerText = ''
		} else if (textContent) {
			topic_img.innerHTML = ''
			topic_text.innerText = randItem(textContent)
		} else {
			topic_img.innerHTML = ''
			topic_text.innerText = game_config.topic
		}
	}
}

// Could make configurable. Eh.
const MEDITATION_SECONDS = 60

interface GameStage {
	label: string,
	type: 'waiting' | 'bead' | 'breath' | 'meditate' | 'contemplation' | 'complete',
	duration: number,
	no_sound?: true,
	r?: number, p?: number,
	id?: string
}

let game_stages: GameStage[] = []
$: {
	game_stages = [{
		label: `${game_config.meditate ? 'Meditation' : 'Game'} starting...`,
		type: 'waiting',
		duration: 3,
		no_sound: true
	}]
	if (game_config.meditate) game_stages.push({
		label: 'Meditate',
		type: 'meditate',
		duration: MEDITATION_SECONDS,
	})
	for (let r = 0; r < game_config.rounds; r++) {
		for (let p = 0; p < game_config.players; p++) {
			if (game_config.seconds_between_bead && (r > 0 || p > 0)) game_stages.push({
				label: 'Breathe',
				duration: game_config.seconds_between_bead,
				type: 'breath',
				id: `b ${r} ${p}`
			})

			game_stages.push({
				label: '',
				// label: game_config.players > 1 ? `Round ${r+1} player ${p+1}` : `Round ${r+1}`,
				duration: game_config.seconds_per_bead,
				type: 'bead', r, p,
				id: `s ${r} ${p}`
			})
		}
	}

	if (game_config.contemplation) game_stages.push({
		label: "Contemplate the game's passing",
		type: 'contemplation',
		duration: MEDITATION_SECONDS,
	})


	console.log('game stages', game_stages, game_config.seconds_between_bead)
}

let total_game_length: number
$: total_game_length = game_stages.reduce((x, s) => x + s.duration, 0)

// Used for the overall game progress indicator.
let inner_game_stages: GameStage[]
$: inner_game_stages = game_stages.filter(s => s.type === 'breath' || s.type === 'bead')
let inner_game_length: number
$: inner_game_length = inner_game_stages.reduce((x, s) => x + s.duration, 0)

// TODO: The protocol for these update methods doesn't use game_state properly.
const update_state = async (patch: Record<string, string | number | boolean | null>) => {
	await fetch(`${room}/configure`, {
		method: 'POST',
		mode: 'same-origin',
		headers: {
			'content-type': 'application/json',
		},
		body: JSON.stringify(patch)
	})
}

const upd = (k: string, v: string | number | boolean | null) => () => update_state({[k]: v})

const config = (k: string): svelte.JSX.FormEventHandler<HTMLInputElement> => (e) => {
	// console.log('k', k, e.data, e.value, e.target.value, e.target.type)
	const target = e.target as HTMLInputElement
	const raw_value = target.value
	const value = target.type === 'number' ? ~~raw_value
		: target.type === 'checkbox' ? target.checked
		: raw_value
	update_state({[k]: value})
}

const roundish = (x: number) => Math.round(x * 10) / 10


const waiting_stage: GameStage = { label: 'Waiting to start', type: 'waiting', duration: Infinity }
const complete_stage: GameStage = { label: 'Game complete', type: 'complete', duration: Infinity }
const get_current_stage = (offset_ms: number): {stage: GameStage, stage_idx: number, offset_sec: number} => {
	if (state === 'waiting') return {stage: waiting_stage, stage_idx: -1, offset_sec: 0}

	let offset_sec = Math.round(offset_ms / 1000)
	for (let s = 0; s < game_stages.length; s++) {
		let stage = game_stages[s]
		if (stage.duration > offset_sec) {
			return {stage, stage_idx: s, offset_sec}
		}
		offset_sec -= stage.duration
	}
	return {
		stage: complete_stage, stage_idx: game_stages.length, offset_sec
	}
}

// Urgh kinda ugly storing state for both the index and stage itself. Better to
// have one derive the other.
let current_stage: GameStage | null = null, current_stage_idx: number = -1, offset_sec: number
$: console.log('current stage', current_stage)
// $: console.log('idx', current_stage_idx)

const tick = (play_audio: boolean) => {
	console.log('tick')
	// console.log('state', state, 'completed', state && state.complete)

	const time = state === 'playing' ? Date.now() + _clock_offset - game_config.start_time
		: state === 'paused' ? game_config.paused_progress!
		: 0
	const {stage: new_stage, stage_idx: new_stage_idx, offset_sec: new_offs} = get_current_stage(time)
	// state_label = state.label

	offset_sec = new_offs
	if (new_stage !== current_stage) {
		console.log('state changed', new_stage.label, new_stage.type === 'complete')

		// This happens sometimes with other kinds of configuration changes -
		// eg if a user enters or leaves the room, or the room is reconfigured.
		// Only make a sound if the *stage* changes.
		let changed = current_stage == null || (new_stage.id ?? new_stage.type) !== (current_stage.id ?? current_stage.type)
		// console.log(new_stage, current_stage, changed)

		current_stage = new_stage
		current_stage_idx = new_stage_idx
		// completed = new_game_state.complete
		// if (!state.complete) round_audio.play()

		if (play_audio && !new_stage.no_sound && changed) {
			if (current_stage.type === 'complete' || current_stage.type === 'contemplation') complete_audio.play()
			else round_audio.play()
		}
	}
}

let timer: number | null | any // Timeout?
$: {
	// Sadly we can't use internal_state here because it generates a cyclic dependancy.
	let completed = current_stage ? current_stage.type === 'complete' : false
	// console.log('xx', state, timer, completed, current_stage)

	// if (state !== 'loading') tick(false)

	if (state === 'playing' && timer == null && !completed) {
		// setTimeout needed to get around some weird race condition.
		// There's probably better ways to structure this :/
		setTimeout(() => tick(false))
		timer = setInterval(() => {
			tick(true)
		}, 1000)
	} else if ((completed || state !== 'playing') && timer != null) {
		console.log('cancelled interval timer')
		clearInterval(timer)
		timer = null
	} else if (state === 'waiting' || state === 'paused') {
		setTimeout(() => tick(false))
	}
}

let game_completed: boolean
$: {
	// console.log('updating game_completed', current_stage)
	game_completed = (state !== 'playing' || current_stage == null) ? false
	: (current_stage.type === 'complete')
}

let internal_state: GameConfig['state'] | 'completed'
$: internal_state = game_completed ? 'completed' : state

let bar_width = 0
$: bar_width = current_stage == null ? 0
	: state === 'waiting' ? 0
	: current_stage.type === 'complete' ? 100
	: 100 * offset_sec / current_stage.duration

let stage_label: string
$: stage_label = internal_state === 'waiting' ? 'Waiting to start'
	: current_stage == null ? 'unknown' : current_stage.label


const progress_class = (stage_idx: number, baseline_idx: number): 's-done' | 's-active' | 's-waiting' => {
	if (current_stage == null || baseline_idx < 0) return 's-waiting'

	return stage_idx < baseline_idx ? 's-done'
		: stage_idx === baseline_idx ? 's-active'
		: 's-waiting'
}

// This will get more complex in time. For now, pause the game to fiddle.
$: settings_disabled = state === 'playing'

let config_open = false

$: if (_magister === true) config_open = true

// The first user has the config open by default.
// $: if (_active_sessions === 1) config_open = true

// The magister box is fully visible once there's a critical mass of players in the room
$: magister_opaque = _magister === true || _active_sessions >= 6

</script>

<svelte:head>
	{#if _magister}
		<style>
body {
	background-color: var(--bg-highlight);
}
		</style>
	{/if}
</svelte:head>

<!-- <main class:magister={_magister}> -->
<main>
	<!-- <audio bind:this={round_audio} src="/lo-metal-tone.mp3" preload="auto" autoplay><track kind="captions"></audio>
	<audio bind:this={complete_audio} src="/hi-metal-tone.mp3" preload="auto"><track kind="captions"></audio> -->

	{#if !audio_works}
		<button id='fixaudio' on:click={fix_audio}>Audio muted. Click to unmute</button>
	{/if}

	{#if internal_state === 'loading'}
		<h1>Loading game state</h1>
	{:else}
		<!-- <h1>Glass Bead Game Timer</h1> -->
		<!-- <h1>{topic}</h1> -->

		<div id='topic'>
			<div id='topicimg' bind:this={topic_img}></div>
			<div id='topictext' bind:this={topic_text}></div>
		</div>

		<h1 id='stagelabel'>{stage_label}</h1>
		<div id='progresscontainer'>
			<div id='progress_time'>{((internal_state === 'playing' || internal_state === 'paused') && current_stage) ? current_stage.duration - offset_sec : ''}</div>
			<div id='progress' style='width: {bar_width}%'></div>
		</div>

		<div id='gameprogress'>
			{#each game_stages as s, i}
				{#if s.type === 'bead' || s.type === 'breath'}
					<span class={'prog-' + s.type + ' ' + progress_class(i, current_stage_idx)} style='width: {100 * s.duration / inner_game_length}%'></span>
				{/if}
			{/each}
		</div>

		{#if (_magister == null || _magister == true)}
			{#if internal_state == 'waiting'}
				<button on:click={upd('state', 'playing')}>Start</button>
			{:else if internal_state == 'playing'}
				<button on:click={upd('state', 'paused')}>Pause</button>
			{:else if internal_state == 'paused'}
				<button on:click={upd('state', 'playing')}>Resume</button>
			{/if}
		{/if}

		<div style='height: 400px;'></div>

		<details>
			<!-- I'm not ready to delete these UI elements but we might not use them -->
			<summary>Info</summary>

			<h1>{game_config.topic}</h1>
			<h4>Room: <em>{room}</em> <a href="../..">(leave)</a></h4>

			<div>
				{state === 'waiting' ? 'Waiting for the game to start'
				: state === 'paused' ? 'GAME PAUSED'
				: state === 'playing' ? 'Game in progress'
				: ''}
			</div>
			{#if connection !== 'connected'}
				<div>DISCONNECTED FROM GAME SERVER</div>
			{:else}
				{#if _active_sessions == 1}
					<div>You are alone in the room</div>
				{:else}
					<div>{_active_sessions} players are in this room</div>
				{/if}
			{/if}
		</details>

		{#if _magister == null || _magister == true}
			<details class='config' bind:open={config_open}>
				<summary>Game controls</summary>

				<p>
					{#if _magister == null}
						This will effect all players. Will you borrow power? Will you steal it?
					{:else}
						You are master of the games. These controls are yours alone.
					{/if}
				</p>

				{#if internal_state == 'waiting'}
					<button on:click={upd('state', 'playing')}>Start</button>
				{:else if internal_state == 'playing'}
					<button on:click={upd('state', 'paused')}>Pause</button>
				{:else if internal_state == 'paused'}
					<button on:click={upd('state', 'playing')}>Resume</button>
				{/if}

				{#if internal_state == 'paused' || internal_state == 'completed' }
					<button on:click={upd('state', 'waiting')}>Restart game</button>
				{/if}

				<label>
					<span>Topic</span>
					<input disabled={settings_disabled} type='text' value={game_config.topic} on:input={config('topic')} list='archetopics' >
					<datalist id='archetopics'>
						{#each ARCHETOPICS as topic}
							<option value={topic}>
						{/each}
					</datalist>
				</label>

				<label>
					<span>Pre-game meditation</span>
					<input disabled={settings_disabled} type='checkbox' checked={game_config.meditate} on:input={config('meditate')} >
				</label>

				<label>
					<span>Post game contemplation</span>
					<input disabled={settings_disabled} type='checkbox' checked={game_config.contemplation} on:input={config('contemplation')} >
				</label>

				<label>
					<span>Number of players</span>
					<input disabled={settings_disabled} type='number' pattern='[0-9]*' value={game_config.players} on:input={config('players')} min=1 max=12 >
				</label>

				<label>
					<span>Number of rounds</span>
					<input disabled={settings_disabled} type='number' pattern='[0-9]*' value={game_config.rounds} on:input={config('rounds')} min=1 max=20>
				</label>

				<label>
					<span>Seconds per bead</span>
					<input disabled={settings_disabled} type='number' pattern='[0-9]*' value={game_config.seconds_per_bead} on:input={config('seconds_per_bead')}>
				</label>

				<label>
					<span>Seconds between beads</span>
					<input disabled={settings_disabled} type='number' pattern='[0-9]*' value={game_config.seconds_between_bead} on:input={config('seconds_between_bead')}>
				</label>

				<div style='margin-top: 1em;'>
					(Total game length: {roundish(
						game_stages.reduce((x, s) => x + s.duration, 0) / 60
					)} minutes)
				</div>

				<div id='magister_box' class:magister_opaque>
					{#if _magister == null}
						<button on:click={upd('_magister', true)}>Assume the mantle of Magister Ludi</button>
						<p><i>Advanced - for large games</i></p>
						<p>When present, the Magister Ludi (master of the games) has exclusive control of the game.</p>
					{:else if _magister == true}
						<button on:click={upd('_magister', null)}>Abdicate Magister Ludi status</button>
						<p>You are the master of the games. You have exclusive control over playing, pausing and configuring this game.</p>
						<p>Do not close this browser window or you will be dethroned.</p>
					{/if}
				</div>
			</details>
		{:else}
			<p class='config'>Magister Ludi is managing this game.</p>
		{/if}
	{/if}
</main>

<style>

main {
	/* margin-bottom: 3em; */
	text-align: center;
}

#fixaudio {
	z-index: 1;
	color: var(--fg-color);
	background-color: var(--bg-highlight);
	position: absolute;
	bottom: 2px;
	width: 300px;
	padding: 0.5em 1em;
	left: 50%;
	transform: translateX(-50%);
	font-size: 130%;
}

#topicimg {
	width: 300px;
	display: inline-block;
}
#topictext:not(:empty) {
	padding: 3em 0 2em 0;
	font-size: 130%;
	font-style: italic;
}

/* .magister {
	background-color: var(--bg-highlight);
} */

/* h1 {
	margin-top: 1em;
} */

#stagelabel:empty {
	height: 1.2em;
}

#progresscontainer {
	/* width: calc(100% - 50px); */
	position: relative;
	margin: 10px 25px;
	height: 5em;
	border: 2px solid var(--fg-color);
	/* margin-bottom: 0; */
}

#progress_time {
	position: absolute;
	/* color: red; */
	/* font-size: var(--bg-color); */
	color: white;
	/* color: white; */
	font-size: 54px;
	margin-left: 5px;
	mix-blend-mode: difference;
}

#progress {
	background-color: var(--fg-color);
	/* width: 50%; */
	height: 100%;
	/* transition: width 1s linear; */
}

#gameprogress {
	/* width: 300px; */
	margin: 25px;
	height: 15px;
	/* background-color: blue; */
	margin-top: 0;
}

#gameprogress > span {
	display: inline-block;
	/* height: 10px; */
	background-color: var(--fg-color);
	/* border-left: 1px solid var(--bg-color);
	border-right: 1px solid var(--bg-color); */
}

/* .prog-waiting {
	height: 100%;
} */
/* .prog-meditate, .prog-contemplation {
	height: 50%;
} */
.prog-bead {
	height: 100%;
}
/* .prog-breath {
} */

.s-done {
	opacity: 20%;
}
/* .s-active {

} */
.s-waiting {
	opacity: 50%;
}


/***** Game config *****/
.config {
	margin-top: 2em;
}

summary {
	text-decoration: underline;
	cursor: pointer;
}

button {
	font-size: 140%;
	margin: 10px 0;
	color: var(--bg-color);
	/* color: var(--fg-color); */
}

details > :first-child {
	margin-bottom: 1em;
}

label {
	margin-bottom: 3px;
}
label > :first-child {
	display: inline-block;
	min-width: 14em;
}

input {
	width: 7em;
	font-size: 16px;
	/* color: var(--bg-color); */
	border: 2px solid #686868;
}

input[type=checkbox] {
	height: 1em;
}

label {
	display: block;
}

#magister_box {
	border: 1px dashed var(--fg-color);
	/* margin: 1em 0; */
	margin: 1em auto;
	padding: 0.8em;
	max-width: 500px;
	background-color: var(--bg-highlight);
	opacity: 40%;
	transition: opacity 0.3s ease-out;
}

#magister_box.magister_opaque, #magister_box:hover {
	opacity: 100%;
}

#magister_box > button {
	display: block;
	font-size: 100%;
	width: 100%;
	margin-top: 0;
	padding: 3px 0;
}

</style>